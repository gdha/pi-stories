{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Raspberry PI k3s stories \u00b6 You can use the editor on GitHub to maintain and preview the content for your website in Markdown files. Whenever you commit to this repository, GitHub Pages will run Jekyll to rebuild the pages in your site, from the content in your Markdown files. Markdown Syntax \u00b6 Markdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for Syntax highlighted code block # Header 1 ## Header 2 ### Header 3 - Bulleted - List 1. Numbered 2. List **Bold** and _Italic_ and `Code` text [Link](url) and ![Image](src) For more details see GitHub Flavored Markdown . Mkdocs Themes with GitHub Pages \u00b6 Read how to set-up the site with GitHub pages Support or Contact \u00b6 Having trouble with Pages? Check out our documentation or contact support and we\u2019ll help you sort it out.","title":"WELCOME"},{"location":"#welcome_to_raspberry_pi_k3s_stories","text":"You can use the editor on GitHub to maintain and preview the content for your website in Markdown files. Whenever you commit to this repository, GitHub Pages will run Jekyll to rebuild the pages in your site, from the content in your Markdown files.","title":"Welcome to Raspberry PI k3s stories"},{"location":"#markdown_syntax","text":"Markdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for Syntax highlighted code block # Header 1 ## Header 2 ### Header 3 - Bulleted - List 1. Numbered 2. List **Bold** and _Italic_ and `Code` text [Link](url) and ![Image](src) For more details see GitHub Flavored Markdown .","title":"Markdown Syntax"},{"location":"#mkdocs_themes_with_github_pages","text":"Read how to set-up the site with GitHub pages","title":"Mkdocs Themes with GitHub Pages"},{"location":"#support_or_contact","text":"Having trouble with Pages? Check out our documentation or contact support and we\u2019ll help you sort it out.","title":"Support or Contact"},{"location":"pi-stories1/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Basic OS configuration \u00b6 Before we can build a PI 4 cluster using kubernetes software of some kind (most likely k3s) we need to buy the required hardware to begin with. In our case we decided to go for 5 Raspberry Pi's type 4 with 4 GB RAM. We leave the exercise to you what size of SD card you want (more GB cost more money, but gives you some room for expansion later on). We also bought some small USB sticks (type SanDisk Ultra Fit USB 3.1 flash drive 128 GB) for building an object oriented file system (but that is for a later series). Also, we bought some PI cases with fans built-in to keep the processor cool as we know kubernetes may heat up the processor [1]. We noticed that the temperature of a case with fans is around 40 degrees Celsius where with a metal case only (without fans) it is around 60 degrees Celsius. Decided to buy quickly an extra case with fans as it is worth its money. We were charmed with Ubuntu 20 series software and downloaded the Pi4 64-bit version and used the dd command to burn it onto the SD cards [2]. Link it all together to start a Pi computer one at the time. The first time we hooked the micro-HDMI to a TV-screen so we could watch the first kick off and to see everything looked right. Also, we needed to reset the default password of the built-in account named 'ubuntu'. By default the Pi computer is using DHCP for retrieving IP addresses, but we want to assign a static IPv4 address. Therefore, think ahaid and use something like this: cat >> /etc/hosts <<EOF # PI cluster 192.168.0.201 n1 192.168.0.202 n2 192.168.0.203 n3 192.168.0.204 n4 192.168.0.205 n5 EOF Of course, change the IPv4 addresses to your local taste. It is also a good idea to hard-code your local timezone, in our case we choose for Europe/Brussels, e.g. timedatectl set-timezone Europe/Brussels The kubernetes cluster (not yet of course) prefers not having IPv6 active, therefore, disable it via: echo \"net.ipv6.conf.all.disable_ipv6 = 1\" >> /etc/sysctl.conf sysctl -p And, check the time settings: systemctl status systemd-timesyncd To disable DHCP at next restart execute the following: cat > /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg <<EOF # network: {config: disabled} network: ethernets: eth0: dhcp4: true optional: true version: 2 EOF And, lets hide the original netplan yaml file: mkdir /etc/netplan/.hide mv /etc/netplan/50-cloud-init.yaml /etc/netplan/.hide The following steps differ node per node as we will define the hostname and fix the IPv4 address: hostnamectl set-hostname n1 Be careful, to use the correct hostname in above command. To define your permanent IPv4 address create your own netplan configuration as follow: cat > /etc/netplan/01-netcfg.yaml <<EOF # This file describes the network interfaces available on your system # For more information, see netplan(5). network: version: 2 renderer: networkd ethernets: eth0: dhcp4: no # IP address/subnet mask addresses: [192.168.0.201/24] # default gateway gateway4: 192.168.0.1 nameservers: # name server this host refers addresses: [192.168.0.1,8.8.8.8] dhcp6: no EOF Then, edit the file /etc/netplan/01-netcfg.yaml to adjust the correct IPv4 address of the node and also modify the gateway IPv4 address to your needs. On your laptop (or control server) we also add the IPv4 addresses of our Pi systems to the /etc/hosts file and to make your live easy copy your public OpenSSH keys to the ubuntu account like: ssh-copy-id ubuntu@n[1-5] Reboot this Pi computer and try to login via your laptop using ssh ubuntu@n1 OK so far for the first part, but just want to share picture of our setup with 5 Raspberry Pi's 4 nodes n[1-5]: References: \u00b6 [1] Joy-it Armor case \"Block Active\" for Raspberry Pi 4 [2] Download your Ubuntu Pi image Edit history \u00b6 Initial post on 01/Jul/2020 Updated title on 09/Sep/2020","title":"Raspberry Pi 4 Basic OS Configuration"},{"location":"pi-stories1/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories1/#raspberry_pi_4_cluster_series_-_basic_os_configuration","text":"Before we can build a PI 4 cluster using kubernetes software of some kind (most likely k3s) we need to buy the required hardware to begin with. In our case we decided to go for 5 Raspberry Pi's type 4 with 4 GB RAM. We leave the exercise to you what size of SD card you want (more GB cost more money, but gives you some room for expansion later on). We also bought some small USB sticks (type SanDisk Ultra Fit USB 3.1 flash drive 128 GB) for building an object oriented file system (but that is for a later series). Also, we bought some PI cases with fans built-in to keep the processor cool as we know kubernetes may heat up the processor [1]. We noticed that the temperature of a case with fans is around 40 degrees Celsius where with a metal case only (without fans) it is around 60 degrees Celsius. Decided to buy quickly an extra case with fans as it is worth its money. We were charmed with Ubuntu 20 series software and downloaded the Pi4 64-bit version and used the dd command to burn it onto the SD cards [2]. Link it all together to start a Pi computer one at the time. The first time we hooked the micro-HDMI to a TV-screen so we could watch the first kick off and to see everything looked right. Also, we needed to reset the default password of the built-in account named 'ubuntu'. By default the Pi computer is using DHCP for retrieving IP addresses, but we want to assign a static IPv4 address. Therefore, think ahaid and use something like this: cat >> /etc/hosts <<EOF # PI cluster 192.168.0.201 n1 192.168.0.202 n2 192.168.0.203 n3 192.168.0.204 n4 192.168.0.205 n5 EOF Of course, change the IPv4 addresses to your local taste. It is also a good idea to hard-code your local timezone, in our case we choose for Europe/Brussels, e.g. timedatectl set-timezone Europe/Brussels The kubernetes cluster (not yet of course) prefers not having IPv6 active, therefore, disable it via: echo \"net.ipv6.conf.all.disable_ipv6 = 1\" >> /etc/sysctl.conf sysctl -p And, check the time settings: systemctl status systemd-timesyncd To disable DHCP at next restart execute the following: cat > /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg <<EOF # network: {config: disabled} network: ethernets: eth0: dhcp4: true optional: true version: 2 EOF And, lets hide the original netplan yaml file: mkdir /etc/netplan/.hide mv /etc/netplan/50-cloud-init.yaml /etc/netplan/.hide The following steps differ node per node as we will define the hostname and fix the IPv4 address: hostnamectl set-hostname n1 Be careful, to use the correct hostname in above command. To define your permanent IPv4 address create your own netplan configuration as follow: cat > /etc/netplan/01-netcfg.yaml <<EOF # This file describes the network interfaces available on your system # For more information, see netplan(5). network: version: 2 renderer: networkd ethernets: eth0: dhcp4: no # IP address/subnet mask addresses: [192.168.0.201/24] # default gateway gateway4: 192.168.0.1 nameservers: # name server this host refers addresses: [192.168.0.1,8.8.8.8] dhcp6: no EOF Then, edit the file /etc/netplan/01-netcfg.yaml to adjust the correct IPv4 address of the node and also modify the gateway IPv4 address to your needs. On your laptop (or control server) we also add the IPv4 addresses of our Pi systems to the /etc/hosts file and to make your live easy copy your public OpenSSH keys to the ubuntu account like: ssh-copy-id ubuntu@n[1-5] Reboot this Pi computer and try to login via your laptop using ssh ubuntu@n1 OK so far for the first part, but just want to share picture of our setup with 5 Raspberry Pi's 4 nodes n[1-5]:","title":"Raspberry Pi 4 cluster Series - Basic OS configuration"},{"location":"pi-stories1/#references","text":"[1] Joy-it Armor case \"Block Active\" for Raspberry Pi 4 [2] Download your Ubuntu Pi image","title":"References:"},{"location":"pi-stories1/#edit_history","text":"Initial post on 01/Jul/2020 Updated title on 09/Sep/2020","title":"Edit history"},{"location":"pi-stories10/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Issues after upgrading k3s \u00b6 We upgrade our k3s version v1.20.4+k3s1 to v1.22.4+k3s1 according the procedure ligned out in [1]. However, we noticed that after the k3s upgrade their where some issues with our cluster as seen with command kubectl get pods -A cert-manager cert-manager-cainjector-6d59c8d4f7-hjszd 0/1 CrashLoopBackOff 39 (102s ago) 270d longhorn-system longhorn-driver-deployer-666c84fbb7-2lqqm 0/1 CrashLoopBackOff 43 (3m13s ago) 3h28m graphite graphite-0 0/1 ContainerCreating 0 167m Fixing the cert-manager issue \u00b6 The first thing you need to do when you have issues with apod is looking at the logs, therefore, we execute the following: $ kubectl logs -n cert-manager cert-manager-cainjector-6d59c8d4f7-hjszd I1207 13:08:09.495826 1 start.go:89] \"starting\" version=\"v1.0.4\" revision=\"4d870e49b43960fad974487a262395e65da1373e\" I1207 13:08:11.316015 1 request.go:645] Throttling request took 1.036581941s, request: GET:https://10.43.0.1:443/apis/admissionregistration.k8s.io/v1?timeout=32s I1207 13:08:12.316195 1 request.go:645] Throttling request took 2.035257813s, request: GET:https://10.43.0.1:443/apis/node.k8s.io/v1beta1?timeout=32s E1207 13:08:12.325362 1 start.go:158] cert-manager/ca-injector \"msg\"=\"error registering core-only controllers\" \"error\"=\"no matches for kind \\\"MutatingWebhookConfiguration\\\" in version \\\"admissionregistration.k8s.io/v1beta1\\\"\" From above output we can see that the cert-manager version is v1.0.4. The best thing to do is going to see the documentation [2] of cert-manager whether there are known issues or other items of interest. At the page [2] we read the following sentence \"Following their deprecation in version 1.4, the cert-manager API versions v1alpha2, v1alpha3, and v1beta1 are no longer served.\" - okay - we need to upgrade cert-manager to fix this. We did found an excellent article [3] that serverd our purposes. We simply have to look at the GitHub page of cert-manager releases [4] to find the latest stable version of cert-manager (today 07 December 2021 it was v1.6.1). Then we just downloaded the yaml file of cert-manager for arm (as this is a pi cluster) with a simple trick described oon [3]: VER=$(curl -s \"https://github.com/jetstack/cert-manager/releases/latest\" | cut -d\\\" -f2 | awk -F '/' '{print $NF}') curl -sL \\ https://github.com/jetstack/cert-manager/releases/download/${VER}/cert-manager.yaml |\\ sed -r 's/(image:.*):(v.*)$/\\1-arm:\\2/g' > cert-manager-arm.yaml Finally, we can upgrade our cert-manager pods with the command: $ kubectl replace -f cert-manager-arm.yaml customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io replaced customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io replaced customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io replaced customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io replaced customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io replaced customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io replaced namespace/cert-manager replaced serviceaccount/cert-manager-cainjector replaced serviceaccount/cert-manager replaced serviceaccount/cert-manager-webhook replaced clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim replaced clusterrole.rbac.authorization.k8s.io/cert-manager-view replaced clusterrole.rbac.authorization.k8s.io/cert-manager-edit replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests replaced clusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews replaced role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection replaced role.rbac.authorization.k8s.io/cert-manager:leaderelection replaced role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving replaced rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection replaced rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection replaced rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving replaced service/cert-manager replaced service/cert-manager-webhook replaced deployment.apps/cert-manager-cainjector replaced deployment.apps/cert-manager replaced deployment.apps/cert-manager-webhook replaced mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook replaced validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook replaced $ kubectl get pods -n cert-manager NAME READY STATUS RESTARTS AGE cert-manager-5d4dc99cfc-mfmrp 1/1 Running 0 161m cert-manager-cainjector-598fcd9fdd-sn9cs 1/1 Running 0 161m cert-manager-webhook-746ff5ffb9-wmnvp 1/1 Running 0 161m Fixing longhorn \u00b6 The easiest way to fix longhorn is to remove and do a fresh install [5]. We remember we also had the issue when longhorn pods were deleted that the namespace of longhorn stayed in terminating status. To remediate this there is a trick Fixing graphite \u00b6 The graphite container was setup [6] via the kubernetes yaml files. Therefore, to remove the failing pod just run (inside the kubernetes directory of our graphite github project ): $ kubectl delete -f *.yaml And, recreate it again: $ kubectle apply -f *.yaml References \u00b6 [1] Raspberry Pi 4 cluster Series - Upgrading k3s software on your cluster [2] Removing Deprecated API Resources in cert-manager [3] K3s on Raspberry Pi - cert-manager [4] GitHub Cert-manager release page [5] Installation of longhorn [6] Graphite kubernetes yaml file for the Pi4 Edit history \u00b6 Added \"Fixing longhorn\" (24/Jan/2023) Adding \"Fixing graphite\" (24/Feb/2023)","title":"Raspberry Pi 4 Issues after upgrading k3s"},{"location":"pi-stories10/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories10/#raspberry_pi_4_cluster_series_-_issues_after_upgrading_k3s","text":"We upgrade our k3s version v1.20.4+k3s1 to v1.22.4+k3s1 according the procedure ligned out in [1]. However, we noticed that after the k3s upgrade their where some issues with our cluster as seen with command kubectl get pods -A cert-manager cert-manager-cainjector-6d59c8d4f7-hjszd 0/1 CrashLoopBackOff 39 (102s ago) 270d longhorn-system longhorn-driver-deployer-666c84fbb7-2lqqm 0/1 CrashLoopBackOff 43 (3m13s ago) 3h28m graphite graphite-0 0/1 ContainerCreating 0 167m","title":"Raspberry Pi 4 cluster Series - Issues after upgrading k3s"},{"location":"pi-stories10/#fixing_the_cert-manager_issue","text":"The first thing you need to do when you have issues with apod is looking at the logs, therefore, we execute the following: $ kubectl logs -n cert-manager cert-manager-cainjector-6d59c8d4f7-hjszd I1207 13:08:09.495826 1 start.go:89] \"starting\" version=\"v1.0.4\" revision=\"4d870e49b43960fad974487a262395e65da1373e\" I1207 13:08:11.316015 1 request.go:645] Throttling request took 1.036581941s, request: GET:https://10.43.0.1:443/apis/admissionregistration.k8s.io/v1?timeout=32s I1207 13:08:12.316195 1 request.go:645] Throttling request took 2.035257813s, request: GET:https://10.43.0.1:443/apis/node.k8s.io/v1beta1?timeout=32s E1207 13:08:12.325362 1 start.go:158] cert-manager/ca-injector \"msg\"=\"error registering core-only controllers\" \"error\"=\"no matches for kind \\\"MutatingWebhookConfiguration\\\" in version \\\"admissionregistration.k8s.io/v1beta1\\\"\" From above output we can see that the cert-manager version is v1.0.4. The best thing to do is going to see the documentation [2] of cert-manager whether there are known issues or other items of interest. At the page [2] we read the following sentence \"Following their deprecation in version 1.4, the cert-manager API versions v1alpha2, v1alpha3, and v1beta1 are no longer served.\" - okay - we need to upgrade cert-manager to fix this. We did found an excellent article [3] that serverd our purposes. We simply have to look at the GitHub page of cert-manager releases [4] to find the latest stable version of cert-manager (today 07 December 2021 it was v1.6.1). Then we just downloaded the yaml file of cert-manager for arm (as this is a pi cluster) with a simple trick described oon [3]: VER=$(curl -s \"https://github.com/jetstack/cert-manager/releases/latest\" | cut -d\\\" -f2 | awk -F '/' '{print $NF}') curl -sL \\ https://github.com/jetstack/cert-manager/releases/download/${VER}/cert-manager.yaml |\\ sed -r 's/(image:.*):(v.*)$/\\1-arm:\\2/g' > cert-manager-arm.yaml Finally, we can upgrade our cert-manager pods with the command: $ kubectl replace -f cert-manager-arm.yaml customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io replaced customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io replaced customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io replaced customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io replaced customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io replaced customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io replaced namespace/cert-manager replaced serviceaccount/cert-manager-cainjector replaced serviceaccount/cert-manager replaced serviceaccount/cert-manager-webhook replaced clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim replaced clusterrole.rbac.authorization.k8s.io/cert-manager-view replaced clusterrole.rbac.authorization.k8s.io/cert-manager-edit replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests replaced clusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews replaced role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection replaced role.rbac.authorization.k8s.io/cert-manager:leaderelection replaced role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving replaced rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection replaced rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection replaced rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving replaced service/cert-manager replaced service/cert-manager-webhook replaced deployment.apps/cert-manager-cainjector replaced deployment.apps/cert-manager replaced deployment.apps/cert-manager-webhook replaced mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook replaced validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook replaced $ kubectl get pods -n cert-manager NAME READY STATUS RESTARTS AGE cert-manager-5d4dc99cfc-mfmrp 1/1 Running 0 161m cert-manager-cainjector-598fcd9fdd-sn9cs 1/1 Running 0 161m cert-manager-webhook-746ff5ffb9-wmnvp 1/1 Running 0 161m","title":"Fixing the cert-manager issue"},{"location":"pi-stories10/#fixing_longhorn","text":"The easiest way to fix longhorn is to remove and do a fresh install [5]. We remember we also had the issue when longhorn pods were deleted that the namespace of longhorn stayed in terminating status. To remediate this there is a trick","title":"Fixing longhorn"},{"location":"pi-stories10/#fixing_graphite","text":"The graphite container was setup [6] via the kubernetes yaml files. Therefore, to remove the failing pod just run (inside the kubernetes directory of our graphite github project ): $ kubectl delete -f *.yaml And, recreate it again: $ kubectle apply -f *.yaml","title":"Fixing graphite"},{"location":"pi-stories10/#references","text":"[1] Raspberry Pi 4 cluster Series - Upgrading k3s software on your cluster [2] Removing Deprecated API Resources in cert-manager [3] K3s on Raspberry Pi - cert-manager [4] GitHub Cert-manager release page [5] Installation of longhorn [6] Graphite kubernetes yaml file for the Pi4","title":"References"},{"location":"pi-stories10/#edit_history","text":"Added \"Fixing longhorn\" (24/Jan/2023) Adding \"Fixing graphite\" (24/Feb/2023)","title":"Edit history"},{"location":"pi-stories11/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Replacing internal traefik with Metallb \u00b6 The problem with have is that with our home pi4 cluster we don't have a decent external load-balancer. Therefore, it is hard to access pods via an external IP address, such as the ones we have on our hosts (in our case in the range of 192.168.0.200-254). To read some more in-depth learnings from metallb and ingresses read the blog Ingresses and Load Balancers in Kubernetes with MetalLB and nginx-ingress The steps we need to perform are... Re-configure the pi4 cluster with ansible \u00b6 Our k3s-ansible project was updated with: $ cat inventory/my-cluster/group_vars/all.yml --- k3s_version: v1.26.0+k3s2 ansible_user: gdha systemd_dir: /etc/systemd/system master_ip: \"{{ hostvars[groups['master'][0]]['ansible_host'] | default(groups['master'][0]) }}\" extra_server_args: \"--write-kubeconfig-mode 644 --disable traefik --disable servicelb\" extra_agent_args: \"\" in such way by disabling the default traefik and internal load-balancer delivered with the standard k3s implementation. While we were busy we also used the latest k3s version available at this given moment. Then it is just a matter of re-running: ansible-playbook site.yml -i inventory/my-cluster/hosts.ini It will remove k3s and re-implement it with the internal traefik, but all pods already installed remain present. Excellent news. Install metalllb layer2 load-balancer \u00b6 The main documentation of metallb can be found at https://metallb.universe.tf/installation/ [1]. We used the following steps: $ helm repo add metallb https://metallb.github.io/metallb $ helm repo list NAME URL longhorn https://charts.longhorn.io kiwigrid https://kiwigrid.github.io metallb https://metallb.github.io/metallb $ cat metallb-values.yaml apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: loadbalancer-pool namespace: kube-system spec: addresses: - 192.168.0.230-192.168.0.250 $ helm install metallb metallb/metallb --namespace kube-system -f metallb-values.yaml NAME: metallb LAST DEPLOYED: Tue Jan 24 11:24:43 2023 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: MetalLB is now running in the cluster. Now you can configure it via its CRs. Please refer to the metallb official docs on how to use the CRs. We still have the old config of metallb so we save it under the config.yaml $ cat >config.yaml <<EOD apiVersion: v1 data: config: | address-pools: - addresses: - 192.168.0.230-192.168.0.250 name: default protocol: layer2 kind: ConfigMap metadata: annotations: meta.helm.sh/release-name: metallb meta.helm.sh/release-namespace: kube-system creationTimestamp: \"2022-05-23T10:38:34Z\" labels: app.kubernetes.io/instance: metallb app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: metallb app.kubernetes.io/version: v0.12.1 helm.sh/chart: metallb-0.12.1 name: metallb namespace: kube-system resourceVersion: \"3256248\" uid: 774b05a7-7ad7-4de3-a9e1-2a636f988ed1 EOD According the procedure to generate a new CRD resource file we got: $ docker run -d -v $(pwd):/var/input quay.io/metallb/configmaptocrs Unable to find image 'quay.io/metallb/configmaptocrs:latest' locally latest: Pulling from metallb/configmaptocrs 9b18e9b68314: Pull complete 24157a5425f3: Pull complete b73e28ff5ad3: Pull complete Digest: sha256:6c144621e060722a082f2d5a2c4bd72f81d84f6cedc1153c33bb8f4f1277fac0 Status: Downloaded newer image for quay.io/metallb/configmaptocrs:latest 4254ef07d9b01effb956a89628915e4f3da15624e92edfc6c6f415f6fbe201cc $ cat resources.yaml # This was autogenerated by MetalLB's custom resource generator. apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: creationTimestamp: null name: default namespace: kube-system spec: addresses: - 192.168.0.230-192.168.0.250 status: {} --- apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: creationTimestamp: null name: l2advertisement1 namespace: kube-system spec: ipAddressPools: - default status: {} --- Now, we can apply (or replace) this in our cluster: $ kubectl create -f resources.yaml ipaddresspool.metallb.io/default created l2advertisement.metallb.io/l2advertisement1 created To verify if our IP range was properly accepted by our helm command we could run: $ kubectl get customresourcedefinitions.apiextensions.k8s.io ipaddresspools.metallb.io NAME CREATED AT ipaddresspools.metallb.io 2023-01-24T10:24:52Z Install NGINX Ingress Controller \u00b6 To verify the load-balancer is working we could execute [4]: $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.5.1/deploy/static/provider/cloud/deploy.yaml namespace/ingress-nginx created serviceaccount/ingress-nginx created serviceaccount/ingress-nginx-admission created role.rbac.authorization.k8s.io/ingress-nginx created role.rbac.authorization.k8s.io/ingress-nginx-admission created clusterrole.rbac.authorization.k8s.io/ingress-nginx created clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created rolebinding.rbac.authorization.k8s.io/ingress-nginx created rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created configmap/ingress-nginx-controller created service/ingress-nginx-controller created service/ingress-nginx-controller-admission created deployment.apps/ingress-nginx-controller created job.batch/ingress-nginx-admission-create created job.batch/ingress-nginx-admission-patch created ingressclass.networking.k8s.io/nginx created validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created $ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller LoadBalancer 10.43.243.82 192.168.0.231 80:30648/TCP,443:30920/TCP 2m28s ingress-nginx-controller-admission ClusterIP 10.43.232.246 <none> 443/TCP 2m28s Okay, so far so good. However, is the external IP address of our ingress-nginx-controller really open for a connection? To test execute: $ nc -vz 192.168.0.231 80 Connection to 192.168.0.231 80 port [tcp/http] succeeded! gdha@n1:~/projects/WIP$ nc -vz 192.168.0.231 443 Connection to 192.168.0.231 443 port [tcp/https] succeeded! Install traefik2 as replacement for the internal traefik of k3s \u00b6 Execute the following commands: $ helm repo add traefik https://helm.traefik.io/traefik $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"metallb\" chart repository ...Successfully got an update from the \"longhorn\" chart repository ...Successfully got an update from the \"traefik\" chart repository ...Successfully got an update from the \"kiwigrid\" chart repository Update Complete. \u2388Happy Helming!\u2388 We need to define a dummy (internal) name for our treafik2 application, therefore, create a file like the one shown below: $ cat traefik-values.yaml dashboard: enabled: true domain: traefik.example.com rbac: enabled: true And, finally use helm to install traefik2 with our hand-crafted values yaml file: $ helm install traefik traefik/traefik -n kube-system -f traefik-values.yaml NAME: traefik LAST DEPLOYED: Mon May 23 14:53:46 2022 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None Check if it is created properly: $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-d76bd69b-8z7dh 1/1 Running 0 4h25m local-path-provisioner-6c79684f77-jvdf6 1/1 Running 0 4h25m metrics-server-7cd5fcb6b7-d6wlx 1/1 Running 0 4h25m metallb-controller-777cbcf64f-vfz5v 1/1 Running 0 136m metallb-speaker-r7wbg 1/1 Running 0 136m metallb-speaker-5lxff 1/1 Running 0 136m metallb-speaker-cxskn 1/1 Running 0 136m metallb-speaker-24vgg 1/1 Running 0 136m metallb-speaker-wmzkg 1/1 Running 0 136m traefik-7b9cf77df9-cwp4l 1/1 Running 0 67s And, also very if the treafik service is present: $ kubectl get svc -n kube-system | grep traefik traefik LoadBalancer 10.43.78.204 192.168.0.230 80:31164/TCP,443:31610/TCP 13m We can also check the logs of traefik: $ kubectl -n kube-system logs $(kubectl -n kube-system get pods --selector \"app.kubernetes.io/name=traefik\" --output=name) time=\"2023-01-24T13:23:31Z\" level=info msg=\"Configuration loaded from flags.\" When we see above listed line the we are sure traefik is properly installed and configured. Now, we are ready to do some more tests with our new load-balancer and traefik. References \u00b6 [1] Metallb [2] Setting up your own k3s home cluster [3] Configuring Traefik 2 Ingress for Kubernetes [4] NGINX Ingress Controller Edit History \u00b6 update with new metallb and NGINC Ingress Controller - 25/Jan/2023","title":"Raspberry Pi 4 Replacing internal traefik with Metallb"},{"location":"pi-stories11/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories11/#raspberry_pi_4_cluster_series_-_replacing_internal_traefik_with_metallb","text":"The problem with have is that with our home pi4 cluster we don't have a decent external load-balancer. Therefore, it is hard to access pods via an external IP address, such as the ones we have on our hosts (in our case in the range of 192.168.0.200-254). To read some more in-depth learnings from metallb and ingresses read the blog Ingresses and Load Balancers in Kubernetes with MetalLB and nginx-ingress The steps we need to perform are...","title":"Raspberry Pi 4 cluster Series - Replacing internal traefik with Metallb"},{"location":"pi-stories11/#re-configure_the_pi4_cluster_with_ansible","text":"Our k3s-ansible project was updated with: $ cat inventory/my-cluster/group_vars/all.yml --- k3s_version: v1.26.0+k3s2 ansible_user: gdha systemd_dir: /etc/systemd/system master_ip: \"{{ hostvars[groups['master'][0]]['ansible_host'] | default(groups['master'][0]) }}\" extra_server_args: \"--write-kubeconfig-mode 644 --disable traefik --disable servicelb\" extra_agent_args: \"\" in such way by disabling the default traefik and internal load-balancer delivered with the standard k3s implementation. While we were busy we also used the latest k3s version available at this given moment. Then it is just a matter of re-running: ansible-playbook site.yml -i inventory/my-cluster/hosts.ini It will remove k3s and re-implement it with the internal traefik, but all pods already installed remain present. Excellent news.","title":"Re-configure the pi4 cluster with ansible"},{"location":"pi-stories11/#install_metalllb_layer2_load-balancer","text":"The main documentation of metallb can be found at https://metallb.universe.tf/installation/ [1]. We used the following steps: $ helm repo add metallb https://metallb.github.io/metallb $ helm repo list NAME URL longhorn https://charts.longhorn.io kiwigrid https://kiwigrid.github.io metallb https://metallb.github.io/metallb $ cat metallb-values.yaml apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: loadbalancer-pool namespace: kube-system spec: addresses: - 192.168.0.230-192.168.0.250 $ helm install metallb metallb/metallb --namespace kube-system -f metallb-values.yaml NAME: metallb LAST DEPLOYED: Tue Jan 24 11:24:43 2023 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: MetalLB is now running in the cluster. Now you can configure it via its CRs. Please refer to the metallb official docs on how to use the CRs. We still have the old config of metallb so we save it under the config.yaml $ cat >config.yaml <<EOD apiVersion: v1 data: config: | address-pools: - addresses: - 192.168.0.230-192.168.0.250 name: default protocol: layer2 kind: ConfigMap metadata: annotations: meta.helm.sh/release-name: metallb meta.helm.sh/release-namespace: kube-system creationTimestamp: \"2022-05-23T10:38:34Z\" labels: app.kubernetes.io/instance: metallb app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: metallb app.kubernetes.io/version: v0.12.1 helm.sh/chart: metallb-0.12.1 name: metallb namespace: kube-system resourceVersion: \"3256248\" uid: 774b05a7-7ad7-4de3-a9e1-2a636f988ed1 EOD According the procedure to generate a new CRD resource file we got: $ docker run -d -v $(pwd):/var/input quay.io/metallb/configmaptocrs Unable to find image 'quay.io/metallb/configmaptocrs:latest' locally latest: Pulling from metallb/configmaptocrs 9b18e9b68314: Pull complete 24157a5425f3: Pull complete b73e28ff5ad3: Pull complete Digest: sha256:6c144621e060722a082f2d5a2c4bd72f81d84f6cedc1153c33bb8f4f1277fac0 Status: Downloaded newer image for quay.io/metallb/configmaptocrs:latest 4254ef07d9b01effb956a89628915e4f3da15624e92edfc6c6f415f6fbe201cc $ cat resources.yaml # This was autogenerated by MetalLB's custom resource generator. apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: creationTimestamp: null name: default namespace: kube-system spec: addresses: - 192.168.0.230-192.168.0.250 status: {} --- apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: creationTimestamp: null name: l2advertisement1 namespace: kube-system spec: ipAddressPools: - default status: {} --- Now, we can apply (or replace) this in our cluster: $ kubectl create -f resources.yaml ipaddresspool.metallb.io/default created l2advertisement.metallb.io/l2advertisement1 created To verify if our IP range was properly accepted by our helm command we could run: $ kubectl get customresourcedefinitions.apiextensions.k8s.io ipaddresspools.metallb.io NAME CREATED AT ipaddresspools.metallb.io 2023-01-24T10:24:52Z","title":"Install metalllb layer2 load-balancer"},{"location":"pi-stories11/#install_nginx_ingress_controller","text":"To verify the load-balancer is working we could execute [4]: $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.5.1/deploy/static/provider/cloud/deploy.yaml namespace/ingress-nginx created serviceaccount/ingress-nginx created serviceaccount/ingress-nginx-admission created role.rbac.authorization.k8s.io/ingress-nginx created role.rbac.authorization.k8s.io/ingress-nginx-admission created clusterrole.rbac.authorization.k8s.io/ingress-nginx created clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created rolebinding.rbac.authorization.k8s.io/ingress-nginx created rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created configmap/ingress-nginx-controller created service/ingress-nginx-controller created service/ingress-nginx-controller-admission created deployment.apps/ingress-nginx-controller created job.batch/ingress-nginx-admission-create created job.batch/ingress-nginx-admission-patch created ingressclass.networking.k8s.io/nginx created validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created $ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller LoadBalancer 10.43.243.82 192.168.0.231 80:30648/TCP,443:30920/TCP 2m28s ingress-nginx-controller-admission ClusterIP 10.43.232.246 <none> 443/TCP 2m28s Okay, so far so good. However, is the external IP address of our ingress-nginx-controller really open for a connection? To test execute: $ nc -vz 192.168.0.231 80 Connection to 192.168.0.231 80 port [tcp/http] succeeded! gdha@n1:~/projects/WIP$ nc -vz 192.168.0.231 443 Connection to 192.168.0.231 443 port [tcp/https] succeeded!","title":"Install NGINX Ingress Controller"},{"location":"pi-stories11/#install_traefik2_as_replacement_for_the_internal_traefik_of_k3s","text":"Execute the following commands: $ helm repo add traefik https://helm.traefik.io/traefik $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"metallb\" chart repository ...Successfully got an update from the \"longhorn\" chart repository ...Successfully got an update from the \"traefik\" chart repository ...Successfully got an update from the \"kiwigrid\" chart repository Update Complete. \u2388Happy Helming!\u2388 We need to define a dummy (internal) name for our treafik2 application, therefore, create a file like the one shown below: $ cat traefik-values.yaml dashboard: enabled: true domain: traefik.example.com rbac: enabled: true And, finally use helm to install traefik2 with our hand-crafted values yaml file: $ helm install traefik traefik/traefik -n kube-system -f traefik-values.yaml NAME: traefik LAST DEPLOYED: Mon May 23 14:53:46 2022 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None Check if it is created properly: $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-d76bd69b-8z7dh 1/1 Running 0 4h25m local-path-provisioner-6c79684f77-jvdf6 1/1 Running 0 4h25m metrics-server-7cd5fcb6b7-d6wlx 1/1 Running 0 4h25m metallb-controller-777cbcf64f-vfz5v 1/1 Running 0 136m metallb-speaker-r7wbg 1/1 Running 0 136m metallb-speaker-5lxff 1/1 Running 0 136m metallb-speaker-cxskn 1/1 Running 0 136m metallb-speaker-24vgg 1/1 Running 0 136m metallb-speaker-wmzkg 1/1 Running 0 136m traefik-7b9cf77df9-cwp4l 1/1 Running 0 67s And, also very if the treafik service is present: $ kubectl get svc -n kube-system | grep traefik traefik LoadBalancer 10.43.78.204 192.168.0.230 80:31164/TCP,443:31610/TCP 13m We can also check the logs of traefik: $ kubectl -n kube-system logs $(kubectl -n kube-system get pods --selector \"app.kubernetes.io/name=traefik\" --output=name) time=\"2023-01-24T13:23:31Z\" level=info msg=\"Configuration loaded from flags.\" When we see above listed line the we are sure traefik is properly installed and configured. Now, we are ready to do some more tests with our new load-balancer and traefik.","title":"Install traefik2 as replacement for the internal traefik of k3s"},{"location":"pi-stories11/#references","text":"[1] Metallb [2] Setting up your own k3s home cluster [3] Configuring Traefik 2 Ingress for Kubernetes [4] NGINX Ingress Controller","title":"References"},{"location":"pi-stories11/#edit_history","text":"update with new metallb and NGINC Ingress Controller - 25/Jan/2023","title":"Edit History"},{"location":"pi-stories12/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Spin up graphite and temperature2celsius pods \u00b6 Graphite is an enterprise-ready monitoring tool that runs equally well on cheap hardware or Cloud infrastructure. Teams use Graphite to track the performance of their websites, applications, business services, and networked servers. It marked the start of a new generation of monitoring tools, making it easier than ever to store, retrieve, share, and visualize time-series data. Info copied from web-site graphiteapp.org . Spin up graphite pod \u00b6 To start we need the sources to build of graphite pod, therefore, clone our pi4-graphite github repository [1]: git clone https://github.com/gdha/pi4-graphite.git You will find in the pi4-graphite directory a Dockerfile which will be used to create a new container image: To build the docker image on your Pi4 system execute the following command: $ ./build.sh v1.2 Login Succeeded Building graphite:v1.2 Sending build context to Docker daemon 137.2kB Step 1/11 : FROM graphiteapp/graphite-statsd:1.1.10-4 1.1.10-4: Pulling from graphiteapp/graphite-statsd 9981e73032c8: Pull complete de219cb0be19: Pull complete a152a03e3913: Pull complete 1e02b3e21032: Pull complete fb7d8007a803: Pull complete Digest: sha256:fb9eb6fdd8f6073dd4ff1acd2169b693d4633045017701713651befbc62fe9f5 Status: Downloaded newer image for graphiteapp/graphite-statsd:1.1.10-4 ... Successfully built afb29746df49 Successfully tagged ghcr.io/gdha/graphite:v1.2 Pushing graphite:v1.2 to GitHub Docker Container registry To start the graphite pod on our kubernetes cluster do the following: $ cd kubernetes $ kubectl apply -f ./graphite-namespace.yaml namespace/graphite created $ kubectl apply -f ./graphite-secret.yaml secret/graphite created $ kubectl apply -f ./ghcr-secret.yaml secret/dockerconfigjson-github-com created $ kubectl apply -f ./persistentvolumeclaim-graphite.yaml persistentvolumeclaim/graphite created $ kubectl apply -f ./statefulset_graphite.yaml statefulset.apps/graphite created $ kubectl apply -f ./service_graphite.yaml service/graphite-svc created Open a browser with URL http://n1:30080 to see the result of the tests: Deploy temperature2celsius pods \u00b6 The purpose of this pod is to demonstrate that we can send information to our graphite application. We choose to send the temperatire in Celsius once a minute. Start with cloning our pi4-temperature2graphite github repository [2]: git clone https://github.com/gdha/pi4-temperature2graphite.git cd pi4-temperature2graphite The Dockerfile content is: # Dockerfile to create the container used to send the temperature of the RPI4 # to the graphite pod #FROM gdha/rpi-alpine-rootfs/alpine:v1.37 FROM alpine:latest LABEL org.opencontainers.image.sourcec=https://github.com/gdha/pi4-temperature2graphite LABEL org.opencontainers.image.description \"pi4-temperature2graphite build for the ARM64\" LABEL org.opencontainers.image.licenses \"GPL-3.0-or-later\" LABEL maintainer \"Gratien Dhaese <gratien.dhaese@gmail.com>\" COPY entrypoint.sh /entrypoint.sh COPY k3s /usr/bin/kubectl COPY api_query.sh /api_query.sh # Update and install dependencies RUN apk add --update nodejs npm curl RUN chmod a+x /entrypoint.sh \\ && chmod a+x /api_query.sh \\ && echo \"Europe/Brussels\" > /etc/timezone \\ && chmod 755 /root ENTRYPOINT [\"/entrypoint.sh\"] And, the center of the image is the /entrypoint.sh script: #!/bin/sh # As this script runs on Alpine (busybox) we cannot use bash syntax #set -e POD=$(echo $HOSTNAME) HOSTNAME=$(cat /etc/hostname) #SERVER=$(/usr/local/bin/kubectl -n graphite describe pod graphite-0 | grep -i node: | cut -d/ -f2) while true do # move the SERVER line inside the loop as at each restart the graphite pod gets a new IP address SERVER=$(/usr/bin/kubectl -n graphite get pods -o wide | tail -1 | awk '{print $6}') cpu_temp=$(cat /sys/class/thermal/thermal_zone0/temp) #cpu_temp=$(($cpu_temp/1000)) cpu_temp=$(expr $cpu_temp / 1000) echo \"carbon.celsius.$HOSTNAME $cpu_temp $(date +%s)\" | timeout 2 nc $SERVER 2003 if [[ $? -eq 0 ]] ; then echo $(date '+%F %T') {\"caller\":\"entrypoint.sh:16\",\"pod\":\"$POD\",\"level\":\"info\",\"msg\":\"temperature $cpu_temp\"} else echo $(date '+%F %T') {\"caller\":\"entrypoint.sh:18\",\"pod\":\"$POD\",\"level\":\"error\",\"msg\":\"cannot connect to server $SERVER\"} fi # echo $cpu_temp sleep 60 done To build the image use the build.sh script and it pushes the image to ghcr.io/gdha/pi4-temperature2graphite:v1.7 (which is also the latest). To build version v1.7 we did the following: ./build v1.7 To bring the pods alive on our kubernetes cluster go to the kubernetes directory and execute: $ kubectl apply -f celsius-namespace.yaml namespace/celsius created kubectl apply -f celsius-secret.yaml secret/celsius created $ kubectl apply -f ghcr-secret.yaml secret/dockerconfigjson-github-com created $ kubectl apply -f celsius-rbac.yaml erviceaccount/celsius-sa created clusterrole.rbac.authorization.k8s.io/celsius-cluster-role created rolebinding.rbac.authorization.k8s.io/celsius-cluster-role-binding-ns-celsius created rolebinding.rbac.authorization.k8s.io/celsius-cluster-role-binding-ns-graphite created $ kubectl apply -f celsius-deployment.yaml deployment.apps/celsius created $ kubectl get pods -n celsius -w NAME READY STATUS RESTARTS AGE celsius-6ffb4f4bcc-jl579 0/1 ContainerCreating 0 16s celsius-6ffb4f4bcc-kh4dh 0/1 ContainerCreating 0 16s celsius-6ffb4f4bcc-qzt8w 0/1 ContainerCreating 0 16s celsius-6ffb4f4bcc-nfgnx 0/1 ContainerCreating 0 16s celsius-6ffb4f4bcc-nptk6 0/1 ContainerCreating 0 16s celsius-6ffb4f4bcc-kh4dh 1/1 Running 0 81s celsius-6ffb4f4bcc-nfgnx 1/1 Running 0 83s celsius-6ffb4f4bcc-nptk6 1/1 Running 0 84s celsius-6ffb4f4bcc-jl579 1/1 Running 0 86s celsius-6ffb4f4bcc-qzt8w 1/1 Running 0 89s $ kubectl logs -n celsius celsius-6ffb4f4bcc-qzt8w INFO[0000] Acquiring lock file /var/lib/rancher/k3s/data/.lock INFO[0000] Preparing data dir /var/lib/rancher/k3s/data/8c4262cf7fdd652cccb03a99a99fdffc96d9ad41d7e57af9eb08c7ac2867c72a After a couple of minutes you can check the graphite site again: When you would like to replace the pi4-temperature2graphite container with a newer version in the k3s cluster edit the file kubernetes/celsius-deployment.yaml and update the line containing the image name and replace the old version number with the new one, e.g. v1.8: image: ghcr.io/gdha/pi4-temperature2graphite:v1.8 To perform a rolling upgrade and watch the upgrade process execute the commands: $ kubectl replace -f kuvernetes/celsius-deployment.yaml $ kubectl get pods -n celsius -w NAME READY STATUS RESTARTS AGE celsius-6ffb4f4bcc-qzt8w 1/1 Running 3 (134m ago) 14d celsius-6ffb4f4bcc-kh4dh 1/1 Running 3 (134m ago) 14d celsius-6ffb4f4bcc-nptk6 1/1 Running 3 (134m ago) 14d celsius-6ffb4f4bcc-nfgnx 1/1 Running 3 (134m ago) 14d celsius-6ffb4f4bcc-jl579 1/1 Terminating 3 (135m ago) 14d celsius-779654f68d-9lmq8 0/1 ContainerCreating 0 10s celsius-779654f68d-d56q6 0/1 ContainerCreating 0 10s celsius-779654f68d-j6m6t 0/1 ContainerCreating 0 10s celsius-6ffb4f4bcc-jl579 0/1 Terminating 3 14d celsius-6ffb4f4bcc-jl579 0/1 Terminating 3 14d celsius-6ffb4f4bcc-jl579 0/1 Terminating 3 14d celsius-779654f68d-9lmq8 1/1 Running 0 50s celsius-6ffb4f4bcc-nptk6 1/1 Terminating 3 (135m ago) 14d celsius-779654f68d-jbxg8 0/1 Pending 0 0s celsius-779654f68d-jbxg8 0/1 Pending 0 0s celsius-779654f68d-jbxg8 0/1 ContainerCreating 0 0s celsius-779654f68d-j6m6t 1/1 Running 0 54s celsius-6ffb4f4bcc-kh4dh 1/1 Terminating 3 (135m ago) 14d celsius-779654f68d-q5rf9 0/1 Pending 0 0s celsius-779654f68d-q5rf9 0/1 Pending 0 0s celsius-779654f68d-q5rf9 0/1 ContainerCreating 0 0s celsius-779654f68d-d56q6 1/1 Running 0 57s celsius-6ffb4f4bcc-qzt8w 1/1 Terminating 3 (135m ago) 14d celsius-6ffb4f4bcc-nptk6 0/1 Terminating 3 14d celsius-6ffb4f4bcc-nptk6 0/1 Terminating 3 14d celsius-6ffb4f4bcc-nptk6 0/1 Terminating 3 14d celsius-779654f68d-jbxg8 1/1 Running 0 39s celsius-6ffb4f4bcc-nfgnx 1/1 Terminating 3 (135m ago) 14d celsius-6ffb4f4bcc-kh4dh 0/1 Terminating 3 14d celsius-6ffb4f4bcc-kh4dh 0/1 Terminating 3 14d celsius-6ffb4f4bcc-kh4dh 0/1 Terminating 3 14d celsius-6ffb4f4bcc-qzt8w 0/1 Terminating 3 14d celsius-6ffb4f4bcc-qzt8w 0/1 Terminating 3 14d celsius-6ffb4f4bcc-qzt8w 0/1 Terminating 3 14d celsius-779654f68d-q5rf9 1/1 Running 0 44s celsius-6ffb4f4bcc-nfgnx 0/1 Terminating 3 (136m ago) 14d celsius-6ffb4f4bcc-nfgnx 0/1 Terminating 3 (136m ago) 14d celsius-6ffb4f4bcc-nfgnx 0/1 Terminating 3 (136m ago) 14d References \u00b6 [1] pi4-graphite GitHub repository [2] pi4-temperature2graphite GitHub repository Edit history \u00b6 09/Feb/2023: adding the references 08/Feb/2023: improved the content of temperature2celsius section","title":"Raspberry Pi 4 Spin up graphite and temperature2celsius pods"},{"location":"pi-stories12/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories12/#raspberry_pi_4_cluster_series_-_spin_up_graphite_and_temperature2celsius_pods","text":"Graphite is an enterprise-ready monitoring tool that runs equally well on cheap hardware or Cloud infrastructure. Teams use Graphite to track the performance of their websites, applications, business services, and networked servers. It marked the start of a new generation of monitoring tools, making it easier than ever to store, retrieve, share, and visualize time-series data. Info copied from web-site graphiteapp.org .","title":"Raspberry Pi 4 cluster Series - Spin up graphite and temperature2celsius pods"},{"location":"pi-stories12/#spin_up_graphite_pod","text":"To start we need the sources to build of graphite pod, therefore, clone our pi4-graphite github repository [1]: git clone https://github.com/gdha/pi4-graphite.git You will find in the pi4-graphite directory a Dockerfile which will be used to create a new container image: To build the docker image on your Pi4 system execute the following command: $ ./build.sh v1.2 Login Succeeded Building graphite:v1.2 Sending build context to Docker daemon 137.2kB Step 1/11 : FROM graphiteapp/graphite-statsd:1.1.10-4 1.1.10-4: Pulling from graphiteapp/graphite-statsd 9981e73032c8: Pull complete de219cb0be19: Pull complete a152a03e3913: Pull complete 1e02b3e21032: Pull complete fb7d8007a803: Pull complete Digest: sha256:fb9eb6fdd8f6073dd4ff1acd2169b693d4633045017701713651befbc62fe9f5 Status: Downloaded newer image for graphiteapp/graphite-statsd:1.1.10-4 ... Successfully built afb29746df49 Successfully tagged ghcr.io/gdha/graphite:v1.2 Pushing graphite:v1.2 to GitHub Docker Container registry To start the graphite pod on our kubernetes cluster do the following: $ cd kubernetes $ kubectl apply -f ./graphite-namespace.yaml namespace/graphite created $ kubectl apply -f ./graphite-secret.yaml secret/graphite created $ kubectl apply -f ./ghcr-secret.yaml secret/dockerconfigjson-github-com created $ kubectl apply -f ./persistentvolumeclaim-graphite.yaml persistentvolumeclaim/graphite created $ kubectl apply -f ./statefulset_graphite.yaml statefulset.apps/graphite created $ kubectl apply -f ./service_graphite.yaml service/graphite-svc created Open a browser with URL http://n1:30080 to see the result of the tests:","title":"Spin up graphite pod"},{"location":"pi-stories12/#deploy_temperature2celsius_pods","text":"The purpose of this pod is to demonstrate that we can send information to our graphite application. We choose to send the temperatire in Celsius once a minute. Start with cloning our pi4-temperature2graphite github repository [2]: git clone https://github.com/gdha/pi4-temperature2graphite.git cd pi4-temperature2graphite The Dockerfile content is: # Dockerfile to create the container used to send the temperature of the RPI4 # to the graphite pod #FROM gdha/rpi-alpine-rootfs/alpine:v1.37 FROM alpine:latest LABEL org.opencontainers.image.sourcec=https://github.com/gdha/pi4-temperature2graphite LABEL org.opencontainers.image.description \"pi4-temperature2graphite build for the ARM64\" LABEL org.opencontainers.image.licenses \"GPL-3.0-or-later\" LABEL maintainer \"Gratien Dhaese <gratien.dhaese@gmail.com>\" COPY entrypoint.sh /entrypoint.sh COPY k3s /usr/bin/kubectl COPY api_query.sh /api_query.sh # Update and install dependencies RUN apk add --update nodejs npm curl RUN chmod a+x /entrypoint.sh \\ && chmod a+x /api_query.sh \\ && echo \"Europe/Brussels\" > /etc/timezone \\ && chmod 755 /root ENTRYPOINT [\"/entrypoint.sh\"] And, the center of the image is the /entrypoint.sh script: #!/bin/sh # As this script runs on Alpine (busybox) we cannot use bash syntax #set -e POD=$(echo $HOSTNAME) HOSTNAME=$(cat /etc/hostname) #SERVER=$(/usr/local/bin/kubectl -n graphite describe pod graphite-0 | grep -i node: | cut -d/ -f2) while true do # move the SERVER line inside the loop as at each restart the graphite pod gets a new IP address SERVER=$(/usr/bin/kubectl -n graphite get pods -o wide | tail -1 | awk '{print $6}') cpu_temp=$(cat /sys/class/thermal/thermal_zone0/temp) #cpu_temp=$(($cpu_temp/1000)) cpu_temp=$(expr $cpu_temp / 1000) echo \"carbon.celsius.$HOSTNAME $cpu_temp $(date +%s)\" | timeout 2 nc $SERVER 2003 if [[ $? -eq 0 ]] ; then echo $(date '+%F %T') {\"caller\":\"entrypoint.sh:16\",\"pod\":\"$POD\",\"level\":\"info\",\"msg\":\"temperature $cpu_temp\"} else echo $(date '+%F %T') {\"caller\":\"entrypoint.sh:18\",\"pod\":\"$POD\",\"level\":\"error\",\"msg\":\"cannot connect to server $SERVER\"} fi # echo $cpu_temp sleep 60 done To build the image use the build.sh script and it pushes the image to ghcr.io/gdha/pi4-temperature2graphite:v1.7 (which is also the latest). To build version v1.7 we did the following: ./build v1.7 To bring the pods alive on our kubernetes cluster go to the kubernetes directory and execute: $ kubectl apply -f celsius-namespace.yaml namespace/celsius created kubectl apply -f celsius-secret.yaml secret/celsius created $ kubectl apply -f ghcr-secret.yaml secret/dockerconfigjson-github-com created $ kubectl apply -f celsius-rbac.yaml erviceaccount/celsius-sa created clusterrole.rbac.authorization.k8s.io/celsius-cluster-role created rolebinding.rbac.authorization.k8s.io/celsius-cluster-role-binding-ns-celsius created rolebinding.rbac.authorization.k8s.io/celsius-cluster-role-binding-ns-graphite created $ kubectl apply -f celsius-deployment.yaml deployment.apps/celsius created $ kubectl get pods -n celsius -w NAME READY STATUS RESTARTS AGE celsius-6ffb4f4bcc-jl579 0/1 ContainerCreating 0 16s celsius-6ffb4f4bcc-kh4dh 0/1 ContainerCreating 0 16s celsius-6ffb4f4bcc-qzt8w 0/1 ContainerCreating 0 16s celsius-6ffb4f4bcc-nfgnx 0/1 ContainerCreating 0 16s celsius-6ffb4f4bcc-nptk6 0/1 ContainerCreating 0 16s celsius-6ffb4f4bcc-kh4dh 1/1 Running 0 81s celsius-6ffb4f4bcc-nfgnx 1/1 Running 0 83s celsius-6ffb4f4bcc-nptk6 1/1 Running 0 84s celsius-6ffb4f4bcc-jl579 1/1 Running 0 86s celsius-6ffb4f4bcc-qzt8w 1/1 Running 0 89s $ kubectl logs -n celsius celsius-6ffb4f4bcc-qzt8w INFO[0000] Acquiring lock file /var/lib/rancher/k3s/data/.lock INFO[0000] Preparing data dir /var/lib/rancher/k3s/data/8c4262cf7fdd652cccb03a99a99fdffc96d9ad41d7e57af9eb08c7ac2867c72a After a couple of minutes you can check the graphite site again: When you would like to replace the pi4-temperature2graphite container with a newer version in the k3s cluster edit the file kubernetes/celsius-deployment.yaml and update the line containing the image name and replace the old version number with the new one, e.g. v1.8: image: ghcr.io/gdha/pi4-temperature2graphite:v1.8 To perform a rolling upgrade and watch the upgrade process execute the commands: $ kubectl replace -f kuvernetes/celsius-deployment.yaml $ kubectl get pods -n celsius -w NAME READY STATUS RESTARTS AGE celsius-6ffb4f4bcc-qzt8w 1/1 Running 3 (134m ago) 14d celsius-6ffb4f4bcc-kh4dh 1/1 Running 3 (134m ago) 14d celsius-6ffb4f4bcc-nptk6 1/1 Running 3 (134m ago) 14d celsius-6ffb4f4bcc-nfgnx 1/1 Running 3 (134m ago) 14d celsius-6ffb4f4bcc-jl579 1/1 Terminating 3 (135m ago) 14d celsius-779654f68d-9lmq8 0/1 ContainerCreating 0 10s celsius-779654f68d-d56q6 0/1 ContainerCreating 0 10s celsius-779654f68d-j6m6t 0/1 ContainerCreating 0 10s celsius-6ffb4f4bcc-jl579 0/1 Terminating 3 14d celsius-6ffb4f4bcc-jl579 0/1 Terminating 3 14d celsius-6ffb4f4bcc-jl579 0/1 Terminating 3 14d celsius-779654f68d-9lmq8 1/1 Running 0 50s celsius-6ffb4f4bcc-nptk6 1/1 Terminating 3 (135m ago) 14d celsius-779654f68d-jbxg8 0/1 Pending 0 0s celsius-779654f68d-jbxg8 0/1 Pending 0 0s celsius-779654f68d-jbxg8 0/1 ContainerCreating 0 0s celsius-779654f68d-j6m6t 1/1 Running 0 54s celsius-6ffb4f4bcc-kh4dh 1/1 Terminating 3 (135m ago) 14d celsius-779654f68d-q5rf9 0/1 Pending 0 0s celsius-779654f68d-q5rf9 0/1 Pending 0 0s celsius-779654f68d-q5rf9 0/1 ContainerCreating 0 0s celsius-779654f68d-d56q6 1/1 Running 0 57s celsius-6ffb4f4bcc-qzt8w 1/1 Terminating 3 (135m ago) 14d celsius-6ffb4f4bcc-nptk6 0/1 Terminating 3 14d celsius-6ffb4f4bcc-nptk6 0/1 Terminating 3 14d celsius-6ffb4f4bcc-nptk6 0/1 Terminating 3 14d celsius-779654f68d-jbxg8 1/1 Running 0 39s celsius-6ffb4f4bcc-nfgnx 1/1 Terminating 3 (135m ago) 14d celsius-6ffb4f4bcc-kh4dh 0/1 Terminating 3 14d celsius-6ffb4f4bcc-kh4dh 0/1 Terminating 3 14d celsius-6ffb4f4bcc-kh4dh 0/1 Terminating 3 14d celsius-6ffb4f4bcc-qzt8w 0/1 Terminating 3 14d celsius-6ffb4f4bcc-qzt8w 0/1 Terminating 3 14d celsius-6ffb4f4bcc-qzt8w 0/1 Terminating 3 14d celsius-779654f68d-q5rf9 1/1 Running 0 44s celsius-6ffb4f4bcc-nfgnx 0/1 Terminating 3 (136m ago) 14d celsius-6ffb4f4bcc-nfgnx 0/1 Terminating 3 (136m ago) 14d celsius-6ffb4f4bcc-nfgnx 0/1 Terminating 3 (136m ago) 14d","title":"Deploy temperature2celsius pods"},{"location":"pi-stories12/#references","text":"[1] pi4-graphite GitHub repository [2] pi4-temperature2graphite GitHub repository","title":"References"},{"location":"pi-stories12/#edit_history","text":"09/Feb/2023: adding the references 08/Feb/2023: improved the content of temperature2celsius section","title":"Edit history"},{"location":"pi-stories13/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Setup monitoring \u00b6 This story is mostly based on K3s Monitoring . Basically, we need to install Prometheus and many other components to be able to feed Grafana. Our goal is to get something like: To start we have to download the pi4-monitoring GitHub project : $ git clone https://github.com/gdha/pi4-monitoring.git $ cd pi4-monitoring $ ls grafana images kubelet kube-state-metrics LICENSE longhorn-servicemonitor.yaml monitoring-namespace.yaml node-exporter prometheus prometheus-operator readme.md traefik Prometheus Operator \u00b6 One instance that will help us provision Prometheus, and some of its components. It extends the Kubernetes API, so that when we create some YAML deployments it will look as if we\u2019re telling Kubernetes to deploy something, but it\u2019s actually telling Prometheus Operator to do it for us. Official documentation: Prometheus Operator We executed the following steps to install the Prometheus Operator [1] - is optional as it is already present and prepared under the prometheus-operator directory: $ cd prometheus-operator $ wget https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/bundle.yaml Followed by editing with sed the bundle.yaml file to replace the default namespace by monitoring (which is the namespace in where we want to built our monitoring tools for kubernetes). Then we first create the namespace and apply the bundle.yaml file as seen below: $ grep 'namespace: default' bundle.yaml namespace: default namespace: default namespace: default namespace: default $ sed -i 's/namespace: default/namespace: monitoring/g' bundle.yaml $ grep 'namespace: ' bundle.yaml a `namespace: <object namespace>` matcher.' namespace: monitoring namespace: monitoring namespace: monitoring namespace: monitoring To create the prometheus-operator we first need to create the namespace monitoring : $ kubectl create -f ../monitoring-namespace.yaml namespace/monitoring created Then, we can create the prometheus-operator as follow: $ cd ~/projects/pi4-monitoring/prometheus-operator $ kubectl apply --server-side -f bundle.yaml customresourcedefinition.apiextensions.k8s.io/alertmanagerconfigs.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com serverside-applied clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator serverside-applied clusterrole.rbac.authorization.k8s.io/prometheus-operator serverside-applied deployment.apps/prometheus-operator serverside-applied serviceaccount/prometheus-operator serverside-applied service/prometheus-operator serverside-applied $ kubectl get pods -n monitoring NAME READY STATUS RESTARTS AGE prometheus-operator-6d56dc87f4-tg5qh 1/1 Running 0 25s Next step is to prepare the service monitors. Install Service Monitors \u00b6 Prometheus Node Exporter \u00b6 We will install the Prometheus Node Exporter [2] service which is a daemonset to collect metrics from individual cluster nodes, and many other details. The installation is quite simple . $ cd ~/projects/pi4-monitoring $ kubectl apply -f node-exporter/ clusterrolebinding.rbac.authorization.k8s.io/node-exporter created clusterrole.rbac.authorization.k8s.io/node-exporter created daemonset.apps/node-exporter created serviceaccount/node-exporter created servicemonitor.monitoring.coreos.com/node-exporter created service/node-exporter created This will create all permissions, and deploy the pod with the application Node Exporter, that will read metrics from Linux. After doing so, you should see node-exporter-xxxx pods in the monitoring namespace; I have 5 nodes, so it\u2019s there 5 times. $ kubectl get pods -n monitoring -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES prometheus-operator-6d56dc87f4-tg5qh 1/1 Running 0 10m 10.42.2.197 n2 <none> <none> node-exporter-flgs7 2/2 Running 0 61s 192.168.0.204 n4 <none> <none> node-exporter-r4wfz 2/2 Running 0 61s 192.168.0.202 n2 <none> <none> node-exporter-jblkk 2/2 Running 0 61s 192.168.0.201 n1 <none> <none> node-exporter-fhkw6 2/2 Running 0 61s 192.168.0.203 n3 <none> <none> node-exporter-jwt8k 2/2 Running 0 61s 192.168.0.205 n5 <none> <none> Kube State Metrics \u00b6 This is a simple service that listens to the Kubernetes API, and generates metrics about the state of the objects. Link to official GitHub: kube-state-metrics . When we download the GitHub project pi4-monitoring then cd ~/projects/pi4monitoring and execute: $ kubectl apply -f kube-state-metrics/ $ kubectl get pods -n monitoring | grep kube-state kube-state-metrics-6f8578cffd-cmks6 1/1 Running 19 (93m ago) 45d Kubelet \u00b6 Kubelet, in case you did not know, is an essential part of Kubernetes\u2019 control plane, and is also something that exposes Prometheus metrics by default in the port 10255. And as before: $ cd ~/projects/pi4monitoring $ kubectl apply -f kubelet-servicemonitor.yaml servicemonitor.monitoring.coreos.com/kubelet created Traefik \u00b6 I do not use Traefik much in my setup, but it is there, and it also exposes Prometheus-ready data, so why not... $ cd ~/projects/pi4monitoring $ kubectl apply -f traefik-servicemonitor.yaml servicemonitor.monitoring.coreos.com/traefik created $ kubectl get servicemonitors.monitoring.coreos.com -n monitoring NAME AGE node-exporter 45d kube-state-metrics 45d kubelet 45d traefik 45d Install Prometheus \u00b6 Now, we are going to deploy a single instance of Prometheus. Normally, you would/should deploy multiple instances spread throughout the cluster. For example, one instance dedicated to monitor just Kubernetes API, the next dedicated to monitor nodes, and so on... As with many things in the Kubernetes world, there is no specific way things should look \ud83d\ude42, so to save resources, we will deploy just one. To deploy a single instance of prometheus perform the following actions: $ cd ~/projects/pi4-monitoring $ kubectl apply -f prometheus/ clusterrole.rbac.authorization.k8s.io/prometheus created clusterrolebinding.rbac.authorization.k8s.io/prometheus created serviceaccount/prometheus created service/prometheus-external created service/prometheus created prometheus.monitoring.coreos.com/prometheus-persistant created When you inside the prometheus.yaml file you will the Service Monitors we created: serviceMonitorSelector: matchExpressions: - key: name operator: In values: - longhorn-prometheus-servicemonitor - kube-state-metrics - node-exporter - kubelet - traefik Furthermore, as storage we will be using the longhorn volumes as you can see: storage: volumeClaimTemplate: spec: accessModes: - ReadWriteOnce storageClassName: longhorn resources: requests: storage: 20Gi The longhorn volume created is: The prometheus-service-ext.yaml file defines the loadbalancer piece. See: $ kubectl get svc -n monitoring | grep external prometheus-external LoadBalancer 10.43.53.220 192.168.0.232 9090:31862/TCP 45d When you browse to URL: http://192.168.0.232:9090/ you will get to see : You can reverse the background colors with the icons on the right corner (I prefer black as background). When you select the \"Service Discovery\" under the status tab you will see the following screen proofing we receive information about our kubernetes cluster: Longhorn service monitor \u00b6 Our storage provisioner Longhorn, that we deployed somewhere near the start of this whole K3s Kubernetes cluster setup, also natively provides data for Prometheus. Create in the folder monitoring , that we will put most of our configs in, the file [longhorn-servicemonitor.yaml](https://github.com/gdha/pi4-monitoring/blob/master/longhorn-servicemonitor.yaml) . As you can see, we are not talking to Kubernetes API (we are... but...), but to apiVersion: monitoring.coreos.com/v1 , so we are basically telling Prometheus Operator to create something for us. In this case it\u2019s kind: ServiceMonitor . This should be clear, metadata: -> namespace: monitoring , we are telling it to deploy into our monitoring namespace. The rest under spec: is basically telling what app the Service Monitor should \"bind to\". It\u2019s looking for app: longhorn-manager in namespace longhorn-system and port: manager . This port could be a port number, but it also can have a name, so in this case it\u2019s named manager. This is the longhorn-manager we are targeting. $ kubectl get daemonset -n longhorn-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE engine-image-ei-fc06c6fb 5 5 5 5 5 <none> 48d longhorn-manager 5 5 5 5 5 <none> 48d longhorn-csi-plugin 5 5 5 5 5 <none> 48d To describea the daemonset of longhorn-manager execute: $ kubectl describe daemonset longhorn-manager -n longhorn-system | grep Port Port: <none> Host Port: <none> Port: 9500/TCP Host Port: 0/TCP Alright, now we can move on the grafana. Install grafana \u00b6 Our grafana pod will also use a longhorn device as defined under file: cat grafana-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: longhorn-grafana-pvc namespace: monitoring spec: accessModes: - ReadWriteOnce storageClassName: longhorn resources: requests: storage: 10Gi To install the pod just run: $ cd ~/projects/pi4-monitoring $ kubectl apply -f grafana/ deployment.apps/grafana created persistentvolumeclaim/longhorn-grafana-pvc created service/grafana created serviceaccount/grafana created $ kubectl get endpoints -n monitoring NAME ENDPOINTS AGE prometheus-operator 10.42.2.225:8080 5d16h kube-state-metrics 10.42.2.220:8081,10.42.2.220:8080 5d16h prometheus-operated 10.42.2.233:9090 5d16h prometheus 10.42.2.233:9090 5d16h prometheus-external 10.42.2.233:9090 5d16h node-exporter 192.168.0.201:9100,192.168.0.202:9100,192.168.0.203:9100 + 2 more... 5d16h grafana 10.42.1.21:3000 2m44s $ kubectl get svc -n monitoring NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE prometheus-operator ClusterIP None <none> 8080/TCP 63d node-exporter ClusterIP None <none> 9100/TCP 62d kube-state-metrics ClusterIP None <none> 8080/TCP,8081/TCP 62d prometheus-external LoadBalancer 10.43.53.220 192.168.0.232 9090:31862/TCP 62d prometheus ClusterIP 10.43.78.140 <none> 9090/TCP 62d prometheus-operated ClusterIP None <none> 9090/TCP 62d grafana LoadBalancer 10.43.108.229 192.168.0.233 3000:32251/TCP 62d Open a browser and use url http://192.168.0.233:3000/ and login with the default admin account with first time password admin. On the left pane we can import grafana graphs with \"+\" -> create -> import From GrafanaLabs we can import some examples: Kubernetes Nodes from GrafanaLabs - copy the ID into your clipboard: And, paste the ID into: and, perform the import in your grafana window. You will see the results in an instance: Another good example is Kubernetes Cluster from GrafanaLabs to import into your grafana dashboard. Even better is to create your own dashboard, in our case the already existing graphite celsius graphs: Try it out as it is not that complicated as an exercise and you will be proud of your first designed dashboard: References \u00b6 [1] Prometheus Operator GitHub sources [2] Prometheus Node Exporter","title":"Raspberry Pi 4 Setup monitoring"},{"location":"pi-stories13/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories13/#raspberry_pi_4_cluster_series_-_setup_monitoring","text":"This story is mostly based on K3s Monitoring . Basically, we need to install Prometheus and many other components to be able to feed Grafana. Our goal is to get something like: To start we have to download the pi4-monitoring GitHub project : $ git clone https://github.com/gdha/pi4-monitoring.git $ cd pi4-monitoring $ ls grafana images kubelet kube-state-metrics LICENSE longhorn-servicemonitor.yaml monitoring-namespace.yaml node-exporter prometheus prometheus-operator readme.md traefik","title":"Raspberry Pi 4 cluster Series - Setup monitoring"},{"location":"pi-stories13/#prometheus_operator","text":"One instance that will help us provision Prometheus, and some of its components. It extends the Kubernetes API, so that when we create some YAML deployments it will look as if we\u2019re telling Kubernetes to deploy something, but it\u2019s actually telling Prometheus Operator to do it for us. Official documentation: Prometheus Operator We executed the following steps to install the Prometheus Operator [1] - is optional as it is already present and prepared under the prometheus-operator directory: $ cd prometheus-operator $ wget https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/bundle.yaml Followed by editing with sed the bundle.yaml file to replace the default namespace by monitoring (which is the namespace in where we want to built our monitoring tools for kubernetes). Then we first create the namespace and apply the bundle.yaml file as seen below: $ grep 'namespace: default' bundle.yaml namespace: default namespace: default namespace: default namespace: default $ sed -i 's/namespace: default/namespace: monitoring/g' bundle.yaml $ grep 'namespace: ' bundle.yaml a `namespace: <object namespace>` matcher.' namespace: monitoring namespace: monitoring namespace: monitoring namespace: monitoring To create the prometheus-operator we first need to create the namespace monitoring : $ kubectl create -f ../monitoring-namespace.yaml namespace/monitoring created Then, we can create the prometheus-operator as follow: $ cd ~/projects/pi4-monitoring/prometheus-operator $ kubectl apply --server-side -f bundle.yaml customresourcedefinition.apiextensions.k8s.io/alertmanagerconfigs.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com serverside-applied clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator serverside-applied clusterrole.rbac.authorization.k8s.io/prometheus-operator serverside-applied deployment.apps/prometheus-operator serverside-applied serviceaccount/prometheus-operator serverside-applied service/prometheus-operator serverside-applied $ kubectl get pods -n monitoring NAME READY STATUS RESTARTS AGE prometheus-operator-6d56dc87f4-tg5qh 1/1 Running 0 25s Next step is to prepare the service monitors.","title":"Prometheus Operator"},{"location":"pi-stories13/#install_service_monitors","text":"","title":"Install Service Monitors"},{"location":"pi-stories13/#prometheus_node_exporter","text":"We will install the Prometheus Node Exporter [2] service which is a daemonset to collect metrics from individual cluster nodes, and many other details. The installation is quite simple . $ cd ~/projects/pi4-monitoring $ kubectl apply -f node-exporter/ clusterrolebinding.rbac.authorization.k8s.io/node-exporter created clusterrole.rbac.authorization.k8s.io/node-exporter created daemonset.apps/node-exporter created serviceaccount/node-exporter created servicemonitor.monitoring.coreos.com/node-exporter created service/node-exporter created This will create all permissions, and deploy the pod with the application Node Exporter, that will read metrics from Linux. After doing so, you should see node-exporter-xxxx pods in the monitoring namespace; I have 5 nodes, so it\u2019s there 5 times. $ kubectl get pods -n monitoring -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES prometheus-operator-6d56dc87f4-tg5qh 1/1 Running 0 10m 10.42.2.197 n2 <none> <none> node-exporter-flgs7 2/2 Running 0 61s 192.168.0.204 n4 <none> <none> node-exporter-r4wfz 2/2 Running 0 61s 192.168.0.202 n2 <none> <none> node-exporter-jblkk 2/2 Running 0 61s 192.168.0.201 n1 <none> <none> node-exporter-fhkw6 2/2 Running 0 61s 192.168.0.203 n3 <none> <none> node-exporter-jwt8k 2/2 Running 0 61s 192.168.0.205 n5 <none> <none>","title":"Prometheus Node Exporter"},{"location":"pi-stories13/#kube_state_metrics","text":"This is a simple service that listens to the Kubernetes API, and generates metrics about the state of the objects. Link to official GitHub: kube-state-metrics . When we download the GitHub project pi4-monitoring then cd ~/projects/pi4monitoring and execute: $ kubectl apply -f kube-state-metrics/ $ kubectl get pods -n monitoring | grep kube-state kube-state-metrics-6f8578cffd-cmks6 1/1 Running 19 (93m ago) 45d","title":"Kube State Metrics"},{"location":"pi-stories13/#kubelet","text":"Kubelet, in case you did not know, is an essential part of Kubernetes\u2019 control plane, and is also something that exposes Prometheus metrics by default in the port 10255. And as before: $ cd ~/projects/pi4monitoring $ kubectl apply -f kubelet-servicemonitor.yaml servicemonitor.monitoring.coreos.com/kubelet created","title":"Kubelet"},{"location":"pi-stories13/#traefik","text":"I do not use Traefik much in my setup, but it is there, and it also exposes Prometheus-ready data, so why not... $ cd ~/projects/pi4monitoring $ kubectl apply -f traefik-servicemonitor.yaml servicemonitor.monitoring.coreos.com/traefik created $ kubectl get servicemonitors.monitoring.coreos.com -n monitoring NAME AGE node-exporter 45d kube-state-metrics 45d kubelet 45d traefik 45d","title":"Traefik"},{"location":"pi-stories13/#install_prometheus","text":"Now, we are going to deploy a single instance of Prometheus. Normally, you would/should deploy multiple instances spread throughout the cluster. For example, one instance dedicated to monitor just Kubernetes API, the next dedicated to monitor nodes, and so on... As with many things in the Kubernetes world, there is no specific way things should look \ud83d\ude42, so to save resources, we will deploy just one. To deploy a single instance of prometheus perform the following actions: $ cd ~/projects/pi4-monitoring $ kubectl apply -f prometheus/ clusterrole.rbac.authorization.k8s.io/prometheus created clusterrolebinding.rbac.authorization.k8s.io/prometheus created serviceaccount/prometheus created service/prometheus-external created service/prometheus created prometheus.monitoring.coreos.com/prometheus-persistant created When you inside the prometheus.yaml file you will the Service Monitors we created: serviceMonitorSelector: matchExpressions: - key: name operator: In values: - longhorn-prometheus-servicemonitor - kube-state-metrics - node-exporter - kubelet - traefik Furthermore, as storage we will be using the longhorn volumes as you can see: storage: volumeClaimTemplate: spec: accessModes: - ReadWriteOnce storageClassName: longhorn resources: requests: storage: 20Gi The longhorn volume created is: The prometheus-service-ext.yaml file defines the loadbalancer piece. See: $ kubectl get svc -n monitoring | grep external prometheus-external LoadBalancer 10.43.53.220 192.168.0.232 9090:31862/TCP 45d When you browse to URL: http://192.168.0.232:9090/ you will get to see : You can reverse the background colors with the icons on the right corner (I prefer black as background). When you select the \"Service Discovery\" under the status tab you will see the following screen proofing we receive information about our kubernetes cluster:","title":"Install Prometheus"},{"location":"pi-stories13/#longhorn_service_monitor","text":"Our storage provisioner Longhorn, that we deployed somewhere near the start of this whole K3s Kubernetes cluster setup, also natively provides data for Prometheus. Create in the folder monitoring , that we will put most of our configs in, the file [longhorn-servicemonitor.yaml](https://github.com/gdha/pi4-monitoring/blob/master/longhorn-servicemonitor.yaml) . As you can see, we are not talking to Kubernetes API (we are... but...), but to apiVersion: monitoring.coreos.com/v1 , so we are basically telling Prometheus Operator to create something for us. In this case it\u2019s kind: ServiceMonitor . This should be clear, metadata: -> namespace: monitoring , we are telling it to deploy into our monitoring namespace. The rest under spec: is basically telling what app the Service Monitor should \"bind to\". It\u2019s looking for app: longhorn-manager in namespace longhorn-system and port: manager . This port could be a port number, but it also can have a name, so in this case it\u2019s named manager. This is the longhorn-manager we are targeting. $ kubectl get daemonset -n longhorn-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE engine-image-ei-fc06c6fb 5 5 5 5 5 <none> 48d longhorn-manager 5 5 5 5 5 <none> 48d longhorn-csi-plugin 5 5 5 5 5 <none> 48d To describea the daemonset of longhorn-manager execute: $ kubectl describe daemonset longhorn-manager -n longhorn-system | grep Port Port: <none> Host Port: <none> Port: 9500/TCP Host Port: 0/TCP Alright, now we can move on the grafana.","title":"Longhorn service monitor"},{"location":"pi-stories13/#install_grafana","text":"Our grafana pod will also use a longhorn device as defined under file: cat grafana-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: longhorn-grafana-pvc namespace: monitoring spec: accessModes: - ReadWriteOnce storageClassName: longhorn resources: requests: storage: 10Gi To install the pod just run: $ cd ~/projects/pi4-monitoring $ kubectl apply -f grafana/ deployment.apps/grafana created persistentvolumeclaim/longhorn-grafana-pvc created service/grafana created serviceaccount/grafana created $ kubectl get endpoints -n monitoring NAME ENDPOINTS AGE prometheus-operator 10.42.2.225:8080 5d16h kube-state-metrics 10.42.2.220:8081,10.42.2.220:8080 5d16h prometheus-operated 10.42.2.233:9090 5d16h prometheus 10.42.2.233:9090 5d16h prometheus-external 10.42.2.233:9090 5d16h node-exporter 192.168.0.201:9100,192.168.0.202:9100,192.168.0.203:9100 + 2 more... 5d16h grafana 10.42.1.21:3000 2m44s $ kubectl get svc -n monitoring NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE prometheus-operator ClusterIP None <none> 8080/TCP 63d node-exporter ClusterIP None <none> 9100/TCP 62d kube-state-metrics ClusterIP None <none> 8080/TCP,8081/TCP 62d prometheus-external LoadBalancer 10.43.53.220 192.168.0.232 9090:31862/TCP 62d prometheus ClusterIP 10.43.78.140 <none> 9090/TCP 62d prometheus-operated ClusterIP None <none> 9090/TCP 62d grafana LoadBalancer 10.43.108.229 192.168.0.233 3000:32251/TCP 62d Open a browser and use url http://192.168.0.233:3000/ and login with the default admin account with first time password admin. On the left pane we can import grafana graphs with \"+\" -> create -> import From GrafanaLabs we can import some examples: Kubernetes Nodes from GrafanaLabs - copy the ID into your clipboard: And, paste the ID into: and, perform the import in your grafana window. You will see the results in an instance: Another good example is Kubernetes Cluster from GrafanaLabs to import into your grafana dashboard. Even better is to create your own dashboard, in our case the already existing graphite celsius graphs: Try it out as it is not that complicated as an exercise and you will be proud of your first designed dashboard:","title":"Install grafana"},{"location":"pi-stories13/#references","text":"[1] Prometheus Operator GitHub sources [2] Prometheus Node Exporter","title":"References"},{"location":"pi-stories14/","text":"PI4 Stories Raspberry Pi 4 cluster Series - Setup logging with Loki \u00b6 This story is mostly based on Logging Logs [1] article. We have our Prometheus and Grafana on k3s cluster if you followed my guides. But Prometheus is not able to get logs from k3s nodes, containers, Kubernetes API, and it's not made of that kind of monitoring. We need Loki + Promtail, you can find all info on their Grafana Loki and Promtail. Loki - or also known as Grafana Loki - log aggregation system, it does not index the contents of the logs, but rather a set of labels for each log stream. In simple terms, it's the app that accepts logs from everything and store it. Promtail - is a tool that reads the logs from various sources like OS, Containers, Kubernetes API server etc... and push them to Loki. We will be using Arkade to install loki, therefore, we first need to install it. Install Arkade \u00b6 This is basically Helm, just made even simpler. More info about Arkade: https://github.com/alexellis/arkade $ curl -SLsf https://dl.get-arkade.dev/ | sudo sh aarch64 Downloading package https://github.com/alexellis/arkade/releases/download/0.8.62/arkade-arm64 as /tmp/arkade-arm64 Download complete. Running with sufficient permissions to attempt to move arkade to /usr/local/bin New version of arkade installed to /usr/local/bin Creating alias 'ark' for 'arkade'. _ _ __ _ _ __| | ____ _ __| | ___ / _` | '__| |/ / _` |/ _` |/ _ \\ | (_| | | | < (_| | (_| | __/ \\__,_|_| |_|\\_\\__,_|\\__,_|\\___| Open Source Marketplace For Developer Tools Version: 0.8.62 Git Commit: 003e8716bbc987c412643af3a90cf87ef778bd2e \ud83d\udc33 arkade needs your support: https://github.com/sponsors/alexellis gdha@n1:~/projects/pi4-logging$ ls /usr/local/bin ark arkade crictl golangci-lint helm k3s k3s-killall.sh k3s-uninstall.sh kubectl kube-linter To upgrade arkade just re-run above command. Installation of loki and promtail \u00b6 Before we start we have to make sure that the namespace has been created: gdha@n1:~/projects/pi4-logging$ kubectl apply -f logging-namespace.yaml namespace/logging created Then, we can use the installed arkade executable to install loki gdha@n1:~/projects/pi4-logging$ arkade install loki -n logging --persistence 2023/02/03 15:24:10 Using Kubeconfig: /home/gdha/.kube/config [Warning] unable to create namespace logging, may already exist: Error from server (AlreadyExists): namespaces \"logging\" already exists Client: aarch64, Linux 2023/02/03 15:24:10 User dir established as: /home/gdha/.arkade/ 2023/02/03 15:24:10 Looking up version for helm 2023/02/03 15:24:10 Found: v3.11.0 Downloading: https://get.helm.sh/helm-v3.11.0-linux-arm64.tar.gz /tmp/helm-v3.11.0-linux-arm64.tar.gz written. 2023/02/03 15:24:14 Looking up version for helm 2023/02/03 15:24:14 Found: v3.11.0 2023/02/03 15:24:15 Extracted: /tmp/helm 2023/02/03 15:24:15 Copying /tmp/helm to /home/gdha/.arkade/bin/helm Downloaded to: /home/gdha/.arkade/bin/helm helm \"grafana\" has been added to your repositories Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"metallb\" chart repository ...Successfully got an update from the \"longhorn\" chart repository ...Successfully got an update from the \"traefik\" chart repository ...Successfully got an update from the \"kiwigrid\" chart repository ...Successfully got an update from the \"grafana\" chart repository Update Complete. \u2388Happy Helming!\u2388 VALUES values.yaml Command: /home/gdha/.arkade/bin/helm [upgrade --install loki-stack grafana/loki-stack --namespace logging --values /tmp/charts/loki-stack/values.yaml --set loki.persistence.enabled=true] Release \"loki-stack\" does not exist. Installing it now. NAME: loki-stack LAST DEPLOYED: Fri Feb 3 15:24:22 2023 NAMESPACE: logging STATUS: deployed REVISION: 1 NOTES: The Loki stack has been deployed to your cluster. Loki can now be added as a datasource in Grafana. See http://docs.grafana.org/features/datasources/loki/ for more detail. ======================================================================= = loki has been installed. = ======================================================================= # Get started with loki here: # https://grafana.com/docs/loki/latest # See how to integrate loki with Grafana here # https://grafana.com/docs/loki/latest/getting-started/grafana # Check loki's logs with: kubectl logs svc/loki-stack kubectl logs svc/loki-stack-headless # If you installed with Grafana you can access the dashboard with the username \"admin\" and password shown below # To get password kubectl get secret loki-stack-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo # Forward traffic to your localhost kubectl port-forward service/loki-stack-grafana 3000:80 whale arkade needs your support: https://github.com/sponsors/alexellis Now, verify if the pods have been created: gdha@n1:~/projects/pi4-logging$ kubectl get pods -n logging NAME READY STATUS RESTARTS AGE loki-stack-0 0/1 Running 0 114s loki-stack-promtail-d45tz 1/1 Running 0 114s loki-stack-promtail-qv75h 1/1 Running 0 114s loki-stack-promtail-rhcr9 1/1 Running 0 114s loki-stack-promtail-dwz7s 1/1 Running 0 114s loki-stack-promtail-4qwlg 1/1 Running 0 114s As last we still need to apply the patch: gdha@n1:~/projects/pi4-logging$ cat patch.yaml spec: template: spec: tolerations: - operator: Exists gdha@n1:~/projects/pi4-logging$ kubectl patch daemonset loki-stack-promtail -n logging --patch \"$(cat patch.yaml)\" daemonset.apps/loki-stack-promtail patched Check if the services are available for usage: gdha@n1:~/projects/pi4-logging$ kubectl get svc -n logging NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE loki-stack-headless ClusterIP None <none> 3100/TCP 6m39s loki-stack-memberlist ClusterIP None <none> 7946/TCP 6m39s loki-stack ClusterIP 10.43.220.29 <none> 3100/TCP 6m39s To view the logs we need to login onto our Grafana site and via configure - data sources - add data sources and search for Loki and select \"loki\". On the next screen fill in the URL section the IP address found with 'loki-stack' service. In our case it is http://10.43.220.29:3100 . Thereafter, it is kust a matter of pressing \"Save & Test\" button. In the side bar of Grafana click on the \"Explore\" button and select \"Loki\" in the upper left corner (of the service to use). Use Loki's \"Log Labels\" to select what you want to see - just play with it... Your imagination is the limit with Loki/Grafana, e.g. References \u00b6 [1] k3s Monitoring - Logging Logs","title":"Raspberry Pi 4 Setup logging with Loki"},{"location":"pi-stories14/#raspberry_pi_4_cluster_series_-_setup_logging_with_loki","text":"This story is mostly based on Logging Logs [1] article. We have our Prometheus and Grafana on k3s cluster if you followed my guides. But Prometheus is not able to get logs from k3s nodes, containers, Kubernetes API, and it's not made of that kind of monitoring. We need Loki + Promtail, you can find all info on their Grafana Loki and Promtail. Loki - or also known as Grafana Loki - log aggregation system, it does not index the contents of the logs, but rather a set of labels for each log stream. In simple terms, it's the app that accepts logs from everything and store it. Promtail - is a tool that reads the logs from various sources like OS, Containers, Kubernetes API server etc... and push them to Loki. We will be using Arkade to install loki, therefore, we first need to install it.","title":"Raspberry Pi 4 cluster Series - Setup logging with Loki"},{"location":"pi-stories14/#install_arkade","text":"This is basically Helm, just made even simpler. More info about Arkade: https://github.com/alexellis/arkade $ curl -SLsf https://dl.get-arkade.dev/ | sudo sh aarch64 Downloading package https://github.com/alexellis/arkade/releases/download/0.8.62/arkade-arm64 as /tmp/arkade-arm64 Download complete. Running with sufficient permissions to attempt to move arkade to /usr/local/bin New version of arkade installed to /usr/local/bin Creating alias 'ark' for 'arkade'. _ _ __ _ _ __| | ____ _ __| | ___ / _` | '__| |/ / _` |/ _` |/ _ \\ | (_| | | | < (_| | (_| | __/ \\__,_|_| |_|\\_\\__,_|\\__,_|\\___| Open Source Marketplace For Developer Tools Version: 0.8.62 Git Commit: 003e8716bbc987c412643af3a90cf87ef778bd2e \ud83d\udc33 arkade needs your support: https://github.com/sponsors/alexellis gdha@n1:~/projects/pi4-logging$ ls /usr/local/bin ark arkade crictl golangci-lint helm k3s k3s-killall.sh k3s-uninstall.sh kubectl kube-linter To upgrade arkade just re-run above command.","title":"Install Arkade"},{"location":"pi-stories14/#installation_of_loki_and_promtail","text":"Before we start we have to make sure that the namespace has been created: gdha@n1:~/projects/pi4-logging$ kubectl apply -f logging-namespace.yaml namespace/logging created Then, we can use the installed arkade executable to install loki gdha@n1:~/projects/pi4-logging$ arkade install loki -n logging --persistence 2023/02/03 15:24:10 Using Kubeconfig: /home/gdha/.kube/config [Warning] unable to create namespace logging, may already exist: Error from server (AlreadyExists): namespaces \"logging\" already exists Client: aarch64, Linux 2023/02/03 15:24:10 User dir established as: /home/gdha/.arkade/ 2023/02/03 15:24:10 Looking up version for helm 2023/02/03 15:24:10 Found: v3.11.0 Downloading: https://get.helm.sh/helm-v3.11.0-linux-arm64.tar.gz /tmp/helm-v3.11.0-linux-arm64.tar.gz written. 2023/02/03 15:24:14 Looking up version for helm 2023/02/03 15:24:14 Found: v3.11.0 2023/02/03 15:24:15 Extracted: /tmp/helm 2023/02/03 15:24:15 Copying /tmp/helm to /home/gdha/.arkade/bin/helm Downloaded to: /home/gdha/.arkade/bin/helm helm \"grafana\" has been added to your repositories Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"metallb\" chart repository ...Successfully got an update from the \"longhorn\" chart repository ...Successfully got an update from the \"traefik\" chart repository ...Successfully got an update from the \"kiwigrid\" chart repository ...Successfully got an update from the \"grafana\" chart repository Update Complete. \u2388Happy Helming!\u2388 VALUES values.yaml Command: /home/gdha/.arkade/bin/helm [upgrade --install loki-stack grafana/loki-stack --namespace logging --values /tmp/charts/loki-stack/values.yaml --set loki.persistence.enabled=true] Release \"loki-stack\" does not exist. Installing it now. NAME: loki-stack LAST DEPLOYED: Fri Feb 3 15:24:22 2023 NAMESPACE: logging STATUS: deployed REVISION: 1 NOTES: The Loki stack has been deployed to your cluster. Loki can now be added as a datasource in Grafana. See http://docs.grafana.org/features/datasources/loki/ for more detail. ======================================================================= = loki has been installed. = ======================================================================= # Get started with loki here: # https://grafana.com/docs/loki/latest # See how to integrate loki with Grafana here # https://grafana.com/docs/loki/latest/getting-started/grafana # Check loki's logs with: kubectl logs svc/loki-stack kubectl logs svc/loki-stack-headless # If you installed with Grafana you can access the dashboard with the username \"admin\" and password shown below # To get password kubectl get secret loki-stack-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo # Forward traffic to your localhost kubectl port-forward service/loki-stack-grafana 3000:80 whale arkade needs your support: https://github.com/sponsors/alexellis Now, verify if the pods have been created: gdha@n1:~/projects/pi4-logging$ kubectl get pods -n logging NAME READY STATUS RESTARTS AGE loki-stack-0 0/1 Running 0 114s loki-stack-promtail-d45tz 1/1 Running 0 114s loki-stack-promtail-qv75h 1/1 Running 0 114s loki-stack-promtail-rhcr9 1/1 Running 0 114s loki-stack-promtail-dwz7s 1/1 Running 0 114s loki-stack-promtail-4qwlg 1/1 Running 0 114s As last we still need to apply the patch: gdha@n1:~/projects/pi4-logging$ cat patch.yaml spec: template: spec: tolerations: - operator: Exists gdha@n1:~/projects/pi4-logging$ kubectl patch daemonset loki-stack-promtail -n logging --patch \"$(cat patch.yaml)\" daemonset.apps/loki-stack-promtail patched Check if the services are available for usage: gdha@n1:~/projects/pi4-logging$ kubectl get svc -n logging NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE loki-stack-headless ClusterIP None <none> 3100/TCP 6m39s loki-stack-memberlist ClusterIP None <none> 7946/TCP 6m39s loki-stack ClusterIP 10.43.220.29 <none> 3100/TCP 6m39s To view the logs we need to login onto our Grafana site and via configure - data sources - add data sources and search for Loki and select \"loki\". On the next screen fill in the URL section the IP address found with 'loki-stack' service. In our case it is http://10.43.220.29:3100 . Thereafter, it is kust a matter of pressing \"Save & Test\" button. In the side bar of Grafana click on the \"Explore\" button and select \"Loki\" in the upper left corner (of the service to use). Use Loki's \"Log Labels\" to select what you want to see - just play with it... Your imagination is the limit with Loki/Grafana, e.g.","title":"Installation of loki and promtail"},{"location":"pi-stories14/#references","text":"[1] k3s Monitoring - Logging Logs","title":"References"},{"location":"pi-stories15/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Troubleshooting pod issues \u00b6 PODs are stuck in termating status \u00b6 If you ever come in the situation that some pods get stuck in \"terminating\" status [1] for one of another reason (upgrade, delete,...) and then these pods stay in the terminating status then what can you do? gdha@n1:~$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE ingress-nginx ingress-nginx-admission-patch-w5dhz 0/1 Completed 0 107d ingress-nginx ingress-nginx-admission-create-lgbdc 0/1 Completed 0 107d monitoring grafana-544f695579-g246k 0/1 Terminating 0 3d20h graphite graphite-0 0/1 Terminating 1 3d20h monitoring node-exporter-dvzxs 2/2 Running 54 (17m ago) 105d celsius celsius-779654f68d-d56q6 1/1 Running 24 (17m ago) 92d ntopng ntopng-7d5d578657-9wj7g 1/1 Running 7 (17m ago) 6d23h No panic, just do the following trick - force a termination without waiting on anything: gdha@n1:~$ kubectl delete pod graphite-0 --grace-period=0 --force --namespace graphite Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. pod \"graphite-0\" force deleted And, there you go - one pod is gone: gdha@n1:~$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE ingress-nginx ingress-nginx-admission-patch-w5dhz 0/1 Completed 0 107d ingress-nginx ingress-nginx-admission-create-lgbdc 0/1 Completed 0 107d monitoring grafana-544f695579-g246k 0/1 Terminating 0 3d20h monitoring node-exporter-dvzxs 2/2 Running 54 (33m ago) 105d celsius celsius-779654f68d-d56q6 1/1 Running 24 (33m ago) 92d ntopng ntopng-7d5d578657-9wj7g 1/1 Running 7 (33m ago) 6d23h longhorn-system engine-image-ei-fc06c6fb-hml5v 1/1 Running 31 (33m ago) 107d longhorn-system csi-resizer-847d645569-hncv2 1/1 Running 1 (33m ago) 3d14h longhorn-system csi-snapshotter-59ff78b47b-jk6rm 1/1 Running 1 (33m ago) 3d14h longhorn-system csi-attacher-689fdb475f-ttnzx 1/1 Running 2 (33m ago) 3d14h longhorn-system csi-attacher-689fdb475f-rxblc 1/1 Running 2 (33m ago) 3d14h longhorn-system csi-attacher-689fdb475f-x9zj7 1/1 Running 2 (33m ago) 3d14h longhorn-system csi-provisioner-65dd48db7b-x8hgf 1/1 Running 2 (33m ago) 3d14h longhorn-system csi-snapshotter-59ff78b47b-mpsk7 1/1 Running 1 (33m ago) 3d14h celsius celsius-779654f68d-jbxg8 1/1 Running 1 (33m ago) 92d longhorn-system csi-provisioner-65dd48db7b-r4dwc 1/1 Running 2 (33m ago) 3d14h monitoring node-exporter-6gdxp 2/2 Running 2 (33m ago) 3d14h celsius celsius-779654f68d-9lmq8 1/1 Running 1 (33m ago) 92d longhorn-system csi-snapshotter-59ff78b47b-8jf5c 1/1 Running 1 (33m ago) 3d14h monitoring node-exporter-2pt9t 2/2 Running 2 (33m ago) 105d longhorn-system longhorn-conversion-webhook-c6f4bf8b6-jgjph 1/1 Running 1 (33m ago) 107d longhorn-system csi-resizer-847d645569-lsk97 1/1 Running 2 (33m ago) 3d14h celsius celsius-779654f68d-j6m6t 1/1 Running 1 (33m ago) 92d kube-system traefik-59497f77c-gxj8z 1/1 Running 15 (33m ago) 6d23h monitoring node-exporter-7m984 2/2 Running 2 (33m ago) 105d celsius celsius-779654f68d-5vs8r 1/1 Running 1 (33m ago) 6d23h cert-manager cert-manager-cainjector-5fcd49c96-ghvkt 1/1 Running 1 (33m ago) 107d kube-system metallb-speaker-wvfzm 1/1 Running 1 (33m ago) 3d14h longhorn-system csi-resizer-847d645569-cfsng 1/1 Running 2 (33m ago) 3d14h kube-system local-path-provisioner-957fdf8bc-zwhws 1/1 Running 1 (33m ago) 3d16h monitoring node-exporter-rbv7f 2/2 Running 2 (33m ago) 105d longhorn-system engine-image-ei-fc06c6fb-k6ssm 1/1 Running 1 (33m ago) 3d14h longhorn-system longhorn-conversion-webhook-c6f4bf8b6-q6qlc 1/1 Running 1 (33m ago) 6d23h cert-manager cert-manager-6ffb79dfdb-gqlnm 1/1 Running 1 (33m ago) 107d monitoring prometheus-operator-754546bd85-pmkcc 1/1 Running 1 (33m ago) 105d kube-system metallb-speaker-6v4gs 1/1 Running 1 (33m ago) 107d longhorn-system engine-image-ei-fc06c6fb-445sw 1/1 Running 1 (33m ago) 107d longhorn-system engine-image-ei-fc06c6fb-8nwh8 1/1 Running 1 (33m ago) 107d kube-system metallb-speaker-t2b5d 1/1 Running 1 (33m ago) 107d longhorn-system longhorn-recovery-backend-786b877b6c-tfcg9 1/1 Running 1 (33m ago) 107d kube-system coredns-77ccd57875-h9dfk 1/1 Running 1 (33m ago) 3d16h longhorn-system longhorn-recovery-backend-786b877b6c-nr5r5 1/1 Running 28 (33m ago) 107d logging loki-stack-promtail-qtvj5 1/1 Running 1 (33m ago) 97d kube-system metallb-speaker-55mxm 1/1 Running 1 (33m ago) 107d kube-system metallb-controller-5b89f7554c-8rhzv 1/1 Running 1 (33m ago) 107d longhorn-system longhorn-ui-788dd6f547-zwjfl 1/1 Running 3 (32m ago) 107d longhorn-system longhorn-ui-788dd6f547-fw5rq 1/1 Running 14 (32m ago) 6d23h ingress-nginx ingress-nginx-controller-65dc77f88f-kh8sn 1/1 Running 1 (33m ago) 107d cert-manager cert-manager-webhook-796ff7697b-ptmhq 1/1 Running 14 (32m ago) 6d23h longhorn-system engine-image-ei-fc06c6fb-jpfwq 1/1 Running 1 (33m ago) 107d kube-system metrics-server-7b67f64457-bk84t 1/1 Running 60 (32m ago) 107d longhorn-system longhorn-admission-webhook-777c6cb845-fb6pl 1/1 Running 1 (33m ago) 107d kube-system metallb-speaker-nhzj6 1/1 Running 65 (32m ago) 107d monitoring kube-state-metrics-6f8578cffd-cmks6 1/1 Running 37 (32m ago) 105d longhorn-system longhorn-admission-webhook-777c6cb845-snjmd 1/1 Running 1 107d longhorn-system instance-manager-e-f7ebd11fee14cab3eb5a0c0884af9492 1/1 Running 0 32m longhorn-system instance-manager-r-f7ebd11fee14cab3eb5a0c0884af9492 1/1 Running 0 32m longhorn-system longhorn-manager-sp98j 1/1 Running 1 (33m ago) 107d longhorn-system longhorn-manager-v6c78 1/1 Running 1 (33m ago) 107d longhorn-system longhorn-manager-25889 1/1 Running 1 (33m ago) 3d14h longhorn-system instance-manager-e-1f203b7579bacda190651c7f40a2cea4 1/1 Running 0 31m longhorn-system longhorn-csi-plugin-28jqn 3/3 Running 5 (32m ago) 3d14h longhorn-system instance-manager-r-11dfece26197f3792b18d4fa97734e04 1/1 Running 0 31m longhorn-system longhorn-manager-hx647 1/1 Running 28 (33m ago) 107d longhorn-system instance-manager-r-1f203b7579bacda190651c7f40a2cea4 1/1 Running 0 31m longhorn-system instance-manager-e-1548f3f3f04de3f9fad47c6ce8941357 1/1 Running 0 31m longhorn-system instance-manager-e-11dfece26197f3792b18d4fa97734e04 1/1 Running 0 31m longhorn-system instance-manager-r-1548f3f3f04de3f9fad47c6ce8941357 1/1 Running 0 31m longhorn-system longhorn-csi-plugin-bmzpb 3/3 Running 5 (32m ago) 3d14h longhorn-system longhorn-driver-deployer-bf975c845-sxtsq 1/1 Running 2 (31m ago) 107d longhorn-system longhorn-csi-plugin-fblxn 3/3 Running 5 (32m ago) 3d14h longhorn-system longhorn-csi-plugin-dgtqs 3/3 Running 5 (32m ago) 3d14h longhorn-system longhorn-csi-plugin-6dfv6 3/3 Running 5 (32m ago) 3d14h longhorn-system instance-manager-r-07ae318e5cf6bed1004f251d3c792412 1/1 Running 0 31m longhorn-system longhorn-manager-69zkg 1/1 Running 1 (33m ago) 107d longhorn-system instance-manager-e-07ae318e5cf6bed1004f251d3c792412 1/1 Running 0 31m longhorn-system csi-provisioner-65dd48db7b-k4lcb 1/1 Running 2 (31m ago) 3d14h monitoring prometheus-prometheus-persistant-0 2/2 Running 0 29m nfs-server nfs-server-f48df7cdc-2zkkq 1/1 Running 0 30m monitoring grafana-544f695579-6jcfz 1/1 Running 0 30m logging loki-stack-promtail-pcrjw 1/1 Running 25 (33m ago) 97d logging loki-stack-0 1/1 Running 0 28m logging loki-stack-promtail-9hls8 1/1 Running 1 (33m ago) 97d logging loki-stack-promtail-zfpdt 1/1 Running 1 (33m ago) 3d14h uptime-kuma uptime-kuma-0 1/1 Running 6 (28m ago) 3d14h logging loki-stack-promtail-nh2m7 1/1 Running 1 (33m ago) 97d graphite graphite-0 1/1 Running Repeat these steps for the remaining pods which are stuck: gdha@n1:~$ kubectl delete pod grafana-544f695579-g246k --grace-period=0 --force --namespace monitoring Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. pod \"grafana-544f695579-g246k\" force deleted PODs are stuck in ContainerCreating status \u00b6 Sometimes we come into a situation where a pod is stuck in ContainerCreating status after restarting a cluster (e.g. after a reboot) - example: $ kubectl get pods -n graphite graphite-0 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES graphite-0 0/1 ContainerCreating 0 23m <none> n5 <none> <none> The reason is not obvious as the logs show only: $ kubectl logs -n graphite graphite-0 Error from server (BadRequest): container \"graphite\" in pod \"graphite-0\" is waiting to start: ContainerCreating We could suspect that it could not download a fresh copy of the image, but it worked before always like a charm? Another way to dig a bit deeper to find the real issue is to use the command: $ kubectl describe pod -n graphite graphite-0 Name: graphite-0 Namespace: graphite Priority: 0 Service Account: default Node: n5/192.168.0.205 Start Time: Wed, 12 Jul 2023 08:36:05 +0200 Labels: app.kubernetes.io/instance=graphite app.kubernetes.io/name=graphite controller-revision-hash=graphite-645d65f6fb statefulset.kubernetes.io/pod-name=graphite-0 workload.user.cattle.io/workloadselector=statefulSet-graphite-graphite Annotations: <none> Status: Pending IP: IPs: <none> Controlled By: StatefulSet/graphite Containers: graphite: Container ID: Image: ghcr.io/gdha/graphite:v1.2 Image ID: Ports: 2003/TCP, 80/TCP Host Ports: 0/TCP, 0/TCP State: Waiting Reason: ContainerCreating Ready: False Restart Count: 0 Limits: memory: 512Mi Requests: memory: 400Mi Liveness: http-get http://:liveness-port/metrics/find%3Fquery=%2A delay=0s timeout=2s period=2s #success=1 #failure=3 Readiness: http-get http://:liveness-port/metrics/find%3Fquery=%2A delay=0s timeout=2s period=2s #success=2 #failure=3 Startup: http-get http://:liveness-port/metrics/find%3Fquery=%2A delay=0s timeout=1s period=10s #success=1 #failure=30 Environment Variables from: graphite Secret Optional: false Environment: <none> Mounts: /opt/graphite/storage from graphite-storage (rw) /var/log from bind-mount--logs (rw) /var/run/secrets from graphite-secrets (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kxvv5 (ro) Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled True Volumes: bind-mount--logs: Type: HostPath (bare host directory volume) Path: /app/util/graphite/logs HostPathType: DirectoryOrCreate graphite-secrets: Type: Secret (a volume populated by a Secret) SecretName: graphite Optional: false graphite-storage: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: graphite ReadOnly: false kube-api-access-kxvv5: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: Burstable Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 30m default-scheduler Successfully assigned graphite/graphite-0 to n5 Warning FailedMount 7m17s (x19 over 29m) kubelet MountVolume.MountDevice failed for volume \"pvc-14e6e723-0fa2-4b8f-8619-ae02f2b74cdc\" : rpc error: code = Internal desc = mount failed: exit status 32 Mounting command: mount Mounting arguments: -t ext4 -o defaults /dev/longhorn/pvc-14e6e723-0fa2-4b8f-8619-ae02f2b74cdc /var/lib/kubelet/plugins/kubernetes.io/csi/driver.longhorn.io/8a5f0348fbb5570ffccf61701ff8b83081168509f5904af61a35649c9f2cc7d9/globalmount Output: mount: /var/lib/kubelet/plugins/kubernetes.io/csi/driver.longhorn.io/8a5f0348fbb5570ffccf61701ff8b83081168509f5904af61a35649c9f2cc7d9/globalmount: /dev/longhorn/pvc-14e6e723-0fa2-4b8f-8619-ae02f2b74cdc already mounted or mount point busy. Warning FailedMount 3m15s (x12 over 28m) kubelet Unable to attach or mount volumes: unmounted volumes=[graphite-storage], unattached volumes=[], failed to process volumes=[]: timed out waiting for the condition Okay, here we see clearly the error \"/dev/longhorn/pvc-14e6e723-0fa2-4b8f-8619-ae02f2b74cdc already mounted or mount point busy\". So, let us try the following: $ kubectl delete pod -n graphite graphite-0 pod \"graphite-0\" deleted $ kubectl get pods -n graphite graphite-0 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES graphite-0 0/1 ContainerCreating 0 12m <none> n5 <none> <none> Unfortunately, that did not fix it. So, we remove the PVC and recreated it, but no luck. To see the last events use the command: $ kubectl get events --all-namespaces --sort-by='.metadata.creationTimestamp' NAMESPACE LAST SEEN TYPE REASON OBJECT MESSAGE graphite 14m Warning FailedMount pod/graphite-0 MountVolume.MountDevice failed for volume \"pvc-90ed5ccf-3152-4e0b-b795-cb985a7d83be\" : rpc error: code = Internal desc = format of disk \"/dev/longhorn/pvc-90ed5ccf-3152-4e0b-b795-cb985a7d83be\" failed: type:(\"ext4\") target:(\"/var/lib/kubelet/plugins/kubernetes.io/csi/driver.longhorn.io/d3ebefb7681c1a1f9472812339e5a4342cca0dee55d743334be457f6a9cf1fb3/globalmount\") options:(\"defaults\") errcode:(exit status 1) output:(mke2fs 1.46.4 (18-Aug-2021)... graphite 5m8s Warning FailedMount pod/graphite-0 Unable to attach or mount volumes: unmounted volumes=[graphite-storage], unattached volumes=[], failed to process volumes=[]: timed out waiting for the condition monitoring 36m Warning Unhealthy pod/grafana-544f695579-st45c Readiness probe failed: Get \"http://10.42.4.117:3000/api/health\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Via some research we came across page Troubleshooting: MountVolume.SetUp failed for volume due to multipathd on the node [2] which might be the solution for our problem here as after rebooting the complete cluster the graphite PVC was mounted correctly, but then the loki-stack PVC was not able to mount with the exact same error. Therefore, we can conclude there is nothing wrong with the PVC, but it is an interaction clash with the multipath daemon. We update our k3s ansible playbook to also update the /etc/multipath.conf file to blacklist /dev/sd* devices. POD stuck in ImagePullBackOff status \u00b6 For example when we tried to upgrade out ntopng image with helm we got an error like: $ kubectl get pods -n ntopng NAME READY STATUS RESTARTS AGE ntopng-6586866d8b-w668c 0/1 ImagePullBackOff 0 69m The first thing we think of is a mismatch of our GitHub Container Registry (ghrc) Personal Access Token (PAT). How can we verify that the PAT our kubernetes cluster knows is still the same as the one listed in our ~/.ghcr-token file? As we know in which namespace to look (in our case ntopng) we can start digging as follow: $ kubectl get secret -n ntopng NAME TYPE DATA AGE ntopng-ghrc kubernetes.io/dockerconfigjson 1 67m ntopng Opaque 0 67m sh.helm.release.v1.ntopng.v1 helm.sh/release.v1 1 67m Right, as we suspect the ghrc secret lets extract this from out cluster: $ kubectl get secret -n ntopng ntopng-ghrc --output=\"jsonpath={.data.\\.dockerconfigjson}\" | base64 --decode {\"auths\":{\"ghcr.io\":{\"username\":\"gdha\",\"password\":\"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\",\"auth\":\"Z2RoYTpnaHBfdXlPNElwUnJ6TmpFc25JTnN3NEQ1dVZRVWdkYVpSNDR2NGM2\"}}} There is our PAT key: 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 and indeed it was an old one which we revoked a time ago. To fix this we must update our ghcr-secret.yaml file and especially the .dockerconfigjson data section. How? Well, as example I take the revoked old PAT key of ghrc: $ kubectl create secret docker-registry dockerconfigjson-github-com --docker-server=ghcr.io --docker-username=gdha --docker-password=$(echo -n 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) --dry-run=client -oyaml apiVersion: v1 data: .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoiZ2RoYSIsInBhc3N3b3JkIjoiZ2hwX3V5TzRJcFJyek5qRXNuSU5zdzRENXVWUVVnZGFaUjQ0djRjNiIsImF1dGgiOiJaMlJvWVRwbmFIQmZkWGxQTkVsd1VuSjZUbXBGYzI1SlRuTjNORVExZFZaUlZXZGtZVnBTTkRSMk5HTTIifX19 kind: Secret metadata: creationTimestamp: null name: dockerconfigjson-github-com type: kubernetes.io/dockerconfigjson How to debug a POD \u00b6 Interesting topic as we stumbled over the pi4-ntopng POD which refused to start with an error: Starting redis-server: *** FATAL CONFIG FILE ERROR *** Reading the configuration file, at line 171 >>> 'logfile /var/log/redis/redis-server.log' Can't open the log file: Permission denied failed 29/Sep/2023 15:30:47 [Redis.cpp:98] ERROR: Connection error [Connection refused] 29/Sep/2023 15:30:48 [Redis.cpp:80] Redis has disconnected, reconnecting [remaining attempts: 14] Of course, the POD exits immediately. So, how can we debug our error? It might just be that there is more wrong the only the permission denied error... We found a good article [3] which gives us some more insight on how to dig deeper. We need to override the \"entrypoint\" as it behaves as init 1, and therefore, we cannot just pass a bash command to it. Execute the following: $ docker run -it --entrypoint /bin/bash ghcr.io/gdha/pi4-ntopng:5.7.0 This will drop us into a shell command from where we can peek around in the container to see what might be wrong. Indeed as the error above mentioned the ownership of /var/log/redis was wrong: root@131f7e6411c5:/var/log# ls -l total 692 -rw-r--r-- 1 root root 4901 Sep 29 15:13 alternatives.log drwxr-xr-x 1 root root 4096 Sep 29 15:12 apt -rw-r--r-- 1 root root 58584 Jan 26 2023 bootstrap.log -rw-rw---- 1 root utmp 0 Jan 26 2023 btmp -rw-r--r-- 1 root root 282997 Sep 29 15:14 dpkg.log -rw-r--r-- 1 root root 32000 Sep 29 15:14 faillog -rw-r--r-- 1 root root 484 Sep 29 15:12 fontconfig.log drwxr-sr-x 2 root root 4096 Sep 29 15:11 journal -rw-rw-r-- 1 root root 296000 Sep 29 15:14 lastlog drwx------ 2 root root 4096 Sep 29 15:11 private drwxr-s--- 1 root root 4096 Sep 29 15:12 redis -rw-rw-r-- 1 root utmp 0 Jan 26 2023 wtmp root@131f7e6411c5:/var/log# chown -R redis:redis /var/log/redis However, as we are inside the container lets try to start redis: root@131f7e6411c5:~# /etc/init.d/redis-server start Starting redis-server: failed Hum, not 100% okay it seems. However, as the log file of redis should be available now we can check it out: root@131f7e6411c5:~# cat /var/log/redis/redis-server.log 19:C 02 Oct 2023 07:11:40.148 # Can't chdir to '/var/lib/redis': Permission denied 33:C 02 Oct 2023 07:13:22.884 # Can't chdir to '/var/lib/redis': Permission denied 69:C 02 Oct 2023 07:17:39.861 # Can't chdir to '/var/lib/redis': Permission denied Indeed still another directory to be tackled: root@131f7e6411c5:~# chown -R redis:redis /var/lib/redis We can start redis again and see what it says: root@131f7e6411c5:~# /usr/bin/redis-server 37:C 02 Oct 2023 07:15:29.235 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 37:C 02 Oct 2023 07:15:29.236 # Redis version=5.0.7, bits=64, commit=00000000, modified=0, pid=37, just started 37:C 02 Oct 2023 07:15:29.236 # Warning: no config file specified, using the default config. In order to specify a config file use /usr/bin/redis-server /path/to/redis.conf _._ _.-``__ ''-._ _.-`` `. `_. ''-._ Redis 5.0.7 (00000000/0) 64 bit .-`` .-```. ```\\/ _.,_ ''-._ ( ' , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|'` _.-'| Port: 6379 | `-._ `._ / _.-' | PID: 37 `-._ `-._ `-./ _.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | http://redis.io `-._ `-._`-.__.-'_.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | `-._ `-._`-.__.-'_.-' _.-' `-._ `-.__.-' _.-' `-._ _.-' `-.__.-' 37:M 02 Oct 2023 07:15:29.258 # Server initialized 37:M 02 Oct 2023 07:15:29.258 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. 37:M 02 Oct 2023 07:15:29.259 * Ready to accept connections Yep, it seems to start, but the message around transparent_hugepage is quite interesting which we can take care of in our Dockerfile. References \u00b6 [1] PODs are stuck in terminating status [2] Troubleshooting: MountVolume.SetUp failed for volume due to multipathd on the node [3] Ten tips for debugging Docker containers","title":"Raspberry Pi 4 Troubleshooting"},{"location":"pi-stories15/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories15/#raspberry_pi_4_cluster_series_-_troubleshooting_pod_issues","text":"","title":"Raspberry Pi 4 cluster Series - Troubleshooting pod issues"},{"location":"pi-stories15/#pods_are_stuck_in_termating_status","text":"If you ever come in the situation that some pods get stuck in \"terminating\" status [1] for one of another reason (upgrade, delete,...) and then these pods stay in the terminating status then what can you do? gdha@n1:~$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE ingress-nginx ingress-nginx-admission-patch-w5dhz 0/1 Completed 0 107d ingress-nginx ingress-nginx-admission-create-lgbdc 0/1 Completed 0 107d monitoring grafana-544f695579-g246k 0/1 Terminating 0 3d20h graphite graphite-0 0/1 Terminating 1 3d20h monitoring node-exporter-dvzxs 2/2 Running 54 (17m ago) 105d celsius celsius-779654f68d-d56q6 1/1 Running 24 (17m ago) 92d ntopng ntopng-7d5d578657-9wj7g 1/1 Running 7 (17m ago) 6d23h No panic, just do the following trick - force a termination without waiting on anything: gdha@n1:~$ kubectl delete pod graphite-0 --grace-period=0 --force --namespace graphite Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. pod \"graphite-0\" force deleted And, there you go - one pod is gone: gdha@n1:~$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE ingress-nginx ingress-nginx-admission-patch-w5dhz 0/1 Completed 0 107d ingress-nginx ingress-nginx-admission-create-lgbdc 0/1 Completed 0 107d monitoring grafana-544f695579-g246k 0/1 Terminating 0 3d20h monitoring node-exporter-dvzxs 2/2 Running 54 (33m ago) 105d celsius celsius-779654f68d-d56q6 1/1 Running 24 (33m ago) 92d ntopng ntopng-7d5d578657-9wj7g 1/1 Running 7 (33m ago) 6d23h longhorn-system engine-image-ei-fc06c6fb-hml5v 1/1 Running 31 (33m ago) 107d longhorn-system csi-resizer-847d645569-hncv2 1/1 Running 1 (33m ago) 3d14h longhorn-system csi-snapshotter-59ff78b47b-jk6rm 1/1 Running 1 (33m ago) 3d14h longhorn-system csi-attacher-689fdb475f-ttnzx 1/1 Running 2 (33m ago) 3d14h longhorn-system csi-attacher-689fdb475f-rxblc 1/1 Running 2 (33m ago) 3d14h longhorn-system csi-attacher-689fdb475f-x9zj7 1/1 Running 2 (33m ago) 3d14h longhorn-system csi-provisioner-65dd48db7b-x8hgf 1/1 Running 2 (33m ago) 3d14h longhorn-system csi-snapshotter-59ff78b47b-mpsk7 1/1 Running 1 (33m ago) 3d14h celsius celsius-779654f68d-jbxg8 1/1 Running 1 (33m ago) 92d longhorn-system csi-provisioner-65dd48db7b-r4dwc 1/1 Running 2 (33m ago) 3d14h monitoring node-exporter-6gdxp 2/2 Running 2 (33m ago) 3d14h celsius celsius-779654f68d-9lmq8 1/1 Running 1 (33m ago) 92d longhorn-system csi-snapshotter-59ff78b47b-8jf5c 1/1 Running 1 (33m ago) 3d14h monitoring node-exporter-2pt9t 2/2 Running 2 (33m ago) 105d longhorn-system longhorn-conversion-webhook-c6f4bf8b6-jgjph 1/1 Running 1 (33m ago) 107d longhorn-system csi-resizer-847d645569-lsk97 1/1 Running 2 (33m ago) 3d14h celsius celsius-779654f68d-j6m6t 1/1 Running 1 (33m ago) 92d kube-system traefik-59497f77c-gxj8z 1/1 Running 15 (33m ago) 6d23h monitoring node-exporter-7m984 2/2 Running 2 (33m ago) 105d celsius celsius-779654f68d-5vs8r 1/1 Running 1 (33m ago) 6d23h cert-manager cert-manager-cainjector-5fcd49c96-ghvkt 1/1 Running 1 (33m ago) 107d kube-system metallb-speaker-wvfzm 1/1 Running 1 (33m ago) 3d14h longhorn-system csi-resizer-847d645569-cfsng 1/1 Running 2 (33m ago) 3d14h kube-system local-path-provisioner-957fdf8bc-zwhws 1/1 Running 1 (33m ago) 3d16h monitoring node-exporter-rbv7f 2/2 Running 2 (33m ago) 105d longhorn-system engine-image-ei-fc06c6fb-k6ssm 1/1 Running 1 (33m ago) 3d14h longhorn-system longhorn-conversion-webhook-c6f4bf8b6-q6qlc 1/1 Running 1 (33m ago) 6d23h cert-manager cert-manager-6ffb79dfdb-gqlnm 1/1 Running 1 (33m ago) 107d monitoring prometheus-operator-754546bd85-pmkcc 1/1 Running 1 (33m ago) 105d kube-system metallb-speaker-6v4gs 1/1 Running 1 (33m ago) 107d longhorn-system engine-image-ei-fc06c6fb-445sw 1/1 Running 1 (33m ago) 107d longhorn-system engine-image-ei-fc06c6fb-8nwh8 1/1 Running 1 (33m ago) 107d kube-system metallb-speaker-t2b5d 1/1 Running 1 (33m ago) 107d longhorn-system longhorn-recovery-backend-786b877b6c-tfcg9 1/1 Running 1 (33m ago) 107d kube-system coredns-77ccd57875-h9dfk 1/1 Running 1 (33m ago) 3d16h longhorn-system longhorn-recovery-backend-786b877b6c-nr5r5 1/1 Running 28 (33m ago) 107d logging loki-stack-promtail-qtvj5 1/1 Running 1 (33m ago) 97d kube-system metallb-speaker-55mxm 1/1 Running 1 (33m ago) 107d kube-system metallb-controller-5b89f7554c-8rhzv 1/1 Running 1 (33m ago) 107d longhorn-system longhorn-ui-788dd6f547-zwjfl 1/1 Running 3 (32m ago) 107d longhorn-system longhorn-ui-788dd6f547-fw5rq 1/1 Running 14 (32m ago) 6d23h ingress-nginx ingress-nginx-controller-65dc77f88f-kh8sn 1/1 Running 1 (33m ago) 107d cert-manager cert-manager-webhook-796ff7697b-ptmhq 1/1 Running 14 (32m ago) 6d23h longhorn-system engine-image-ei-fc06c6fb-jpfwq 1/1 Running 1 (33m ago) 107d kube-system metrics-server-7b67f64457-bk84t 1/1 Running 60 (32m ago) 107d longhorn-system longhorn-admission-webhook-777c6cb845-fb6pl 1/1 Running 1 (33m ago) 107d kube-system metallb-speaker-nhzj6 1/1 Running 65 (32m ago) 107d monitoring kube-state-metrics-6f8578cffd-cmks6 1/1 Running 37 (32m ago) 105d longhorn-system longhorn-admission-webhook-777c6cb845-snjmd 1/1 Running 1 107d longhorn-system instance-manager-e-f7ebd11fee14cab3eb5a0c0884af9492 1/1 Running 0 32m longhorn-system instance-manager-r-f7ebd11fee14cab3eb5a0c0884af9492 1/1 Running 0 32m longhorn-system longhorn-manager-sp98j 1/1 Running 1 (33m ago) 107d longhorn-system longhorn-manager-v6c78 1/1 Running 1 (33m ago) 107d longhorn-system longhorn-manager-25889 1/1 Running 1 (33m ago) 3d14h longhorn-system instance-manager-e-1f203b7579bacda190651c7f40a2cea4 1/1 Running 0 31m longhorn-system longhorn-csi-plugin-28jqn 3/3 Running 5 (32m ago) 3d14h longhorn-system instance-manager-r-11dfece26197f3792b18d4fa97734e04 1/1 Running 0 31m longhorn-system longhorn-manager-hx647 1/1 Running 28 (33m ago) 107d longhorn-system instance-manager-r-1f203b7579bacda190651c7f40a2cea4 1/1 Running 0 31m longhorn-system instance-manager-e-1548f3f3f04de3f9fad47c6ce8941357 1/1 Running 0 31m longhorn-system instance-manager-e-11dfece26197f3792b18d4fa97734e04 1/1 Running 0 31m longhorn-system instance-manager-r-1548f3f3f04de3f9fad47c6ce8941357 1/1 Running 0 31m longhorn-system longhorn-csi-plugin-bmzpb 3/3 Running 5 (32m ago) 3d14h longhorn-system longhorn-driver-deployer-bf975c845-sxtsq 1/1 Running 2 (31m ago) 107d longhorn-system longhorn-csi-plugin-fblxn 3/3 Running 5 (32m ago) 3d14h longhorn-system longhorn-csi-plugin-dgtqs 3/3 Running 5 (32m ago) 3d14h longhorn-system longhorn-csi-plugin-6dfv6 3/3 Running 5 (32m ago) 3d14h longhorn-system instance-manager-r-07ae318e5cf6bed1004f251d3c792412 1/1 Running 0 31m longhorn-system longhorn-manager-69zkg 1/1 Running 1 (33m ago) 107d longhorn-system instance-manager-e-07ae318e5cf6bed1004f251d3c792412 1/1 Running 0 31m longhorn-system csi-provisioner-65dd48db7b-k4lcb 1/1 Running 2 (31m ago) 3d14h monitoring prometheus-prometheus-persistant-0 2/2 Running 0 29m nfs-server nfs-server-f48df7cdc-2zkkq 1/1 Running 0 30m monitoring grafana-544f695579-6jcfz 1/1 Running 0 30m logging loki-stack-promtail-pcrjw 1/1 Running 25 (33m ago) 97d logging loki-stack-0 1/1 Running 0 28m logging loki-stack-promtail-9hls8 1/1 Running 1 (33m ago) 97d logging loki-stack-promtail-zfpdt 1/1 Running 1 (33m ago) 3d14h uptime-kuma uptime-kuma-0 1/1 Running 6 (28m ago) 3d14h logging loki-stack-promtail-nh2m7 1/1 Running 1 (33m ago) 97d graphite graphite-0 1/1 Running Repeat these steps for the remaining pods which are stuck: gdha@n1:~$ kubectl delete pod grafana-544f695579-g246k --grace-period=0 --force --namespace monitoring Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. pod \"grafana-544f695579-g246k\" force deleted","title":"PODs are stuck in termating status"},{"location":"pi-stories15/#pods_are_stuck_in_containercreating_status","text":"Sometimes we come into a situation where a pod is stuck in ContainerCreating status after restarting a cluster (e.g. after a reboot) - example: $ kubectl get pods -n graphite graphite-0 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES graphite-0 0/1 ContainerCreating 0 23m <none> n5 <none> <none> The reason is not obvious as the logs show only: $ kubectl logs -n graphite graphite-0 Error from server (BadRequest): container \"graphite\" in pod \"graphite-0\" is waiting to start: ContainerCreating We could suspect that it could not download a fresh copy of the image, but it worked before always like a charm? Another way to dig a bit deeper to find the real issue is to use the command: $ kubectl describe pod -n graphite graphite-0 Name: graphite-0 Namespace: graphite Priority: 0 Service Account: default Node: n5/192.168.0.205 Start Time: Wed, 12 Jul 2023 08:36:05 +0200 Labels: app.kubernetes.io/instance=graphite app.kubernetes.io/name=graphite controller-revision-hash=graphite-645d65f6fb statefulset.kubernetes.io/pod-name=graphite-0 workload.user.cattle.io/workloadselector=statefulSet-graphite-graphite Annotations: <none> Status: Pending IP: IPs: <none> Controlled By: StatefulSet/graphite Containers: graphite: Container ID: Image: ghcr.io/gdha/graphite:v1.2 Image ID: Ports: 2003/TCP, 80/TCP Host Ports: 0/TCP, 0/TCP State: Waiting Reason: ContainerCreating Ready: False Restart Count: 0 Limits: memory: 512Mi Requests: memory: 400Mi Liveness: http-get http://:liveness-port/metrics/find%3Fquery=%2A delay=0s timeout=2s period=2s #success=1 #failure=3 Readiness: http-get http://:liveness-port/metrics/find%3Fquery=%2A delay=0s timeout=2s period=2s #success=2 #failure=3 Startup: http-get http://:liveness-port/metrics/find%3Fquery=%2A delay=0s timeout=1s period=10s #success=1 #failure=30 Environment Variables from: graphite Secret Optional: false Environment: <none> Mounts: /opt/graphite/storage from graphite-storage (rw) /var/log from bind-mount--logs (rw) /var/run/secrets from graphite-secrets (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kxvv5 (ro) Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled True Volumes: bind-mount--logs: Type: HostPath (bare host directory volume) Path: /app/util/graphite/logs HostPathType: DirectoryOrCreate graphite-secrets: Type: Secret (a volume populated by a Secret) SecretName: graphite Optional: false graphite-storage: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: graphite ReadOnly: false kube-api-access-kxvv5: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: Burstable Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 30m default-scheduler Successfully assigned graphite/graphite-0 to n5 Warning FailedMount 7m17s (x19 over 29m) kubelet MountVolume.MountDevice failed for volume \"pvc-14e6e723-0fa2-4b8f-8619-ae02f2b74cdc\" : rpc error: code = Internal desc = mount failed: exit status 32 Mounting command: mount Mounting arguments: -t ext4 -o defaults /dev/longhorn/pvc-14e6e723-0fa2-4b8f-8619-ae02f2b74cdc /var/lib/kubelet/plugins/kubernetes.io/csi/driver.longhorn.io/8a5f0348fbb5570ffccf61701ff8b83081168509f5904af61a35649c9f2cc7d9/globalmount Output: mount: /var/lib/kubelet/plugins/kubernetes.io/csi/driver.longhorn.io/8a5f0348fbb5570ffccf61701ff8b83081168509f5904af61a35649c9f2cc7d9/globalmount: /dev/longhorn/pvc-14e6e723-0fa2-4b8f-8619-ae02f2b74cdc already mounted or mount point busy. Warning FailedMount 3m15s (x12 over 28m) kubelet Unable to attach or mount volumes: unmounted volumes=[graphite-storage], unattached volumes=[], failed to process volumes=[]: timed out waiting for the condition Okay, here we see clearly the error \"/dev/longhorn/pvc-14e6e723-0fa2-4b8f-8619-ae02f2b74cdc already mounted or mount point busy\". So, let us try the following: $ kubectl delete pod -n graphite graphite-0 pod \"graphite-0\" deleted $ kubectl get pods -n graphite graphite-0 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES graphite-0 0/1 ContainerCreating 0 12m <none> n5 <none> <none> Unfortunately, that did not fix it. So, we remove the PVC and recreated it, but no luck. To see the last events use the command: $ kubectl get events --all-namespaces --sort-by='.metadata.creationTimestamp' NAMESPACE LAST SEEN TYPE REASON OBJECT MESSAGE graphite 14m Warning FailedMount pod/graphite-0 MountVolume.MountDevice failed for volume \"pvc-90ed5ccf-3152-4e0b-b795-cb985a7d83be\" : rpc error: code = Internal desc = format of disk \"/dev/longhorn/pvc-90ed5ccf-3152-4e0b-b795-cb985a7d83be\" failed: type:(\"ext4\") target:(\"/var/lib/kubelet/plugins/kubernetes.io/csi/driver.longhorn.io/d3ebefb7681c1a1f9472812339e5a4342cca0dee55d743334be457f6a9cf1fb3/globalmount\") options:(\"defaults\") errcode:(exit status 1) output:(mke2fs 1.46.4 (18-Aug-2021)... graphite 5m8s Warning FailedMount pod/graphite-0 Unable to attach or mount volumes: unmounted volumes=[graphite-storage], unattached volumes=[], failed to process volumes=[]: timed out waiting for the condition monitoring 36m Warning Unhealthy pod/grafana-544f695579-st45c Readiness probe failed: Get \"http://10.42.4.117:3000/api/health\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Via some research we came across page Troubleshooting: MountVolume.SetUp failed for volume due to multipathd on the node [2] which might be the solution for our problem here as after rebooting the complete cluster the graphite PVC was mounted correctly, but then the loki-stack PVC was not able to mount with the exact same error. Therefore, we can conclude there is nothing wrong with the PVC, but it is an interaction clash with the multipath daemon. We update our k3s ansible playbook to also update the /etc/multipath.conf file to blacklist /dev/sd* devices.","title":"PODs are stuck in ContainerCreating status"},{"location":"pi-stories15/#pod_stuck_in_imagepullbackoff_status","text":"For example when we tried to upgrade out ntopng image with helm we got an error like: $ kubectl get pods -n ntopng NAME READY STATUS RESTARTS AGE ntopng-6586866d8b-w668c 0/1 ImagePullBackOff 0 69m The first thing we think of is a mismatch of our GitHub Container Registry (ghrc) Personal Access Token (PAT). How can we verify that the PAT our kubernetes cluster knows is still the same as the one listed in our ~/.ghcr-token file? As we know in which namespace to look (in our case ntopng) we can start digging as follow: $ kubectl get secret -n ntopng NAME TYPE DATA AGE ntopng-ghrc kubernetes.io/dockerconfigjson 1 67m ntopng Opaque 0 67m sh.helm.release.v1.ntopng.v1 helm.sh/release.v1 1 67m Right, as we suspect the ghrc secret lets extract this from out cluster: $ kubectl get secret -n ntopng ntopng-ghrc --output=\"jsonpath={.data.\\.dockerconfigjson}\" | base64 --decode {\"auths\":{\"ghcr.io\":{\"username\":\"gdha\",\"password\":\"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\",\"auth\":\"Z2RoYTpnaHBfdXlPNElwUnJ6TmpFc25JTnN3NEQ1dVZRVWdkYVpSNDR2NGM2\"}}} There is our PAT key: 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 and indeed it was an old one which we revoked a time ago. To fix this we must update our ghcr-secret.yaml file and especially the .dockerconfigjson data section. How? Well, as example I take the revoked old PAT key of ghrc: $ kubectl create secret docker-registry dockerconfigjson-github-com --docker-server=ghcr.io --docker-username=gdha --docker-password=$(echo -n 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) --dry-run=client -oyaml apiVersion: v1 data: .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoiZ2RoYSIsInBhc3N3b3JkIjoiZ2hwX3V5TzRJcFJyek5qRXNuSU5zdzRENXVWUVVnZGFaUjQ0djRjNiIsImF1dGgiOiJaMlJvWVRwbmFIQmZkWGxQTkVsd1VuSjZUbXBGYzI1SlRuTjNORVExZFZaUlZXZGtZVnBTTkRSMk5HTTIifX19 kind: Secret metadata: creationTimestamp: null name: dockerconfigjson-github-com type: kubernetes.io/dockerconfigjson","title":"POD stuck in ImagePullBackOff status"},{"location":"pi-stories15/#how_to_debug_a_pod","text":"Interesting topic as we stumbled over the pi4-ntopng POD which refused to start with an error: Starting redis-server: *** FATAL CONFIG FILE ERROR *** Reading the configuration file, at line 171 >>> 'logfile /var/log/redis/redis-server.log' Can't open the log file: Permission denied failed 29/Sep/2023 15:30:47 [Redis.cpp:98] ERROR: Connection error [Connection refused] 29/Sep/2023 15:30:48 [Redis.cpp:80] Redis has disconnected, reconnecting [remaining attempts: 14] Of course, the POD exits immediately. So, how can we debug our error? It might just be that there is more wrong the only the permission denied error... We found a good article [3] which gives us some more insight on how to dig deeper. We need to override the \"entrypoint\" as it behaves as init 1, and therefore, we cannot just pass a bash command to it. Execute the following: $ docker run -it --entrypoint /bin/bash ghcr.io/gdha/pi4-ntopng:5.7.0 This will drop us into a shell command from where we can peek around in the container to see what might be wrong. Indeed as the error above mentioned the ownership of /var/log/redis was wrong: root@131f7e6411c5:/var/log# ls -l total 692 -rw-r--r-- 1 root root 4901 Sep 29 15:13 alternatives.log drwxr-xr-x 1 root root 4096 Sep 29 15:12 apt -rw-r--r-- 1 root root 58584 Jan 26 2023 bootstrap.log -rw-rw---- 1 root utmp 0 Jan 26 2023 btmp -rw-r--r-- 1 root root 282997 Sep 29 15:14 dpkg.log -rw-r--r-- 1 root root 32000 Sep 29 15:14 faillog -rw-r--r-- 1 root root 484 Sep 29 15:12 fontconfig.log drwxr-sr-x 2 root root 4096 Sep 29 15:11 journal -rw-rw-r-- 1 root root 296000 Sep 29 15:14 lastlog drwx------ 2 root root 4096 Sep 29 15:11 private drwxr-s--- 1 root root 4096 Sep 29 15:12 redis -rw-rw-r-- 1 root utmp 0 Jan 26 2023 wtmp root@131f7e6411c5:/var/log# chown -R redis:redis /var/log/redis However, as we are inside the container lets try to start redis: root@131f7e6411c5:~# /etc/init.d/redis-server start Starting redis-server: failed Hum, not 100% okay it seems. However, as the log file of redis should be available now we can check it out: root@131f7e6411c5:~# cat /var/log/redis/redis-server.log 19:C 02 Oct 2023 07:11:40.148 # Can't chdir to '/var/lib/redis': Permission denied 33:C 02 Oct 2023 07:13:22.884 # Can't chdir to '/var/lib/redis': Permission denied 69:C 02 Oct 2023 07:17:39.861 # Can't chdir to '/var/lib/redis': Permission denied Indeed still another directory to be tackled: root@131f7e6411c5:~# chown -R redis:redis /var/lib/redis We can start redis again and see what it says: root@131f7e6411c5:~# /usr/bin/redis-server 37:C 02 Oct 2023 07:15:29.235 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 37:C 02 Oct 2023 07:15:29.236 # Redis version=5.0.7, bits=64, commit=00000000, modified=0, pid=37, just started 37:C 02 Oct 2023 07:15:29.236 # Warning: no config file specified, using the default config. In order to specify a config file use /usr/bin/redis-server /path/to/redis.conf _._ _.-``__ ''-._ _.-`` `. `_. ''-._ Redis 5.0.7 (00000000/0) 64 bit .-`` .-```. ```\\/ _.,_ ''-._ ( ' , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|'` _.-'| Port: 6379 | `-._ `._ / _.-' | PID: 37 `-._ `-._ `-./ _.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | http://redis.io `-._ `-._`-.__.-'_.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | `-._ `-._`-.__.-'_.-' _.-' `-._ `-.__.-' _.-' `-._ _.-' `-.__.-' 37:M 02 Oct 2023 07:15:29.258 # Server initialized 37:M 02 Oct 2023 07:15:29.258 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. 37:M 02 Oct 2023 07:15:29.259 * Ready to accept connections Yep, it seems to start, but the message around transparent_hugepage is quite interesting which we can take care of in our Dockerfile.","title":"How to debug a POD"},{"location":"pi-stories15/#references","text":"[1] PODs are stuck in terminating status [2] Troubleshooting: MountVolume.SetUp failed for volume due to multipathd on the node [3] Ten tips for debugging Docker containers","title":"References"},{"location":"pi-stories16/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Deploying uptime kuma with helm \u00b6 Download the GitHub sources of Uptime Kuma \u00b6 We forked the original GitHub Sources of https://github.com/k3rnelpan1c-dev/uptime-kuma-helm to make our adjustments [1]. If you wonder what you can do with Uptime Kuma, then the best description found is \" Uptime Kuma is an easy-to-use self-hosted monitoring tool. \" - a simple example is to watch a NAS system with a simple ping : The container image content \u00b6 The directory contains the Dockerfile to build the container image. Within Uptime Kuma we can use apprise which is an interesting tool to send out notifications that we will be using outside this project as well. Build the Uptime Kuma for ARM64 architecture \u00b6 We have set-up GitHub Actions to build a container image for Uptime Kuma for x86_64/arm64 based images . Before we used the GitHub Actions successfully, we wanted to have an image for ARM64 to build ourselves. We made a script container/build.sh to build the ARM64 based image: REL=${1:-v1.0} cat ~/.ghcr-token | docker login ghcr.io -u $USER --password-stdin echo \"Building pi4-uptime-kuma:$REL\" docker build --tag ghcr.io/gdha/pi4-uptime-kuma:$REL . docker tag ghcr.io/gdha/pi4-uptime-kuma:$REL ghcr.io/gdha/pi4-uptime-kuma:latest echo \"Pushing pi4-uptime-kuma:$REL to GitHub Docker Container registry\" docker push ghcr.io/gdha/pi4-uptime-kuma:$REL You can find the latest Pi4 images for ARM64 . However, as GitHub Actions is now capable of building both x86_64 and arm64 we prefer to use uptime-kuma images Introducing the Helm chart of the Uptime Kuma application \u00b6 Well this will be the first introduction with Helm charts [2]. Helm is a package manager for Kubernetes [3] and we were interesting to have some more knowledge about it. It might be better to clone our uptime-kuma repository and go into directory uptime-kuma to find the helm chart, which we will use file based and not from a helm repository. We can do that later with a local helm repository, but we wanted to play around with helm first. The helm chart of uptime kuma Chart.yaml [4]: apiVersion: v2 name: uptime-kuma description: A Helm chart for Uptime Kuma. A fancy self-hosted monitoring tool. # A chart can be either an 'application' or a 'library' chart. # # Application charts are a collection of templates that can be packaged into versioned archives # to be deployed. # # Library charts provide useful utilities or functions for the chart developer. They're included as # a dependency of application charts to inject those utilities and functions into the rendering # pipeline. Library charts do not define any templates and therefore cannot be deployed. type: application # This is the chart version. This version number should be incremented each time you make changes # to the chart and its templates, including the app version. # Versions are expected to follow Semantic Versioning (https://semver.org/) version: 1.0.6 # This is the version number of the (container) application being deployed. This version number should be # incremented each time you make changes to the application. Versions are not expected to # follow Semantic Versioning. They should reflect the version the application is using. # It is recommended to use it with quotes. # Use: docker image ls (to find the latest one) appVersion: \"1.23.2\" icon: https://github.com/louislam/uptime-kuma/blob/master/public/apple-touch-icon-precomposed.png sources: - https://github.com/k3rnelpan1c-dev/uptime-kuma-helm - https://github.com/louislam/uptime-kuma - https://helm.sh/docs/topics/charts/ And, the values.yaml file looks like: # Default values for uptime-kuma. # This is a YAML-formatted file. # Declare variables to be passed into your templates. ## Specify override namespace, specifically this is useful for using as sub-chart ## and its release namespace is not the `uptime-kuma` namespaceOverride: \"uptime-kuma\" image: registry: \"ghcr.io\" repository: \"gdha/uptime-kuma\" pullPolicy: IfNotPresent # Overrides the image tag whose default is the Chart's appVersion. tag: \"\" imagePullSecrets: [{name: dockerconfigjson-github-com}] nameOverride: \"\" fullnameOverride: \"\" statefulSet: labels: {} annotations: {} serviceAccount: # Specifies whether a service account should be created create: true # Annotations to add to the service account annotations: {} # The name of the service account to use. # If not set and create is true, a name is generated using the fullname template name: \"\" podAnnotations: {} podSecurityContext: {} # fsGroup: 2000 securityContext: {} # capabilities: # drop: # - ALL # readOnlyRootFilesystem: true # runAsNonRoot: true # runAsUser: 1000 livenessProbe: exec: command: - extra/healthcheck readinessProbe: httpGet: path: / port: http scheme: HTTP service: type: LoadBalancer port: 80 route: enabled: false annotations: {} host: \"\" labels: {} ingress: enabled: false className: \"\" annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" hosts: - host: chart-example.local paths: - path: / pathType: ImplementationSpecific tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 200m # memory: 256Mi # requests: # cpu: 100m # memory: 128Mi persistence: enabled: true claimNameOverwrite: \"\" sizeLimit: 4Gi storageClass: \"longhorn\" annotations: {} labels: {} nodeSelector: {} tolerations: [] affinity: {} testPod: image: docker.io/busybox@sha256:3fbc632167424a6d997e74f52b878d7cc478225cffac6bc977eedfe51c7f4e79 annotations: {} Go into the templates directory: n1:~/projects/pi4-uptime-kuma-helm/uptime-kuma/templates$ ll total 52 drwxrwxr-x 4 gdha gdha 4096 Sep 28 15:44 ./ drwxrwxr-x 3 gdha gdha 4096 Aug 11 16:28 ../ lrwxrwxrwx 1 gdha gdha 24 Aug 7 16:33 ghcr-secret.yaml -> .hidden/ghcr-secret.yaml -rw-rw-r-- 1 gdha gdha 2578 Feb 22 2023 _helpers.tpl drwxrwxr-x 2 gdha gdha 4096 Sep 28 15:44 .hidden/ -rw-rw-r-- 1 gdha gdha 1705 Feb 22 2023 ingress.yaml -rw-rw-r-- 1 gdha gdha 2140 Feb 22 2023 NOTES.txt -rw-rw-r-- 1 gdha gdha 732 Feb 22 2023 pvc.yaml -rw-rw-r-- 1 gdha gdha 817 Feb 22 2023 route.yaml -rw-rw-r-- 1 gdha gdha 215 Feb 22 2023 secrets.yaml -rw-rw-r-- 1 gdha gdha 381 Feb 22 2023 serviceaccount.yaml -rw-rw-r-- 1 gdha gdha 457 Feb 22 2023 service.yaml -rw-rw-r-- 1 gdha gdha 2557 Feb 22 2023 statefulSet.yaml drwxrwxr-x 2 gdha gdha 4096 Feb 22 2023 tests/ where you will find the needed yaml files to start-up the uptime-kuma pod in your k3s kubernetes cluster. Be aware that the ghcr-secret.yaml is hidden and you need to create this yourself under the .hidden directory as follow: kubectl create secret docker-registry dockerconfigjson-github-com --docker-server=ghcr.io --docker-username=$USER --docker-password=$(cat ~/.ghcr-token) --dry-run=client -oyaml >.hidden/ghcr-secret.yaml Of course, you must have saved your secret GitHub token in the file ~/.ghcr-token . To to be to use the helm chart we need to first to create the namespace and do a dry-run of our chart: $ pwd ~/projects/pi4-uptime-kuma-helm $ kubectl create namespace uptime-kuma $ helm install --dry-run --namespace uptime-kuma uptime-kuma ./uptime-kuma walk.go:74: found symbolic link in path: ~/projects/pi4-uptime-kuma-helm/uptime-kuma/templates/ghcr-secret.yaml resolves to ~/projects/pi4-uptime-kuma-helm/uptime-kuma/templates/.hidden/ghcr-secret.yaml. Contents of linked file included and used NAME: uptime-kuma LAST DEPLOYED: Thu Oct 5 16:40:04 2023 NAMESPACE: uptime-kuma STATUS: pending-install REVISION: 1 HOOKS: --- # Source: uptime-kuma/templates/tests/test-connection.yaml apiVersion: v1 kind: Pod metadata: name: \"uptime-kuma-test-connection\" namespace: uptime-kuma labels: helm.sh/chart: uptime-kuma-1.0.6 app.kubernetes.io/name: uptime-kuma app.kubernetes.io/instance: uptime-kuma app.kubernetes.io/version: \"1.22.1\" app.kubernetes.io/managed-by: Helm annotations: \"helm.sh/hook\": test spec: containers: - name: wget image: docker.io/busybox@sha256:3fbc632167424a6d997e74f52b878d7cc478225cffac6bc977eedfe51c7f4e79 command: [\"/bin/sh\"] args: ['uptime-kuma:80'] args: [\"-c\", \"cd tmp/ ; wget 'uptime-kuma:80'\"] restartPolicy: Never MANIFEST: --- # Source: uptime-kuma/templates/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: uptime-kuma namespace: uptime-kuma labels: helm.sh/chart: uptime-kuma-1.0.6 app.kubernetes.io/name: uptime-kuma app.kubernetes.io/instance: uptime-kuma app.kubernetes.io/version: \"1.22.1\" app.kubernetes.io/managed-by: Helm --- # Source: uptime-kuma/templates/ghcr-secret.yaml # kubectl create secret docker-registry dockerconfigjson-github-com --docker-server=ghcr.io --docker-username=$USER --docker-password=$(cat ~/.ghcr-token) --dry-run=client -oyaml >.hidden/ghcr-secret.yaml apiVersion: v1 data: .dockerconfigjson: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx kind: Secret metadata: creationTimestamp: null name: dockerconfigjson-github-com namespace: uptime-kuma labels: helm.sh/chart: uptime-kuma-1.0.6 app.kubernetes.io/name: uptime-kuma app.kubernetes.io/instance: uptime-kuma app.kubernetes.io/version: \"1.22.1\" app.kubernetes.io/managed-by: Helm type: kubernetes.io/dockerconfigjson --- # Source: uptime-kuma/templates/secrets.yaml apiVersion: v1 kind: Secret metadata: name: uptime-kuma namespace: uptime-kuma labels: helm.sh/chart: uptime-kuma-1.0.6 app.kubernetes.io/name: uptime-kuma app.kubernetes.io/instance: uptime-kuma app.kubernetes.io/version: \"1.22.1\" app.kubernetes.io/managed-by: Helm type: Opaque --- # Source: uptime-kuma/templates/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: uptime-kuma namespace: uptime-kuma annotations: labels: helm.sh/chart: uptime-kuma-1.0.6 app.kubernetes.io/name: uptime-kuma app.kubernetes.io/instance: uptime-kuma app.kubernetes.io/version: \"1.22.1\" app.kubernetes.io/managed-by: Helm spec: accessModes: - ReadWriteOnce resources: requests: storage: \"4Gi\" storageClassName: \"longhorn\" --- # Source: uptime-kuma/templates/service.yaml apiVersion: v1 kind: Service metadata: name: uptime-kuma namespace: uptime-kuma labels: helm.sh/chart: uptime-kuma-1.0.6 app.kubernetes.io/name: uptime-kuma app.kubernetes.io/instance: uptime-kuma app.kubernetes.io/version: \"1.22.1\" app.kubernetes.io/managed-by: Helm spec: type: LoadBalancer externalTrafficPolicy: Local ports: - port: 80 targetPort: http protocol: TCP name: http selector: app.kubernetes.io/name: uptime-kuma app.kubernetes.io/instance: uptime-kuma --- # Source: uptime-kuma/templates/statefulSet.yaml apiVersion: apps/v1 kind: StatefulSet metadata: name: uptime-kuma namespace: uptime-kuma labels: helm.sh/chart: uptime-kuma-1.0.6 app.kubernetes.io/name: uptime-kuma app.kubernetes.io/instance: uptime-kuma app.kubernetes.io/version: \"1.22.1\" app.kubernetes.io/managed-by: Helm annotations: spec: replicas: 1 serviceName: uptime-kuma selector: matchLabels: app.kubernetes.io/name: uptime-kuma app.kubernetes.io/instance: uptime-kuma template: metadata: labels: app.kubernetes.io/name: uptime-kuma app.kubernetes.io/instance: uptime-kuma spec: imagePullSecrets: - name: dockerconfigjson-github-com serviceAccountName: uptime-kuma securityContext: {} containers: - name: uptime-kuma securityContext: {} image: \"ghcr.io/gdha/uptime-kuma:1.22.1\" imagePullPolicy: IfNotPresent env: - name: UPTIME_KUMA_PORT value: '3001' - name: PORT value: '3001' ports: - name: http containerPort: 3001 protocol: TCP volumeMounts: - name: uptime-storage mountPath: /app/data livenessProbe: exec: command: - extra/healthcheck readinessProbe: httpGet: path: / port: http scheme: HTTP resources: {} volumes: - name: uptime-storage persistentVolumeClaim: claimName: uptime-kuma NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get --namespace uptime-kuma svc -w uptime-kuma' export SERVICE_IP=$(kubectl get svc --namespace uptime-kuma uptime-kuma --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\") echo http://$SERVICE_IP:80 To perform the real install remove the --dry-run option, and to get the external IP address do the following: $ kubectl get svc --namespace uptime-kuma uptime-kuma --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\" 192.168.0.234 We had an issue with the longhorn PVC with missing permissions - see the issue #16 In the meantime we fixed issue #16 with introducing an initContainer which mounts the longhorn volume and perform the chmod. Note: the uid 3310 is the one used in the Container building. To see if the initContainer worked well you can check the logs: $ kubectl logs -n uptime-kuma uptime-kuma-0 -c fix-volume-rights drwxr-xr-x 3 3310 3310 4096 Oct 6 09:51 /home To test if it works there is a simple test: $ curl http://192.168.0.234:80 Found. Redirecting to /dashboard Remove the uptime-kuma pod \u00b6 To delete the pod \"uptime-kuma\" you just have to execute the following: $ helm uninstall --debug --namespace uptime-kuma uptime-kuma uninstall.go:95: [debug] uninstall: Deleting uptime-kuma client.go:477: [debug] Starting delete for \"uptime-kuma\" Service client.go:477: [debug] Starting delete for \"uptime-kuma\" StatefulSet client.go:477: [debug] Starting delete for \"uptime-kuma\" PersistentVolumeClaim client.go:477: [debug] Starting delete for \"uptime-kuma\" Secret client.go:477: [debug] Starting delete for \"dockerconfigjson-github-com\" Secret client.go:477: [debug] Starting delete for \"uptime-kuma\" ServiceAccount uninstall.go:148: [debug] purge requested for uptime-kuma release \"uptime-kuma\" uninstalled References \u00b6 [1] https://github.com/gdha/pi4-uptime-kuma-helm/ [2] How to Create Helm Chart (Comprehensive Beginners Guide) [3] Introduction to Helm and Creating your first Helm Chart [4] Chart.yaml of Uptime Kuma [5] PVC longhorn has permission issues","title":"Raspberry Pi 4 Deploying uptime kuma with helm"},{"location":"pi-stories16/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories16/#raspberry_pi_4_cluster_series_-_deploying_uptime_kuma_with_helm","text":"","title":"Raspberry Pi 4 cluster Series - Deploying uptime kuma with helm"},{"location":"pi-stories16/#download_the_github_sources_of_uptime_kuma","text":"We forked the original GitHub Sources of https://github.com/k3rnelpan1c-dev/uptime-kuma-helm to make our adjustments [1]. If you wonder what you can do with Uptime Kuma, then the best description found is \" Uptime Kuma is an easy-to-use self-hosted monitoring tool. \" - a simple example is to watch a NAS system with a simple ping :","title":"Download the GitHub sources of Uptime Kuma"},{"location":"pi-stories16/#the_container_image_content","text":"The directory contains the Dockerfile to build the container image. Within Uptime Kuma we can use apprise which is an interesting tool to send out notifications that we will be using outside this project as well.","title":"The container image content"},{"location":"pi-stories16/#build_the_uptime_kuma_for_arm64_architecture","text":"We have set-up GitHub Actions to build a container image for Uptime Kuma for x86_64/arm64 based images . Before we used the GitHub Actions successfully, we wanted to have an image for ARM64 to build ourselves. We made a script container/build.sh to build the ARM64 based image: REL=${1:-v1.0} cat ~/.ghcr-token | docker login ghcr.io -u $USER --password-stdin echo \"Building pi4-uptime-kuma:$REL\" docker build --tag ghcr.io/gdha/pi4-uptime-kuma:$REL . docker tag ghcr.io/gdha/pi4-uptime-kuma:$REL ghcr.io/gdha/pi4-uptime-kuma:latest echo \"Pushing pi4-uptime-kuma:$REL to GitHub Docker Container registry\" docker push ghcr.io/gdha/pi4-uptime-kuma:$REL You can find the latest Pi4 images for ARM64 . However, as GitHub Actions is now capable of building both x86_64 and arm64 we prefer to use uptime-kuma images","title":"Build the Uptime Kuma for ARM64 architecture"},{"location":"pi-stories16/#introducing_the_helm_chart_of_the_uptime_kuma_application","text":"Well this will be the first introduction with Helm charts [2]. Helm is a package manager for Kubernetes [3] and we were interesting to have some more knowledge about it. It might be better to clone our uptime-kuma repository and go into directory uptime-kuma to find the helm chart, which we will use file based and not from a helm repository. We can do that later with a local helm repository, but we wanted to play around with helm first. The helm chart of uptime kuma Chart.yaml [4]: apiVersion: v2 name: uptime-kuma description: A Helm chart for Uptime Kuma. A fancy self-hosted monitoring tool. # A chart can be either an 'application' or a 'library' chart. # # Application charts are a collection of templates that can be packaged into versioned archives # to be deployed. # # Library charts provide useful utilities or functions for the chart developer. They're included as # a dependency of application charts to inject those utilities and functions into the rendering # pipeline. Library charts do not define any templates and therefore cannot be deployed. type: application # This is the chart version. This version number should be incremented each time you make changes # to the chart and its templates, including the app version. # Versions are expected to follow Semantic Versioning (https://semver.org/) version: 1.0.6 # This is the version number of the (container) application being deployed. This version number should be # incremented each time you make changes to the application. Versions are not expected to # follow Semantic Versioning. They should reflect the version the application is using. # It is recommended to use it with quotes. # Use: docker image ls (to find the latest one) appVersion: \"1.23.2\" icon: https://github.com/louislam/uptime-kuma/blob/master/public/apple-touch-icon-precomposed.png sources: - https://github.com/k3rnelpan1c-dev/uptime-kuma-helm - https://github.com/louislam/uptime-kuma - https://helm.sh/docs/topics/charts/ And, the values.yaml file looks like: # Default values for uptime-kuma. # This is a YAML-formatted file. # Declare variables to be passed into your templates. ## Specify override namespace, specifically this is useful for using as sub-chart ## and its release namespace is not the `uptime-kuma` namespaceOverride: \"uptime-kuma\" image: registry: \"ghcr.io\" repository: \"gdha/uptime-kuma\" pullPolicy: IfNotPresent # Overrides the image tag whose default is the Chart's appVersion. tag: \"\" imagePullSecrets: [{name: dockerconfigjson-github-com}] nameOverride: \"\" fullnameOverride: \"\" statefulSet: labels: {} annotations: {} serviceAccount: # Specifies whether a service account should be created create: true # Annotations to add to the service account annotations: {} # The name of the service account to use. # If not set and create is true, a name is generated using the fullname template name: \"\" podAnnotations: {} podSecurityContext: {} # fsGroup: 2000 securityContext: {} # capabilities: # drop: # - ALL # readOnlyRootFilesystem: true # runAsNonRoot: true # runAsUser: 1000 livenessProbe: exec: command: - extra/healthcheck readinessProbe: httpGet: path: / port: http scheme: HTTP service: type: LoadBalancer port: 80 route: enabled: false annotations: {} host: \"\" labels: {} ingress: enabled: false className: \"\" annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" hosts: - host: chart-example.local paths: - path: / pathType: ImplementationSpecific tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after 'resources:'. # limits: # cpu: 200m # memory: 256Mi # requests: # cpu: 100m # memory: 128Mi persistence: enabled: true claimNameOverwrite: \"\" sizeLimit: 4Gi storageClass: \"longhorn\" annotations: {} labels: {} nodeSelector: {} tolerations: [] affinity: {} testPod: image: docker.io/busybox@sha256:3fbc632167424a6d997e74f52b878d7cc478225cffac6bc977eedfe51c7f4e79 annotations: {} Go into the templates directory: n1:~/projects/pi4-uptime-kuma-helm/uptime-kuma/templates$ ll total 52 drwxrwxr-x 4 gdha gdha 4096 Sep 28 15:44 ./ drwxrwxr-x 3 gdha gdha 4096 Aug 11 16:28 ../ lrwxrwxrwx 1 gdha gdha 24 Aug 7 16:33 ghcr-secret.yaml -> .hidden/ghcr-secret.yaml -rw-rw-r-- 1 gdha gdha 2578 Feb 22 2023 _helpers.tpl drwxrwxr-x 2 gdha gdha 4096 Sep 28 15:44 .hidden/ -rw-rw-r-- 1 gdha gdha 1705 Feb 22 2023 ingress.yaml -rw-rw-r-- 1 gdha gdha 2140 Feb 22 2023 NOTES.txt -rw-rw-r-- 1 gdha gdha 732 Feb 22 2023 pvc.yaml -rw-rw-r-- 1 gdha gdha 817 Feb 22 2023 route.yaml -rw-rw-r-- 1 gdha gdha 215 Feb 22 2023 secrets.yaml -rw-rw-r-- 1 gdha gdha 381 Feb 22 2023 serviceaccount.yaml -rw-rw-r-- 1 gdha gdha 457 Feb 22 2023 service.yaml -rw-rw-r-- 1 gdha gdha 2557 Feb 22 2023 statefulSet.yaml drwxrwxr-x 2 gdha gdha 4096 Feb 22 2023 tests/ where you will find the needed yaml files to start-up the uptime-kuma pod in your k3s kubernetes cluster. Be aware that the ghcr-secret.yaml is hidden and you need to create this yourself under the .hidden directory as follow: kubectl create secret docker-registry dockerconfigjson-github-com --docker-server=ghcr.io --docker-username=$USER --docker-password=$(cat ~/.ghcr-token) --dry-run=client -oyaml >.hidden/ghcr-secret.yaml Of course, you must have saved your secret GitHub token in the file ~/.ghcr-token . To to be to use the helm chart we need to first to create the namespace and do a dry-run of our chart: $ pwd ~/projects/pi4-uptime-kuma-helm $ kubectl create namespace uptime-kuma $ helm install --dry-run --namespace uptime-kuma uptime-kuma ./uptime-kuma walk.go:74: found symbolic link in path: ~/projects/pi4-uptime-kuma-helm/uptime-kuma/templates/ghcr-secret.yaml resolves to ~/projects/pi4-uptime-kuma-helm/uptime-kuma/templates/.hidden/ghcr-secret.yaml. Contents of linked file included and used NAME: uptime-kuma LAST DEPLOYED: Thu Oct 5 16:40:04 2023 NAMESPACE: uptime-kuma STATUS: pending-install REVISION: 1 HOOKS: --- # Source: uptime-kuma/templates/tests/test-connection.yaml apiVersion: v1 kind: Pod metadata: name: \"uptime-kuma-test-connection\" namespace: uptime-kuma labels: helm.sh/chart: uptime-kuma-1.0.6 app.kubernetes.io/name: uptime-kuma app.kubernetes.io/instance: uptime-kuma app.kubernetes.io/version: \"1.22.1\" app.kubernetes.io/managed-by: Helm annotations: \"helm.sh/hook\": test spec: containers: - name: wget image: docker.io/busybox@sha256:3fbc632167424a6d997e74f52b878d7cc478225cffac6bc977eedfe51c7f4e79 command: [\"/bin/sh\"] args: ['uptime-kuma:80'] args: [\"-c\", \"cd tmp/ ; wget 'uptime-kuma:80'\"] restartPolicy: Never MANIFEST: --- # Source: uptime-kuma/templates/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: uptime-kuma namespace: uptime-kuma labels: helm.sh/chart: uptime-kuma-1.0.6 app.kubernetes.io/name: uptime-kuma app.kubernetes.io/instance: uptime-kuma app.kubernetes.io/version: \"1.22.1\" app.kubernetes.io/managed-by: Helm --- # Source: uptime-kuma/templates/ghcr-secret.yaml # kubectl create secret docker-registry dockerconfigjson-github-com --docker-server=ghcr.io --docker-username=$USER --docker-password=$(cat ~/.ghcr-token) --dry-run=client -oyaml >.hidden/ghcr-secret.yaml apiVersion: v1 data: .dockerconfigjson: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx kind: Secret metadata: creationTimestamp: null name: dockerconfigjson-github-com namespace: uptime-kuma labels: helm.sh/chart: uptime-kuma-1.0.6 app.kubernetes.io/name: uptime-kuma app.kubernetes.io/instance: uptime-kuma app.kubernetes.io/version: \"1.22.1\" app.kubernetes.io/managed-by: Helm type: kubernetes.io/dockerconfigjson --- # Source: uptime-kuma/templates/secrets.yaml apiVersion: v1 kind: Secret metadata: name: uptime-kuma namespace: uptime-kuma labels: helm.sh/chart: uptime-kuma-1.0.6 app.kubernetes.io/name: uptime-kuma app.kubernetes.io/instance: uptime-kuma app.kubernetes.io/version: \"1.22.1\" app.kubernetes.io/managed-by: Helm type: Opaque --- # Source: uptime-kuma/templates/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: uptime-kuma namespace: uptime-kuma annotations: labels: helm.sh/chart: uptime-kuma-1.0.6 app.kubernetes.io/name: uptime-kuma app.kubernetes.io/instance: uptime-kuma app.kubernetes.io/version: \"1.22.1\" app.kubernetes.io/managed-by: Helm spec: accessModes: - ReadWriteOnce resources: requests: storage: \"4Gi\" storageClassName: \"longhorn\" --- # Source: uptime-kuma/templates/service.yaml apiVersion: v1 kind: Service metadata: name: uptime-kuma namespace: uptime-kuma labels: helm.sh/chart: uptime-kuma-1.0.6 app.kubernetes.io/name: uptime-kuma app.kubernetes.io/instance: uptime-kuma app.kubernetes.io/version: \"1.22.1\" app.kubernetes.io/managed-by: Helm spec: type: LoadBalancer externalTrafficPolicy: Local ports: - port: 80 targetPort: http protocol: TCP name: http selector: app.kubernetes.io/name: uptime-kuma app.kubernetes.io/instance: uptime-kuma --- # Source: uptime-kuma/templates/statefulSet.yaml apiVersion: apps/v1 kind: StatefulSet metadata: name: uptime-kuma namespace: uptime-kuma labels: helm.sh/chart: uptime-kuma-1.0.6 app.kubernetes.io/name: uptime-kuma app.kubernetes.io/instance: uptime-kuma app.kubernetes.io/version: \"1.22.1\" app.kubernetes.io/managed-by: Helm annotations: spec: replicas: 1 serviceName: uptime-kuma selector: matchLabels: app.kubernetes.io/name: uptime-kuma app.kubernetes.io/instance: uptime-kuma template: metadata: labels: app.kubernetes.io/name: uptime-kuma app.kubernetes.io/instance: uptime-kuma spec: imagePullSecrets: - name: dockerconfigjson-github-com serviceAccountName: uptime-kuma securityContext: {} containers: - name: uptime-kuma securityContext: {} image: \"ghcr.io/gdha/uptime-kuma:1.22.1\" imagePullPolicy: IfNotPresent env: - name: UPTIME_KUMA_PORT value: '3001' - name: PORT value: '3001' ports: - name: http containerPort: 3001 protocol: TCP volumeMounts: - name: uptime-storage mountPath: /app/data livenessProbe: exec: command: - extra/healthcheck readinessProbe: httpGet: path: / port: http scheme: HTTP resources: {} volumes: - name: uptime-storage persistentVolumeClaim: claimName: uptime-kuma NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get --namespace uptime-kuma svc -w uptime-kuma' export SERVICE_IP=$(kubectl get svc --namespace uptime-kuma uptime-kuma --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\") echo http://$SERVICE_IP:80 To perform the real install remove the --dry-run option, and to get the external IP address do the following: $ kubectl get svc --namespace uptime-kuma uptime-kuma --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\" 192.168.0.234 We had an issue with the longhorn PVC with missing permissions - see the issue #16 In the meantime we fixed issue #16 with introducing an initContainer which mounts the longhorn volume and perform the chmod. Note: the uid 3310 is the one used in the Container building. To see if the initContainer worked well you can check the logs: $ kubectl logs -n uptime-kuma uptime-kuma-0 -c fix-volume-rights drwxr-xr-x 3 3310 3310 4096 Oct 6 09:51 /home To test if it works there is a simple test: $ curl http://192.168.0.234:80 Found. Redirecting to /dashboard","title":"Introducing the Helm chart of the Uptime Kuma application"},{"location":"pi-stories16/#remove_the_uptime-kuma_pod","text":"To delete the pod \"uptime-kuma\" you just have to execute the following: $ helm uninstall --debug --namespace uptime-kuma uptime-kuma uninstall.go:95: [debug] uninstall: Deleting uptime-kuma client.go:477: [debug] Starting delete for \"uptime-kuma\" Service client.go:477: [debug] Starting delete for \"uptime-kuma\" StatefulSet client.go:477: [debug] Starting delete for \"uptime-kuma\" PersistentVolumeClaim client.go:477: [debug] Starting delete for \"uptime-kuma\" Secret client.go:477: [debug] Starting delete for \"dockerconfigjson-github-com\" Secret client.go:477: [debug] Starting delete for \"uptime-kuma\" ServiceAccount uninstall.go:148: [debug] purge requested for uptime-kuma release \"uptime-kuma\" uninstalled","title":"Remove the uptime-kuma pod"},{"location":"pi-stories16/#references","text":"[1] https://github.com/gdha/pi4-uptime-kuma-helm/ [2] How to Create Helm Chart (Comprehensive Beginners Guide) [3] Introduction to Helm and Creating your first Helm Chart [4] Chart.yaml of Uptime Kuma [5] PVC longhorn has permission issues","title":"References"},{"location":"pi-stories17/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Deploying ntopng with helm \u00b6 Download the GitHub sources of ntopng \u00b6 We liked the ntopng application [1] so we thought why not integrate it with our kubernetes cluster. However, the original project did not have the required code to integrate it with our kubernetes cluster, but we did find another project that provides a helm chart for ntopng. Yet another challenge was building an image for arm64. Therefore, we cloned these 2 projects into our pi4-ntopng github project [2]. We have 2 ways to build a pi4_ntopng container. One with the build.sh script which uses the ntopng package which available in ubuntu 20.04 repository (currently version 3.8.190813 or v1.5). The second way is building from scratch (from the sources of https://github.com/ntop/ntopng dev branch) with the script builder.sh which uses the development version (beginning of October 2023 it is version 5.7.0 or v1.9). Build pi4-ntopng with build.sh script (arm64) \u00b6 We built the pi4_ntopng with the build.sh script that uses the ntopng executable provided by the operating system used by the container (in our case ubuntu 20). To test the image (version 3.8.190813) with docker (before trying to integrate it within kubernetes) we can do the following: $ docker run --net=host -t -p 3000:3000 ghcr.io/gdha/pi4-ntopng:v1.5 WARNING: Published ports are discarded when using host network mode Starting redis-server: redis-server. 23/Feb/2023 15:54:51 [Ntop.cpp:1902] Setting local networks to 127.0.0.0/8 23/Feb/2023 15:54:51 [Redis.cpp:127] Successfully connected to redis 127.0.0.1:6379@0 23/Feb/2023 15:54:51 [Redis.cpp:127] Successfully connected to redis 127.0.0.1:6379@0 23/Feb/2023 15:54:52 [PcapInterface.cpp:93] Reading packets from interface veth51dfafac... 23/Feb/2023 15:54:52 [Ntop.cpp:1996] Registered interface veth51dfafac [id: 1] sh: 1: netstat: not found 23/Feb/2023 15:54:52 [PcapInterface.cpp:93] Reading packets from interface eth0... 23/Feb/2023 15:54:52 [Ntop.cpp:1996] Registered interface eth0 [id: 2] sh: 1: netstat: not found 23/Feb/2023 15:54:52 [PcapInterface.cpp:93] Reading packets from interface cni0... 23/Feb/2023 15:54:52 [Ntop.cpp:1996] Registered interface cni0 [id: 3] sh: 1: netstat: not found 23/Feb/2023 15:54:53 [PcapInterface.cpp:93] Reading packets from interface flannel.1... 23/Feb/2023 15:54:53 [Ntop.cpp:1996] Registered interface flannel.1 [id: 4] 23/Feb/2023 15:54:53 [PcapInterface.cpp:93] Reading packets from interface vethfa36f5b1... 23/Feb/2023 15:54:53 [Ntop.cpp:1996] Registered interface vethfa36f5b1 [id: 5] 23/Feb/2023 15:54:53 [PcapInterface.cpp:93] Reading packets from interface veth8ab316d2... 23/Feb/2023 15:54:53 [Ntop.cpp:1996] Registered interface veth8ab316d2 [id: 6] 23/Feb/2023 15:54:53 [PcapInterface.cpp:93] Reading packets from interface vethcbc7a4d3... 23/Feb/2023 15:54:53 [Ntop.cpp:1996] Registered interface vethcbc7a4d3 [id: 7] 23/Feb/2023 15:54:54 [PcapInterface.cpp:93] Reading packets from interface veth4b6b52e6... 23/Feb/2023 15:54:54 [Ntop.cpp:1996] Registered interface veth4b6b52e6 [id: 8] 23/Feb/2023 15:54:54 [PcapInterface.cpp:93] Reading packets from interface veth78a0b0e7... 23/Feb/2023 15:54:54 [Ntop.cpp:1996] Registered interface veth78a0b0e7 [id: 9] 23/Feb/2023 15:54:55 [PcapInterface.cpp:93] Reading packets from interface veth6e8b72d7... 23/Feb/2023 15:54:55 [Ntop.cpp:1996] Registered interface veth6e8b72d7 [id: 10] 23/Feb/2023 15:54:55 [PcapInterface.cpp:93] Reading packets from interface veth9cbeefd9... 23/Feb/2023 15:54:55 [Ntop.cpp:1996] Registered interface veth9cbeefd9 [id: 11] 23/Feb/2023 15:54:55 [PcapInterface.cpp:93] Reading packets from interface veth1f978b38... 23/Feb/2023 15:54:55 [Ntop.cpp:1996] Registered interface veth1f978b38 [id: 12] 23/Feb/2023 15:54:55 [PcapInterface.cpp:93] Reading packets from interface veth95b9e068... 23/Feb/2023 15:54:55 [Ntop.cpp:1996] Registered interface veth95b9e068 [id: 13] 23/Feb/2023 15:54:55 [PcapInterface.cpp:93] Reading packets from interface vethc58d8e82... 23/Feb/2023 15:54:55 [Ntop.cpp:1996] Registered interface vethc58d8e82 [id: 14] 23/Feb/2023 15:54:55 [PcapInterface.cpp:93] Reading packets from interface veth06e8023d... 23/Feb/2023 15:54:55 [Ntop.cpp:1996] Registered interface veth06e8023d [id: 15] 23/Feb/2023 15:54:55 [PcapInterface.cpp:93] Reading packets from interface vethed261334... 23/Feb/2023 15:54:55 [Ntop.cpp:1996] Registered interface vethed261334 [id: 16] 23/Feb/2023 15:54:56 [PcapInterface.cpp:93] Reading packets from interface vethce929971... 23/Feb/2023 15:54:56 [Ntop.cpp:1996] Registered interface vethce929971 [id: 17] 23/Feb/2023 15:54:56 [PcapInterface.cpp:93] Reading packets from interface vethd9135091... 23/Feb/2023 15:54:56 [Ntop.cpp:1996] Registered interface vethd9135091 [id: 18] 23/Feb/2023 15:54:56 [PcapInterface.cpp:93] Reading packets from interface lo... 23/Feb/2023 15:54:56 [Ntop.cpp:1996] Registered interface lo [id: 19] sh: 1: netstat: not found 23/Feb/2023 15:54:56 [PcapInterface.cpp:93] Reading packets from interface docker0... 23/Feb/2023 15:54:56 [Ntop.cpp:1996] Registered interface docker0 [id: 20] 23/Feb/2023 15:54:56 [main.cpp:308] PID stored in file /var/run/ntopng.pid 23/Feb/2023 15:54:57 [HTTPserver.cpp:1029] HTTPS Disabled: missing SSL certificate /usr/share/ntopng/httpdocs/ssl/ntopng-cert.pem 23/Feb/2023 15:54:57 [HTTPserver.cpp:1031] Please read https://github.com/ntop/ntopng/blob/dev/doc/README.SSL if you want to enable SSL. 23/Feb/2023 15:54:57 [Utils.cpp:563] WARNING: Unable to retain privileges for privileged file writing 23/Feb/2023 15:54:57 [Utils.cpp:592] User changed to ntopng 23/Feb/2023 15:54:57 [HTTPserver.cpp:1198] Web server dirs [/usr/share/ntopng/httpdocs][/usr/share/ntopng/scripts] 23/Feb/2023 15:54:57 [HTTPserver.cpp:1201] HTTP server listening on 3000 23/Feb/2023 15:54:57 [main.cpp:390] Working directory: /var/lib/ntopng 23/Feb/2023 15:54:57 [main.cpp:392] Scripts/HTML pages directory: /usr/share/ntopng 23/Feb/2023 15:54:57 [Ntop.cpp:403] Welcome to ntopng aarch64 v.3.8.190813 - (C) 1998-18 ntop.org 23/Feb/2023 15:54:57 [Ntop.cpp:717] Adding 127.0.0.1/32 as IPv4 interface address for lo 23/Feb/2023 15:54:57 [Ntop.cpp:725] Adding 127.0.0.0/8 as IPv4 local network for lo 23/Feb/2023 15:54:57 [Ntop.cpp:717] Adding 192.168.0.201/32 as IPv4 interface address for eth0 23/Feb/2023 15:54:57 [Ntop.cpp:725] Adding 192.168.0.0/24 as IPv4 local network for eth0 23/Feb/2023 15:54:57 [Ntop.cpp:717] Adding 172.17.0.1/32 as IPv4 interface address for docker0 23/Feb/2023 15:54:57 [Ntop.cpp:725] Adding 172.17.0.0/16 as IPv4 local network for docker0 23/Feb/2023 15:54:57 [Ntop.cpp:717] Adding 10.42.0.0/32 as IPv4 interface address for flannel.1 23/Feb/2023 15:54:57 [Ntop.cpp:725] Adding 10.42.0.0/32 as IPv4 local network for flannel.1 23/Feb/2023 15:54:57 [Ntop.cpp:717] Adding 10.42.0.1/32 as IPv4 interface address for cni0 23/Feb/2023 15:54:57 [Ntop.cpp:725] Adding 10.42.0.0/24 as IPv4 local network for cni0 23/Feb/2023 15:54:57 [Ntop.cpp:744] Adding fd28:f4dd:39e1:0:dea6:32ff:fe87:61f/128 as IPv6 interface address for eth0 23/Feb/2023 15:54:57 [Ntop.cpp:753] Adding fd28:f4dd:39e1:0:dea6:32ff:fe87:61f/64 as IPv6 local network for eth0 23/Feb/2023 15:54:57 [Ntop.cpp:744] Adding 2a02:a03f:837b:a800:dea6:32ff:fe87:61f/128 as IPv6 interface address for eth0 23/Feb/2023 15:54:57 [Ntop.cpp:753] Adding 2a02:a03f:837b:a800:dea6:32ff:fe87:61f/64 as IPv6 local network for eth0 23/Feb/2023 15:54:57 [Ntop.cpp:744] Adding fe80::dea6:32ff:fe87:61f/128 as IPv6 interface address for eth0 23/Feb/2023 15:54:57 [Ntop.cpp:753] Adding fe80::dea6:32ff:fe87:61f/64 as IPv6 local network for eth0 23/Feb/2023 15:55:11 [PeriodicActivities.cpp:72] Started periodic activities loop... 23/Feb/2023 15:55:12 [PeriodicActivities.cpp:113] Each periodic activity script will use 5 threads 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface veth51dfafac [id: 1]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface eth0 [id: 2]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface cni0 [id: 3]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface flannel.1 [id: 4]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface vethfa36f5b1 [id: 5]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface veth8ab316d2 [id: 6]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface vethcbc7a4d3 [id: 7]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface veth4b6b52e6 [id: 8]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface veth78a0b0e7 [id: 9]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface veth6e8b72d7 [id: 10]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface veth9cbeefd9 [id: 11]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface veth1f978b38 [id: 12]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface veth95b9e068 [id: 13]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface vethc58d8e82 [id: 14]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface veth06e8023d [id: 15]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface vethed261334 [id: 16]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface vethce929971 [id: 17]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface vethd9135091 [id: 18]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface lo [id: 19]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface docker0 [id: 20]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2007] Invalid packet received [len: 2862][max len: 1468]. 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K vethed261334 gro off gso off tso off 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2007] Invalid packet received [len: 6018][max len: 1468]. 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K veth8ab316d2 gro off gso off tso off 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2007] Invalid packet received [len: 2764][max len: 1468]. 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K veth4b6b52e6 gro off gso off tso off 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2007] Invalid packet received [len: 2381][max len: 1468]. 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K veth78a0b0e7 gro off gso off tso off 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2007] Invalid packet received [len: 2381][max len: 1468]. 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K cni0 gro off gso off tso off 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2007] Invalid packet received [len: 4162][max len: 1468]. 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K veth1f978b38 gro off gso off tso off 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2007] Invalid packet received [len: 2862][max len: 1468]. 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K veth95b9e068 gro off gso off tso off 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2007] Invalid packet received [len: 5658][max len: 1468]. 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K flannel.1 gro off gso off tso off 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2007] Invalid packet received [len: 2862][max len: 1468]. 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K vethc58d8e82 gro off gso off tso off 23/Feb/2023 15:55:13 [NetworkInterface.cpp:2007] Invalid packet received [len: 5708][max len: 1518]. 23/Feb/2023 15:55:13 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:13 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K eth0 gro off gso off tso off 23/Feb/2023 15:55:14 [NetworkInterface.cpp:2007] Invalid packet received [len: 4193][max len: 1468]. 23/Feb/2023 15:55:14 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:14 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K veth06e8023d gro off gso off tso off 23/Feb/2023 15:58:35 [NetworkInterface.cpp:2007] Invalid packet received [len: 1690][max len: 1468]. 23/Feb/2023 15:58:35 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:58:35 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K vethd9135091 gro off gso off tso off 23/Feb/2023 15:58:35 [NetworkInterface.cpp:2007] Invalid packet received [len: 1690][max len: 1468]. 23/Feb/2023 15:58:35 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:58:35 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K veth9cbeefd9 gro off gso off tso off Open a browser and access the ntopng application via URL http:<ip address of docker host>:3000/ Build pi4-ntopng from the sources with builder.sh script (arm64) \u00b6 The builder.sh script is using the Dockerfile.builder to build the ntopng executable from scratch and it was a serious battle to get it working, but we are proud on the result. By using the builder.sh script we also get the complete log saved as builder.log which might be useful when something went wrong. It might be that the compilation fails as we are using the development branch of the ntopng project. If all goed well we always have the state of art application of ntopng. To test with docker use the same method as described above with the build.sh script. Open a browser and access the ntopng application via URL http:<ip address of docker host>:3000/ Use the kubernetes helm chart to get it into kubernetes \u00b6 Go into directory kubernetes-helm-chart-ntopng and review the kubernetes/Chart.yaml and kubernetes/values.yaml files. We have pinned it to use version 5.7.0 . $ kubectl create -f ./kubernetes/namespace.yaml $ helm install --debug --dry-run --namespace ntopng ntopng ./kubernetes install.go:193: [debug] Original chart version: \"\" install.go:210: [debug] CHART PATH: /home/gdha/projects/pi4-ntopng/kubernetes-helm-chart-ntopng/kubernetes walk.go:74: found symbolic link in path: /home/gdha/projects/pi4-ntopng/kubernetes-helm-chart-ntopng/kubernetes/templates/ghcr-secret.yaml resolves to /home/gdha/projects/pi4-ntopng/kubernetes-helm-chart-ntopng/kubernetes/templates/.hidden/ghcr-secret.yaml. Contents of linked file included and used NAME: ntopng LAST DEPLOYED: Fri Oct 6 16:25:29 2023 NAMESPACE: ntopng STATUS: pending-install REVISION: 1 TEST SUITE: None USER-SUPPLIED VALUES: {} COMPUTED VALUES: ntopngConfig: |- --disable-login=1 --dns-mode=3 # Limit memory usage --max-num-flows=200000 --max-num-hosts=250000 #--interface=xxxxxx --no-promisc ntopngImageName: ghcr.io/gdha/pi4-ntopng ntopngImageVersion: 5.7.0 ntopngNodeSelector: kubernetes.io/os: linux ntopngResources: null ntopngService: port: 80 type: LoadBalancer HOOKS: MANIFEST: --- # Source: ntopng/templates/ghcr-secret.yaml # kubectl create secret docker-registry dockerconfigjson-github-com --docker-server=ghcr.io --docker-username=$USER --docker-password=$(cat ~/.ghcr-token) --dry-run=client -oyaml >.hidden/ghcr-secret.yaml # ln -s .hidden/ghcr-secret.yaml ghcr-secret.yaml # Edit ghcr-secret.yaml and modify namespace and labels in the metadata section. apiVersion: v1 kind: Secret metadata: name: ntopng-ghrc namespace: ntopng labels: app: ntopng chart: ntopng-5.7.0 release: ntopng heritage: Helm creationTimestamp: null type: kubernetes.io/dockerconfigjson data: .dockerconfigjson: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx --- # Source: ntopng/templates/secrets.yaml apiVersion: v1 kind: Secret metadata: name: ntopng namespace: ntopng labels: app: ntopng chart: ntopng-5.7.0 release: ntopng heritage: Helm type: Opaque --- # Source: ntopng/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: ntopng namespace: ntopng labels: app: ntopng chart: ntopng-5.7.0 release: ntopng heritage: Helm data: ntopng.conf: |- --disable-login=1 --dns-mode=3 # Limit memory usage --max-num-flows=200000 --max-num-hosts=250000 #--interface=xxxxxx --no-promisc --- # Source: ntopng/templates/service.yaml apiVersion: v1 kind: Service metadata: name: ntopng namespace: ntopng labels: app: ntopng chart: ntopng-5.7.0 release: ntopng heritage: Helm spec: type: LoadBalancer externalTrafficPolicy: Local selector: app: ntopng ports: - name: ntopng port: 80 targetPort: 3000 protocol: TCP --- # Source: ntopng/templates/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: ntopng namespace: ntopng labels: app: ntopng chart: ntopng-5.7.0 release: ntopng heritage: Helm spec: replicas: 1 selector: matchLabels: app: ntopng template: metadata: labels: app: ntopng chart: ntopng-5.7.0 release: ntopng heritage: Helm spec: terminationGracePeriodSeconds: 10 hostNetwork: true nodeSelector: kubernetes.io/os: \"linux\" imagePullSecrets: - name: ntopng-ghrc containers: - name: ntopng image: ghcr.io/gdha/pi4-ntopng:5.7.0 imagePullPolicy: IfNotPresent ports: - name: ntopng containerPort: 3000 protocol: TCP resources: null env: - name: CONFIG value: /ntopng/ntopng.conf volumeMounts: - name: config mountPath: /ntopng volumes: - name: config configMap: name: ntopng To find the endpoint IP address to point to in the web browser do the following: $ kubectl get endpoints -n ntopng NAME ENDPOINTS AGE ntopng 192.168.0.205:3000 32h Rederences \u00b6 [1] GitHub project ntopng [2] GitHub project ntopng with helm chart for arm64","title":"Raspberry Pi 4 Deploying ntopng with helm"},{"location":"pi-stories17/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories17/#raspberry_pi_4_cluster_series_-_deploying_ntopng_with_helm","text":"","title":"Raspberry Pi 4 cluster Series - Deploying ntopng with helm"},{"location":"pi-stories17/#download_the_github_sources_of_ntopng","text":"We liked the ntopng application [1] so we thought why not integrate it with our kubernetes cluster. However, the original project did not have the required code to integrate it with our kubernetes cluster, but we did find another project that provides a helm chart for ntopng. Yet another challenge was building an image for arm64. Therefore, we cloned these 2 projects into our pi4-ntopng github project [2]. We have 2 ways to build a pi4_ntopng container. One with the build.sh script which uses the ntopng package which available in ubuntu 20.04 repository (currently version 3.8.190813 or v1.5). The second way is building from scratch (from the sources of https://github.com/ntop/ntopng dev branch) with the script builder.sh which uses the development version (beginning of October 2023 it is version 5.7.0 or v1.9).","title":"Download the GitHub sources of ntopng"},{"location":"pi-stories17/#build_pi4-ntopng_with_buildsh_script_arm64","text":"We built the pi4_ntopng with the build.sh script that uses the ntopng executable provided by the operating system used by the container (in our case ubuntu 20). To test the image (version 3.8.190813) with docker (before trying to integrate it within kubernetes) we can do the following: $ docker run --net=host -t -p 3000:3000 ghcr.io/gdha/pi4-ntopng:v1.5 WARNING: Published ports are discarded when using host network mode Starting redis-server: redis-server. 23/Feb/2023 15:54:51 [Ntop.cpp:1902] Setting local networks to 127.0.0.0/8 23/Feb/2023 15:54:51 [Redis.cpp:127] Successfully connected to redis 127.0.0.1:6379@0 23/Feb/2023 15:54:51 [Redis.cpp:127] Successfully connected to redis 127.0.0.1:6379@0 23/Feb/2023 15:54:52 [PcapInterface.cpp:93] Reading packets from interface veth51dfafac... 23/Feb/2023 15:54:52 [Ntop.cpp:1996] Registered interface veth51dfafac [id: 1] sh: 1: netstat: not found 23/Feb/2023 15:54:52 [PcapInterface.cpp:93] Reading packets from interface eth0... 23/Feb/2023 15:54:52 [Ntop.cpp:1996] Registered interface eth0 [id: 2] sh: 1: netstat: not found 23/Feb/2023 15:54:52 [PcapInterface.cpp:93] Reading packets from interface cni0... 23/Feb/2023 15:54:52 [Ntop.cpp:1996] Registered interface cni0 [id: 3] sh: 1: netstat: not found 23/Feb/2023 15:54:53 [PcapInterface.cpp:93] Reading packets from interface flannel.1... 23/Feb/2023 15:54:53 [Ntop.cpp:1996] Registered interface flannel.1 [id: 4] 23/Feb/2023 15:54:53 [PcapInterface.cpp:93] Reading packets from interface vethfa36f5b1... 23/Feb/2023 15:54:53 [Ntop.cpp:1996] Registered interface vethfa36f5b1 [id: 5] 23/Feb/2023 15:54:53 [PcapInterface.cpp:93] Reading packets from interface veth8ab316d2... 23/Feb/2023 15:54:53 [Ntop.cpp:1996] Registered interface veth8ab316d2 [id: 6] 23/Feb/2023 15:54:53 [PcapInterface.cpp:93] Reading packets from interface vethcbc7a4d3... 23/Feb/2023 15:54:53 [Ntop.cpp:1996] Registered interface vethcbc7a4d3 [id: 7] 23/Feb/2023 15:54:54 [PcapInterface.cpp:93] Reading packets from interface veth4b6b52e6... 23/Feb/2023 15:54:54 [Ntop.cpp:1996] Registered interface veth4b6b52e6 [id: 8] 23/Feb/2023 15:54:54 [PcapInterface.cpp:93] Reading packets from interface veth78a0b0e7... 23/Feb/2023 15:54:54 [Ntop.cpp:1996] Registered interface veth78a0b0e7 [id: 9] 23/Feb/2023 15:54:55 [PcapInterface.cpp:93] Reading packets from interface veth6e8b72d7... 23/Feb/2023 15:54:55 [Ntop.cpp:1996] Registered interface veth6e8b72d7 [id: 10] 23/Feb/2023 15:54:55 [PcapInterface.cpp:93] Reading packets from interface veth9cbeefd9... 23/Feb/2023 15:54:55 [Ntop.cpp:1996] Registered interface veth9cbeefd9 [id: 11] 23/Feb/2023 15:54:55 [PcapInterface.cpp:93] Reading packets from interface veth1f978b38... 23/Feb/2023 15:54:55 [Ntop.cpp:1996] Registered interface veth1f978b38 [id: 12] 23/Feb/2023 15:54:55 [PcapInterface.cpp:93] Reading packets from interface veth95b9e068... 23/Feb/2023 15:54:55 [Ntop.cpp:1996] Registered interface veth95b9e068 [id: 13] 23/Feb/2023 15:54:55 [PcapInterface.cpp:93] Reading packets from interface vethc58d8e82... 23/Feb/2023 15:54:55 [Ntop.cpp:1996] Registered interface vethc58d8e82 [id: 14] 23/Feb/2023 15:54:55 [PcapInterface.cpp:93] Reading packets from interface veth06e8023d... 23/Feb/2023 15:54:55 [Ntop.cpp:1996] Registered interface veth06e8023d [id: 15] 23/Feb/2023 15:54:55 [PcapInterface.cpp:93] Reading packets from interface vethed261334... 23/Feb/2023 15:54:55 [Ntop.cpp:1996] Registered interface vethed261334 [id: 16] 23/Feb/2023 15:54:56 [PcapInterface.cpp:93] Reading packets from interface vethce929971... 23/Feb/2023 15:54:56 [Ntop.cpp:1996] Registered interface vethce929971 [id: 17] 23/Feb/2023 15:54:56 [PcapInterface.cpp:93] Reading packets from interface vethd9135091... 23/Feb/2023 15:54:56 [Ntop.cpp:1996] Registered interface vethd9135091 [id: 18] 23/Feb/2023 15:54:56 [PcapInterface.cpp:93] Reading packets from interface lo... 23/Feb/2023 15:54:56 [Ntop.cpp:1996] Registered interface lo [id: 19] sh: 1: netstat: not found 23/Feb/2023 15:54:56 [PcapInterface.cpp:93] Reading packets from interface docker0... 23/Feb/2023 15:54:56 [Ntop.cpp:1996] Registered interface docker0 [id: 20] 23/Feb/2023 15:54:56 [main.cpp:308] PID stored in file /var/run/ntopng.pid 23/Feb/2023 15:54:57 [HTTPserver.cpp:1029] HTTPS Disabled: missing SSL certificate /usr/share/ntopng/httpdocs/ssl/ntopng-cert.pem 23/Feb/2023 15:54:57 [HTTPserver.cpp:1031] Please read https://github.com/ntop/ntopng/blob/dev/doc/README.SSL if you want to enable SSL. 23/Feb/2023 15:54:57 [Utils.cpp:563] WARNING: Unable to retain privileges for privileged file writing 23/Feb/2023 15:54:57 [Utils.cpp:592] User changed to ntopng 23/Feb/2023 15:54:57 [HTTPserver.cpp:1198] Web server dirs [/usr/share/ntopng/httpdocs][/usr/share/ntopng/scripts] 23/Feb/2023 15:54:57 [HTTPserver.cpp:1201] HTTP server listening on 3000 23/Feb/2023 15:54:57 [main.cpp:390] Working directory: /var/lib/ntopng 23/Feb/2023 15:54:57 [main.cpp:392] Scripts/HTML pages directory: /usr/share/ntopng 23/Feb/2023 15:54:57 [Ntop.cpp:403] Welcome to ntopng aarch64 v.3.8.190813 - (C) 1998-18 ntop.org 23/Feb/2023 15:54:57 [Ntop.cpp:717] Adding 127.0.0.1/32 as IPv4 interface address for lo 23/Feb/2023 15:54:57 [Ntop.cpp:725] Adding 127.0.0.0/8 as IPv4 local network for lo 23/Feb/2023 15:54:57 [Ntop.cpp:717] Adding 192.168.0.201/32 as IPv4 interface address for eth0 23/Feb/2023 15:54:57 [Ntop.cpp:725] Adding 192.168.0.0/24 as IPv4 local network for eth0 23/Feb/2023 15:54:57 [Ntop.cpp:717] Adding 172.17.0.1/32 as IPv4 interface address for docker0 23/Feb/2023 15:54:57 [Ntop.cpp:725] Adding 172.17.0.0/16 as IPv4 local network for docker0 23/Feb/2023 15:54:57 [Ntop.cpp:717] Adding 10.42.0.0/32 as IPv4 interface address for flannel.1 23/Feb/2023 15:54:57 [Ntop.cpp:725] Adding 10.42.0.0/32 as IPv4 local network for flannel.1 23/Feb/2023 15:54:57 [Ntop.cpp:717] Adding 10.42.0.1/32 as IPv4 interface address for cni0 23/Feb/2023 15:54:57 [Ntop.cpp:725] Adding 10.42.0.0/24 as IPv4 local network for cni0 23/Feb/2023 15:54:57 [Ntop.cpp:744] Adding fd28:f4dd:39e1:0:dea6:32ff:fe87:61f/128 as IPv6 interface address for eth0 23/Feb/2023 15:54:57 [Ntop.cpp:753] Adding fd28:f4dd:39e1:0:dea6:32ff:fe87:61f/64 as IPv6 local network for eth0 23/Feb/2023 15:54:57 [Ntop.cpp:744] Adding 2a02:a03f:837b:a800:dea6:32ff:fe87:61f/128 as IPv6 interface address for eth0 23/Feb/2023 15:54:57 [Ntop.cpp:753] Adding 2a02:a03f:837b:a800:dea6:32ff:fe87:61f/64 as IPv6 local network for eth0 23/Feb/2023 15:54:57 [Ntop.cpp:744] Adding fe80::dea6:32ff:fe87:61f/128 as IPv6 interface address for eth0 23/Feb/2023 15:54:57 [Ntop.cpp:753] Adding fe80::dea6:32ff:fe87:61f/64 as IPv6 local network for eth0 23/Feb/2023 15:55:11 [PeriodicActivities.cpp:72] Started periodic activities loop... 23/Feb/2023 15:55:12 [PeriodicActivities.cpp:113] Each periodic activity script will use 5 threads 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface veth51dfafac [id: 1]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface eth0 [id: 2]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface cni0 [id: 3]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface flannel.1 [id: 4]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface vethfa36f5b1 [id: 5]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface veth8ab316d2 [id: 6]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface vethcbc7a4d3 [id: 7]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface veth4b6b52e6 [id: 8]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface veth78a0b0e7 [id: 9]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface veth6e8b72d7 [id: 10]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface veth9cbeefd9 [id: 11]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface veth1f978b38 [id: 12]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface veth95b9e068 [id: 13]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface vethc58d8e82 [id: 14]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface veth06e8023d [id: 15]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface vethed261334 [id: 16]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface vethce929971 [id: 17]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface vethd9135091 [id: 18]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface lo [id: 19]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2577] Started packet polling on interface docker0 [id: 20]... 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2007] Invalid packet received [len: 2862][max len: 1468]. 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K vethed261334 gro off gso off tso off 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2007] Invalid packet received [len: 6018][max len: 1468]. 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K veth8ab316d2 gro off gso off tso off 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2007] Invalid packet received [len: 2764][max len: 1468]. 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K veth4b6b52e6 gro off gso off tso off 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2007] Invalid packet received [len: 2381][max len: 1468]. 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K veth78a0b0e7 gro off gso off tso off 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2007] Invalid packet received [len: 2381][max len: 1468]. 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K cni0 gro off gso off tso off 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2007] Invalid packet received [len: 4162][max len: 1468]. 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K veth1f978b38 gro off gso off tso off 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2007] Invalid packet received [len: 2862][max len: 1468]. 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K veth95b9e068 gro off gso off tso off 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2007] Invalid packet received [len: 5658][max len: 1468]. 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K flannel.1 gro off gso off tso off 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2007] Invalid packet received [len: 2862][max len: 1468]. 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:12 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K vethc58d8e82 gro off gso off tso off 23/Feb/2023 15:55:13 [NetworkInterface.cpp:2007] Invalid packet received [len: 5708][max len: 1518]. 23/Feb/2023 15:55:13 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:13 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K eth0 gro off gso off tso off 23/Feb/2023 15:55:14 [NetworkInterface.cpp:2007] Invalid packet received [len: 4193][max len: 1468]. 23/Feb/2023 15:55:14 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:55:14 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K veth06e8023d gro off gso off tso off 23/Feb/2023 15:58:35 [NetworkInterface.cpp:2007] Invalid packet received [len: 1690][max len: 1468]. 23/Feb/2023 15:58:35 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:58:35 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K vethd9135091 gro off gso off tso off 23/Feb/2023 15:58:35 [NetworkInterface.cpp:2007] Invalid packet received [len: 1690][max len: 1468]. 23/Feb/2023 15:58:35 [NetworkInterface.cpp:2008] WARNING: If you have TSO/GRO enabled, please disable it 23/Feb/2023 15:58:35 [NetworkInterface.cpp:2010] WARNING: Use sudo ethtool -K veth9cbeefd9 gro off gso off tso off Open a browser and access the ntopng application via URL http:<ip address of docker host>:3000/","title":"Build pi4-ntopng with build.sh script (arm64)"},{"location":"pi-stories17/#build_pi4-ntopng_from_the_sources_with_buildersh_script_arm64","text":"The builder.sh script is using the Dockerfile.builder to build the ntopng executable from scratch and it was a serious battle to get it working, but we are proud on the result. By using the builder.sh script we also get the complete log saved as builder.log which might be useful when something went wrong. It might be that the compilation fails as we are using the development branch of the ntopng project. If all goed well we always have the state of art application of ntopng. To test with docker use the same method as described above with the build.sh script. Open a browser and access the ntopng application via URL http:<ip address of docker host>:3000/","title":"Build pi4-ntopng from the sources with builder.sh script (arm64)"},{"location":"pi-stories17/#use_the_kubernetes_helm_chart_to_get_it_into_kubernetes","text":"Go into directory kubernetes-helm-chart-ntopng and review the kubernetes/Chart.yaml and kubernetes/values.yaml files. We have pinned it to use version 5.7.0 . $ kubectl create -f ./kubernetes/namespace.yaml $ helm install --debug --dry-run --namespace ntopng ntopng ./kubernetes install.go:193: [debug] Original chart version: \"\" install.go:210: [debug] CHART PATH: /home/gdha/projects/pi4-ntopng/kubernetes-helm-chart-ntopng/kubernetes walk.go:74: found symbolic link in path: /home/gdha/projects/pi4-ntopng/kubernetes-helm-chart-ntopng/kubernetes/templates/ghcr-secret.yaml resolves to /home/gdha/projects/pi4-ntopng/kubernetes-helm-chart-ntopng/kubernetes/templates/.hidden/ghcr-secret.yaml. Contents of linked file included and used NAME: ntopng LAST DEPLOYED: Fri Oct 6 16:25:29 2023 NAMESPACE: ntopng STATUS: pending-install REVISION: 1 TEST SUITE: None USER-SUPPLIED VALUES: {} COMPUTED VALUES: ntopngConfig: |- --disable-login=1 --dns-mode=3 # Limit memory usage --max-num-flows=200000 --max-num-hosts=250000 #--interface=xxxxxx --no-promisc ntopngImageName: ghcr.io/gdha/pi4-ntopng ntopngImageVersion: 5.7.0 ntopngNodeSelector: kubernetes.io/os: linux ntopngResources: null ntopngService: port: 80 type: LoadBalancer HOOKS: MANIFEST: --- # Source: ntopng/templates/ghcr-secret.yaml # kubectl create secret docker-registry dockerconfigjson-github-com --docker-server=ghcr.io --docker-username=$USER --docker-password=$(cat ~/.ghcr-token) --dry-run=client -oyaml >.hidden/ghcr-secret.yaml # ln -s .hidden/ghcr-secret.yaml ghcr-secret.yaml # Edit ghcr-secret.yaml and modify namespace and labels in the metadata section. apiVersion: v1 kind: Secret metadata: name: ntopng-ghrc namespace: ntopng labels: app: ntopng chart: ntopng-5.7.0 release: ntopng heritage: Helm creationTimestamp: null type: kubernetes.io/dockerconfigjson data: .dockerconfigjson: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx --- # Source: ntopng/templates/secrets.yaml apiVersion: v1 kind: Secret metadata: name: ntopng namespace: ntopng labels: app: ntopng chart: ntopng-5.7.0 release: ntopng heritage: Helm type: Opaque --- # Source: ntopng/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: ntopng namespace: ntopng labels: app: ntopng chart: ntopng-5.7.0 release: ntopng heritage: Helm data: ntopng.conf: |- --disable-login=1 --dns-mode=3 # Limit memory usage --max-num-flows=200000 --max-num-hosts=250000 #--interface=xxxxxx --no-promisc --- # Source: ntopng/templates/service.yaml apiVersion: v1 kind: Service metadata: name: ntopng namespace: ntopng labels: app: ntopng chart: ntopng-5.7.0 release: ntopng heritage: Helm spec: type: LoadBalancer externalTrafficPolicy: Local selector: app: ntopng ports: - name: ntopng port: 80 targetPort: 3000 protocol: TCP --- # Source: ntopng/templates/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: ntopng namespace: ntopng labels: app: ntopng chart: ntopng-5.7.0 release: ntopng heritage: Helm spec: replicas: 1 selector: matchLabels: app: ntopng template: metadata: labels: app: ntopng chart: ntopng-5.7.0 release: ntopng heritage: Helm spec: terminationGracePeriodSeconds: 10 hostNetwork: true nodeSelector: kubernetes.io/os: \"linux\" imagePullSecrets: - name: ntopng-ghrc containers: - name: ntopng image: ghcr.io/gdha/pi4-ntopng:5.7.0 imagePullPolicy: IfNotPresent ports: - name: ntopng containerPort: 3000 protocol: TCP resources: null env: - name: CONFIG value: /ntopng/ntopng.conf volumeMounts: - name: config mountPath: /ntopng volumes: - name: config configMap: name: ntopng To find the endpoint IP address to point to in the web browser do the following: $ kubectl get endpoints -n ntopng NAME ENDPOINTS AGE ntopng 192.168.0.205:3000 32h","title":"Use the kubernetes helm chart to get it into kubernetes"},{"location":"pi-stories17/#rederences","text":"[1] GitHub project ntopng [2] GitHub project ntopng with helm chart for arm64","title":"Rederences"},{"location":"pi-stories18/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Deploying Jenkins with yaml files \u00b6 Download the GitHub yaml sources of Jenkins POD \u00b6 We do not want copy/paste the procedure in setting up Jenkins on a kubernetes cluster described in article - How To Setup Jenkins On Kubernetes Cluster \u2013 Beginners Guide ] [2]. However, the sources we used to set it up in our cluster can be download from our pi4-jenkins github project [1]. Basic configuration of the Jenkins application in our cluster \u00b6 After doing the first initial Jenkins configuration and adding some packages you might need or find useful we can define our k3s cluster within Jenkins so we can use it to deploy an Jenkins agent via k3s. Therefore, go to \"Manage Jenkins -> Clouds\" and select on the button \"new cloud\". Then fill in what you see below: Set up a workflow k3s-test \u00b6 We can create a simple pipeline workflow within Jenkins called k3s-test and add a simple test script in the pipeline section: // Uses Declarative syntax to run commands inside a container. pipeline { agent { kubernetes { defaultContainer 'jnlp' } } stages { stage('Main') { steps { sh 'hostname; ls /' } } } } Then, just click on the \"Build Now\" button to get some action and wait for the result (output of the console): Started by user gratien dhaese [Pipeline] Start of Pipeline [Pipeline] podTemplate [Pipeline] { [Pipeline] node Created Pod: k3s devops-tools/k3s-test-12-r7t20-tp9h0-w3md4 Still waiting to schedule task \u2018k3s-test-12-r7t20-tp9h0-w3md4\u2019 is offline Agent k3s-test-12-r7t20-tp9h0-w3md4 is provisioned from template k3s-test_12-r7t20-tp9h0 --- apiVersion: \"v1\" kind: \"Pod\" metadata: annotations: buildUrl: \"http://jenkins-service.devops-tools.svc.cluster.local:8080/job/k3s-test/12/\" runUrl: \"job/k3s-test/12/\" labels: jenkins: \"agent\" jenkins/label-digest: \"3ea935590b9ee455238d862a312e8637c1a83963\" jenkins/label: \"k3s-test_12-r7t20\" name: \"k3s-test-12-r7t20-tp9h0-w3md4\" namespace: \"devops-tools\" spec: containers: - env: - name: \"JENKINS_SECRET\" value: \"********\" - name: \"JENKINS_AGENT_NAME\" value: \"k3s-test-12-r7t20-tp9h0-w3md4\" - name: \"JENKINS_NAME\" value: \"k3s-test-12-r7t20-tp9h0-w3md4\" - name: \"JENKINS_AGENT_WORKDIR\" value: \"/home/jenkins/agent\" - name: \"JENKINS_URL\" value: \"http://jenkins-service.devops-tools.svc.cluster.local:8080/\" image: \"jenkins/inbound-agent:3148.v532a_7e715ee3-1\" name: \"jnlp\" resources: requests: memory: \"256Mi\" cpu: \"100m\" volumeMounts: - mountPath: \"/home/jenkins/agent\" name: \"workspace-volume\" readOnly: false nodeSelector: kubernetes.io/os: \"linux\" restartPolicy: \"Never\" volumes: - emptyDir: medium: \"\" name: \"workspace-volume\" Running on k3s-test-12-r7t20-tp9h0-w3md4 in /home/jenkins/agent/workspace/k3s-test [Pipeline] { [Pipeline] stage [Pipeline] { (Main) [Pipeline] sh + hostname k3s-test-12-r7t20-tp9h0-w3md4 + ls / bin boot dev etc home lib media mnt opt proc root run sbin srv sys tmp usr var [Pipeline] } [Pipeline] // stage [Pipeline] } [Pipeline] // node [Pipeline] } [Pipeline] // podTemplate [Pipeline] End of Pipeline Finished: SUCCESS References \u00b6 [1] Github sources of the Pi4 Jenkins kubernetes yaml files [2] How To Setup Jenkins On Kubernetes Cluster \u2013 Beginners Guide","title":"Raspberry Pi 4 Deploying jenkins with yaml files"},{"location":"pi-stories18/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories18/#raspberry_pi_4_cluster_series_-_deploying_jenkins_with_yaml_files","text":"","title":"Raspberry Pi 4 cluster Series - Deploying Jenkins with yaml files"},{"location":"pi-stories18/#download_the_github_yaml_sources_of_jenkins_pod","text":"We do not want copy/paste the procedure in setting up Jenkins on a kubernetes cluster described in article - How To Setup Jenkins On Kubernetes Cluster \u2013 Beginners Guide ] [2]. However, the sources we used to set it up in our cluster can be download from our pi4-jenkins github project [1].","title":"Download the GitHub yaml sources of Jenkins POD"},{"location":"pi-stories18/#basic_configuration_of_the_jenkins_application_in_our_cluster","text":"After doing the first initial Jenkins configuration and adding some packages you might need or find useful we can define our k3s cluster within Jenkins so we can use it to deploy an Jenkins agent via k3s. Therefore, go to \"Manage Jenkins -> Clouds\" and select on the button \"new cloud\". Then fill in what you see below:","title":"Basic configuration of the Jenkins application in our cluster"},{"location":"pi-stories18/#set_up_a_workflow_k3s-test","text":"We can create a simple pipeline workflow within Jenkins called k3s-test and add a simple test script in the pipeline section: // Uses Declarative syntax to run commands inside a container. pipeline { agent { kubernetes { defaultContainer 'jnlp' } } stages { stage('Main') { steps { sh 'hostname; ls /' } } } } Then, just click on the \"Build Now\" button to get some action and wait for the result (output of the console): Started by user gratien dhaese [Pipeline] Start of Pipeline [Pipeline] podTemplate [Pipeline] { [Pipeline] node Created Pod: k3s devops-tools/k3s-test-12-r7t20-tp9h0-w3md4 Still waiting to schedule task \u2018k3s-test-12-r7t20-tp9h0-w3md4\u2019 is offline Agent k3s-test-12-r7t20-tp9h0-w3md4 is provisioned from template k3s-test_12-r7t20-tp9h0 --- apiVersion: \"v1\" kind: \"Pod\" metadata: annotations: buildUrl: \"http://jenkins-service.devops-tools.svc.cluster.local:8080/job/k3s-test/12/\" runUrl: \"job/k3s-test/12/\" labels: jenkins: \"agent\" jenkins/label-digest: \"3ea935590b9ee455238d862a312e8637c1a83963\" jenkins/label: \"k3s-test_12-r7t20\" name: \"k3s-test-12-r7t20-tp9h0-w3md4\" namespace: \"devops-tools\" spec: containers: - env: - name: \"JENKINS_SECRET\" value: \"********\" - name: \"JENKINS_AGENT_NAME\" value: \"k3s-test-12-r7t20-tp9h0-w3md4\" - name: \"JENKINS_NAME\" value: \"k3s-test-12-r7t20-tp9h0-w3md4\" - name: \"JENKINS_AGENT_WORKDIR\" value: \"/home/jenkins/agent\" - name: \"JENKINS_URL\" value: \"http://jenkins-service.devops-tools.svc.cluster.local:8080/\" image: \"jenkins/inbound-agent:3148.v532a_7e715ee3-1\" name: \"jnlp\" resources: requests: memory: \"256Mi\" cpu: \"100m\" volumeMounts: - mountPath: \"/home/jenkins/agent\" name: \"workspace-volume\" readOnly: false nodeSelector: kubernetes.io/os: \"linux\" restartPolicy: \"Never\" volumes: - emptyDir: medium: \"\" name: \"workspace-volume\" Running on k3s-test-12-r7t20-tp9h0-w3md4 in /home/jenkins/agent/workspace/k3s-test [Pipeline] { [Pipeline] stage [Pipeline] { (Main) [Pipeline] sh + hostname k3s-test-12-r7t20-tp9h0-w3md4 + ls / bin boot dev etc home lib media mnt opt proc root run sbin srv sys tmp usr var [Pipeline] } [Pipeline] // stage [Pipeline] } [Pipeline] // node [Pipeline] } [Pipeline] // podTemplate [Pipeline] End of Pipeline Finished: SUCCESS","title":"Set up a workflow k3s-test"},{"location":"pi-stories18/#references","text":"[1] Github sources of the Pi4 Jenkins kubernetes yaml files [2] How To Setup Jenkins On Kubernetes Cluster \u2013 Beginners Guide","title":"References"},{"location":"pi-stories2/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Prepare for kubernetes installation \u00b6 Add account with Secure Shell keys and common software packages \u00b6 It is important that my account is created on each host with the requires secure shell keys. Also, we install what we think are required software packages on each host: vim git rsync acl dnsutils dphys-swapfile python-is-python3 sshpass ca-certificates curl gnupg-agent software-properties-common jq To get going first install git on the system where you start ansible playbooks (in our case it is host n5) and afterwards, clone the playbook pi4-cluster-ansible-roles . Do check the inventory.yml file and add you preferences. The playbook will also disable swap on each host, which is a requirement for kubernetes. Compile ARM side libraries for interfacing to Raspberry Pi GPU \u00b6 Download the source code from 2 . Compile and install these binaries on the host from where you will use the playbook. In my case its was host n5 (do not forget to install ansible ). Run the ansible playbook provision.yml \u00b6 $ ansible-playbook provision.yml -k --vault-password-file .my_password SSH password: PLAY [all] ************************************************************************************************************************************* TASK [Gathering Facts] ************************************************************************************************************************* ok: [n5] ok: [n3] ok: [n2] ok: [n4] ok: [n1] TASK [user : debug] **************************************************************************************************************************** ok: [n1] => { \"msg\": \"creating user gdha and OpenSSH public key distribution\" } ok: [n2] => { \"msg\": \"creating user gdha and OpenSSH public key distribution\" } ok: [n3] => { \"msg\": \"creating user gdha and OpenSSH public key distribution\" } ok: [n4] => { \"msg\": \"creating user gdha and OpenSSH public key distribution\" } ok: [n5] => { \"msg\": \"creating user gdha and OpenSSH public key distribution\" } TASK [user : Creating admin group] ************************************************************************************************************* ok: [n3] ok: [n1] ok: [n4] ok: [n2] ok: [n5] TASK [user : Add group gdha (1001)] ************************************************************************************************************ ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [user : Add user 'gdha' with specific uid (1001) and group 'gdha' (1001) and secondary group 'admin'] ************************************* ok: [n5] ok: [n2] ok: [n3] ok: [n1] ok: [n4] TASK [user : Create the /home/gdha/.ssh directory] ********************************************************************************************* ok: [n2] ok: [n1] ok: [n3] ok: [n4] ok: [n5] TASK [user : Copy /home/gdha/.ssh/id_rsa.pub to remote nodes] ********************************************************************************** ok: [n2] ok: [n3] ok: [n1] ok: [n4] ok: [n5] TASK [user : Append public ssh key of gdha to authorized_keys] ********************************************************************************* changed: [n1] => (item=/home/gdha/.ssh/id_rsa.pub) changed: [n4] => (item=/home/gdha/.ssh/id_rsa.pub) changed: [n2] => (item=/home/gdha/.ssh/id_rsa.pub) changed: [n3] => (item=/home/gdha/.ssh/id_rsa.pub) ok: [n5] => (item=/home/gdha/.ssh/id_rsa.pub) TASK [user : Create /etc/sudoers.d/gdha-sudoers file] ****************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [base : Set /etc/hosts for other nodes] *************************************************************************************************** ok: [n1] => (item=n2) ok: [n2] => (item=n1) ok: [n3] => (item=n1) ok: [n4] => (item=n1) ok: [n5] => (item=n1) ok: [n1] => (item=n3) ok: [n2] => (item=n3) ok: [n3] => (item=n2) ok: [n4] => (item=n2) ok: [n5] => (item=n2) ok: [n1] => (item=n4) ok: [n2] => (item=n4) ok: [n4] => (item=n3) ok: [n3] => (item=n4) ok: [n5] => (item=n3) ok: [n1] => (item=n5) ok: [n2] => (item=n5) ok: [n4] => (item=n5) ok: [n3] => (item=n5) ok: [n5] => (item=n4) TASK [base : Install common packages] ********************************************************************************************************** ok: [n5] changed: [n2] changed: [n3] changed: [n4] changed: [n1] TASK [base : Set default locale] *************************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [base : Enable locale] ******************************************************************************************************************** ok: [n2] ok: [n1] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Create directory /opt/vc/bin] ****************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : copy tvservice to /opt/vc/bin] ***************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Create the /opt/vc/lib directory] ************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Copy /opt/vc/lib/libvchiq_arm.so] ************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Copy /opt/vc/lib/libvcos.so] ******************************************************************************************************* ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : add /opt/vc/lib to /etc/ld.so.conf file] ******************************************************************************************* ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Run ldconfig] ********************************************************************************************************************** changed: [n2] changed: [n1] changed: [n3] changed: [n4] changed: [n5] TASK [rpi : Check if HDMI is on] *************************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Switch off HDMI] ******************************************************************************************************************* changed: [n1] changed: [n2] changed: [n3] changed: [n4] changed: [n5] TASK [rpi : Ensure rc.local exists] ************************************************************************************************************ changed: [n1] changed: [n2] changed: [n3] changed: [n4] changed: [n5] TASK [rpi : Switch off HDMI on boot] *********************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [os : Set hostname] *********************************************************************************************************************** ok: [n2] ok: [n3] ok: [n1] ok: [n4] ok: [n5] TASK [os : Set /etc/hosts hostname] ************************************************************************************************************ ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [os : Check if swap is enabled] *********************************************************************************************************** changed: [n1] changed: [n2] changed: [n3] changed: [n4] changed: [n5] TASK [os : Disable swap] *********************************************************************************************************************** changed: [n1] => (item=dphys-swapfile swapoff) changed: [n2] => (item=dphys-swapfile swapoff) changed: [n3] => (item=dphys-swapfile swapoff) changed: [n4] => (item=dphys-swapfile swapoff) changed: [n5] => (item=dphys-swapfile swapoff) changed: [n1] => (item=dphys-swapfile uninstall) changed: [n2] => (item=dphys-swapfile uninstall) changed: [n3] => (item=dphys-swapfile uninstall) changed: [n4] => (item=dphys-swapfile uninstall) changed: [n5] => (item=dphys-swapfile uninstall) changed: [n2] => (item=update-rc.d dphys-swapfile disable) changed: [n3] => (item=update-rc.d dphys-swapfile disable) changed: [n1] => (item=update-rc.d dphys-swapfile disable) changed: [n4] => (item=update-rc.d dphys-swapfile disable) changed: [n5] => (item=update-rc.d dphys-swapfile disable) TASK [os : Add users to passwordless sudoers.] ************************************************************************************************* ok: [n1] => (item=gdha) ok: [n2] => (item=gdha) ok: [n3] => (item=gdha) ok: [n4] => (item=gdha) ok: [n5] => (item=gdha) ok: [n1] => (item=ubuntu) ok: [n2] => (item=ubuntu) ok: [n3] => (item=ubuntu) ok: [n4] => (item=ubuntu) ok: [n5] => (item=ubuntu) TASK [ssh : Check if ssh key exists] *********************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [ssh : Create .ssh directory] ************************************************************************************************************* ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [ssh : Generate ssh key] ****************************************************************************************************************** skipping: [n5] changed: [n1] changed: [n3] changed: [n2] changed: [n4] TASK [ssh : Slurp public keys from all nodes] ************************************************************************************************** ok: [n1] ok: [n3] ok: [n2] ok: [n4] ok: [n5] TASK [ssh : Copy public keys of all nodes into authorized_keys] ******************************************************************************** skipping: [n1] => (item=n1) skipping: [n1] => (item=n2) skipping: [n1] => (item=n3) skipping: [n1] => (item=n4) skipping: [n1] => (item=n5) skipping: [n2] => (item=n1) skipping: [n2] => (item=n2) skipping: [n2] => (item=n3) skipping: [n2] => (item=n4) skipping: [n2] => (item=n5) skipping: [n3] => (item=n1) skipping: [n3] => (item=n2) skipping: [n3] => (item=n3) skipping: [n3] => (item=n4) skipping: [n3] => (item=n5) skipping: [n4] => (item=n1) skipping: [n4] => (item=n2) skipping: [n4] => (item=n3) skipping: [n4] => (item=n4) skipping: [n4] => (item=n5) skipping: [n5] => (item=n1) skipping: [n5] => (item=n2) skipping: [n5] => (item=n3) skipping: [n5] => (item=n4) skipping: [n5] => (item=n5) TASK [ssh : Copy local public key to authorized_keys] ****************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [ssh : Slurp host keys from all nodes] **************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [ssh : Insert all nodes into global known_hosts] ****************************************************************************************** ok: [n1] => (item=n1) ok: [n2] => (item=n1) ok: [n3] => (item=n1) ok: [n4] => (item=n1) ok: [n5] => (item=n1) ok: [n2] => (item=n2) ok: [n3] => (item=n2) ok: [n1] => (item=n2) ok: [n4] => (item=n2) ok: [n5] => (item=n2) ok: [n2] => (item=n3) ok: [n3] => (item=n3) ok: [n4] => (item=n3) ok: [n1] => (item=n3) ok: [n5] => (item=n3) ok: [n2] => (item=n4) ok: [n3] => (item=n4) ok: [n4] => (item=n4) ok: [n1] => (item=n4) ok: [n5] => (item=n4) ok: [n2] => (item=n5) ok: [n3] => (item=n5) ok: [n4] => (item=n5) ok: [n1] => (item=n5) ok: [n5] => (item=n5) TASK [ssh : Secure SSH configuration] ********************************************************************************************************** ok: [n1] => (item={'regexp': '^[# \\\\t]*Port', 'line': 'Port 22'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*Port', 'line': 'Port 22'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*Port', 'line': 'Port 22'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*Port', 'line': 'Port 22'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*Port', 'line': 'Port 22'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*PasswordAuthentication', 'line': 'PasswordAuthentication yes'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*PasswordAuthentication', 'line': 'PasswordAuthentication yes'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*PasswordAuthentication', 'line': 'PasswordAuthentication yes'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*PasswordAuthentication', 'line': 'PasswordAuthentication yes'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*PasswordAuthentication', 'line': 'PasswordAuthentication yes'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*PermitRootLogin', 'line': 'PermitRootLogin no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*PermitRootLogin', 'line': 'PermitRootLogin no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*PermitRootLogin', 'line': 'PermitRootLogin no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*PermitRootLogin', 'line': 'PermitRootLogin no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*PermitRootLogin', 'line': 'PermitRootLogin no'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*UseDNS', 'line': 'UseDNS no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*UseDNS', 'line': 'UseDNS no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*UseDNS', 'line': 'UseDNS no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*UseDNS', 'line': 'UseDNS no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*UseDNS', 'line': 'UseDNS no'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*PermitEmptyPasswords', 'line': 'PermitEmptyPasswords no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*PermitEmptyPasswords', 'line': 'PermitEmptyPasswords no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*PermitEmptyPasswords', 'line': 'PermitEmptyPasswords no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*PermitEmptyPasswords', 'line': 'PermitEmptyPasswords no'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*ChallengeResponseAuthentication', 'line': 'ChallengeResponseAuthentication no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*PermitEmptyPasswords', 'line': 'PermitEmptyPasswords no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*ChallengeResponseAuthentication', 'line': 'ChallengeResponseAuthentication no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*ChallengeResponseAuthentication', 'line': 'ChallengeResponseAuthentication no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*ChallengeResponseAuthentication', 'line': 'ChallengeResponseAuthentication no'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*GSSAPIAuthentication', 'line': 'GSSAPIAuthentication no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*GSSAPIAuthentication', 'line': 'GSSAPIAuthentication no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*ChallengeResponseAuthentication', 'line': 'ChallengeResponseAuthentication no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*GSSAPIAuthentication', 'line': 'GSSAPIAuthentication no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*GSSAPIAuthentication', 'line': 'GSSAPIAuthentication no'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*X11Forwarding', 'line': 'X11Forwarding no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*X11Forwarding', 'line': 'X11Forwarding no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*GSSAPIAuthentication', 'line': 'GSSAPIAuthentication no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*X11Forwarding', 'line': 'X11Forwarding no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*X11Forwarding', 'line': 'X11Forwarding no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*X11Forwarding', 'line': 'X11Forwarding no'}) TASK [ssh : Secure SSH hosts configuration] **************************************************************************************************** ok: [n1] => (item={'regexp': '^[# \\\\t]*HashKnownHosts', 'line': 'HashKnownHosts no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*HashKnownHosts', 'line': 'HashKnownHosts no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*HashKnownHosts', 'line': 'HashKnownHosts no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*HashKnownHosts', 'line': 'HashKnownHosts no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*HashKnownHosts', 'line': 'HashKnownHosts no'}) PLAY RECAP ************************************************************************************************************************************* n1 : ok=38 changed=8 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 n2 : ok=38 changed=8 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 n3 : ok=38 changed=8 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 n4 : ok=38 changed=8 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 n5 : ok=37 changed=5 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0 References \u00b6 [1] Ansible pi4-cluster-ansible-roles playbook [2] Source code for ARM side libraries for interfacing to Raspberry Pi GPU Edit history \u00b6 initial post on 09/Sep/2020","title":"Raspberry Pi 4 Prepare for kubernetes"},{"location":"pi-stories2/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories2/#raspberry_pi_4_cluster_series_-_prepare_for_kubernetes_installation","text":"","title":"Raspberry Pi 4 cluster Series - Prepare for kubernetes installation"},{"location":"pi-stories2/#add_account_with_secure_shell_keys_and_common_software_packages","text":"It is important that my account is created on each host with the requires secure shell keys. Also, we install what we think are required software packages on each host: vim git rsync acl dnsutils dphys-swapfile python-is-python3 sshpass ca-certificates curl gnupg-agent software-properties-common jq To get going first install git on the system where you start ansible playbooks (in our case it is host n5) and afterwards, clone the playbook pi4-cluster-ansible-roles . Do check the inventory.yml file and add you preferences. The playbook will also disable swap on each host, which is a requirement for kubernetes.","title":"Add account with Secure Shell keys and common software packages"},{"location":"pi-stories2/#compile_arm_side_libraries_for_interfacing_to_raspberry_pi_gpu","text":"Download the source code from 2 . Compile and install these binaries on the host from where you will use the playbook. In my case its was host n5 (do not forget to install ansible ).","title":"Compile ARM side libraries for interfacing to Raspberry Pi GPU"},{"location":"pi-stories2/#run_the_ansible_playbook_provisionyml","text":"$ ansible-playbook provision.yml -k --vault-password-file .my_password SSH password: PLAY [all] ************************************************************************************************************************************* TASK [Gathering Facts] ************************************************************************************************************************* ok: [n5] ok: [n3] ok: [n2] ok: [n4] ok: [n1] TASK [user : debug] **************************************************************************************************************************** ok: [n1] => { \"msg\": \"creating user gdha and OpenSSH public key distribution\" } ok: [n2] => { \"msg\": \"creating user gdha and OpenSSH public key distribution\" } ok: [n3] => { \"msg\": \"creating user gdha and OpenSSH public key distribution\" } ok: [n4] => { \"msg\": \"creating user gdha and OpenSSH public key distribution\" } ok: [n5] => { \"msg\": \"creating user gdha and OpenSSH public key distribution\" } TASK [user : Creating admin group] ************************************************************************************************************* ok: [n3] ok: [n1] ok: [n4] ok: [n2] ok: [n5] TASK [user : Add group gdha (1001)] ************************************************************************************************************ ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [user : Add user 'gdha' with specific uid (1001) and group 'gdha' (1001) and secondary group 'admin'] ************************************* ok: [n5] ok: [n2] ok: [n3] ok: [n1] ok: [n4] TASK [user : Create the /home/gdha/.ssh directory] ********************************************************************************************* ok: [n2] ok: [n1] ok: [n3] ok: [n4] ok: [n5] TASK [user : Copy /home/gdha/.ssh/id_rsa.pub to remote nodes] ********************************************************************************** ok: [n2] ok: [n3] ok: [n1] ok: [n4] ok: [n5] TASK [user : Append public ssh key of gdha to authorized_keys] ********************************************************************************* changed: [n1] => (item=/home/gdha/.ssh/id_rsa.pub) changed: [n4] => (item=/home/gdha/.ssh/id_rsa.pub) changed: [n2] => (item=/home/gdha/.ssh/id_rsa.pub) changed: [n3] => (item=/home/gdha/.ssh/id_rsa.pub) ok: [n5] => (item=/home/gdha/.ssh/id_rsa.pub) TASK [user : Create /etc/sudoers.d/gdha-sudoers file] ****************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [base : Set /etc/hosts for other nodes] *************************************************************************************************** ok: [n1] => (item=n2) ok: [n2] => (item=n1) ok: [n3] => (item=n1) ok: [n4] => (item=n1) ok: [n5] => (item=n1) ok: [n1] => (item=n3) ok: [n2] => (item=n3) ok: [n3] => (item=n2) ok: [n4] => (item=n2) ok: [n5] => (item=n2) ok: [n1] => (item=n4) ok: [n2] => (item=n4) ok: [n4] => (item=n3) ok: [n3] => (item=n4) ok: [n5] => (item=n3) ok: [n1] => (item=n5) ok: [n2] => (item=n5) ok: [n4] => (item=n5) ok: [n3] => (item=n5) ok: [n5] => (item=n4) TASK [base : Install common packages] ********************************************************************************************************** ok: [n5] changed: [n2] changed: [n3] changed: [n4] changed: [n1] TASK [base : Set default locale] *************************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [base : Enable locale] ******************************************************************************************************************** ok: [n2] ok: [n1] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Create directory /opt/vc/bin] ****************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : copy tvservice to /opt/vc/bin] ***************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Create the /opt/vc/lib directory] ************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Copy /opt/vc/lib/libvchiq_arm.so] ************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Copy /opt/vc/lib/libvcos.so] ******************************************************************************************************* ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : add /opt/vc/lib to /etc/ld.so.conf file] ******************************************************************************************* ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Run ldconfig] ********************************************************************************************************************** changed: [n2] changed: [n1] changed: [n3] changed: [n4] changed: [n5] TASK [rpi : Check if HDMI is on] *************************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Switch off HDMI] ******************************************************************************************************************* changed: [n1] changed: [n2] changed: [n3] changed: [n4] changed: [n5] TASK [rpi : Ensure rc.local exists] ************************************************************************************************************ changed: [n1] changed: [n2] changed: [n3] changed: [n4] changed: [n5] TASK [rpi : Switch off HDMI on boot] *********************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [os : Set hostname] *********************************************************************************************************************** ok: [n2] ok: [n3] ok: [n1] ok: [n4] ok: [n5] TASK [os : Set /etc/hosts hostname] ************************************************************************************************************ ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [os : Check if swap is enabled] *********************************************************************************************************** changed: [n1] changed: [n2] changed: [n3] changed: [n4] changed: [n5] TASK [os : Disable swap] *********************************************************************************************************************** changed: [n1] => (item=dphys-swapfile swapoff) changed: [n2] => (item=dphys-swapfile swapoff) changed: [n3] => (item=dphys-swapfile swapoff) changed: [n4] => (item=dphys-swapfile swapoff) changed: [n5] => (item=dphys-swapfile swapoff) changed: [n1] => (item=dphys-swapfile uninstall) changed: [n2] => (item=dphys-swapfile uninstall) changed: [n3] => (item=dphys-swapfile uninstall) changed: [n4] => (item=dphys-swapfile uninstall) changed: [n5] => (item=dphys-swapfile uninstall) changed: [n2] => (item=update-rc.d dphys-swapfile disable) changed: [n3] => (item=update-rc.d dphys-swapfile disable) changed: [n1] => (item=update-rc.d dphys-swapfile disable) changed: [n4] => (item=update-rc.d dphys-swapfile disable) changed: [n5] => (item=update-rc.d dphys-swapfile disable) TASK [os : Add users to passwordless sudoers.] ************************************************************************************************* ok: [n1] => (item=gdha) ok: [n2] => (item=gdha) ok: [n3] => (item=gdha) ok: [n4] => (item=gdha) ok: [n5] => (item=gdha) ok: [n1] => (item=ubuntu) ok: [n2] => (item=ubuntu) ok: [n3] => (item=ubuntu) ok: [n4] => (item=ubuntu) ok: [n5] => (item=ubuntu) TASK [ssh : Check if ssh key exists] *********************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [ssh : Create .ssh directory] ************************************************************************************************************* ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [ssh : Generate ssh key] ****************************************************************************************************************** skipping: [n5] changed: [n1] changed: [n3] changed: [n2] changed: [n4] TASK [ssh : Slurp public keys from all nodes] ************************************************************************************************** ok: [n1] ok: [n3] ok: [n2] ok: [n4] ok: [n5] TASK [ssh : Copy public keys of all nodes into authorized_keys] ******************************************************************************** skipping: [n1] => (item=n1) skipping: [n1] => (item=n2) skipping: [n1] => (item=n3) skipping: [n1] => (item=n4) skipping: [n1] => (item=n5) skipping: [n2] => (item=n1) skipping: [n2] => (item=n2) skipping: [n2] => (item=n3) skipping: [n2] => (item=n4) skipping: [n2] => (item=n5) skipping: [n3] => (item=n1) skipping: [n3] => (item=n2) skipping: [n3] => (item=n3) skipping: [n3] => (item=n4) skipping: [n3] => (item=n5) skipping: [n4] => (item=n1) skipping: [n4] => (item=n2) skipping: [n4] => (item=n3) skipping: [n4] => (item=n4) skipping: [n4] => (item=n5) skipping: [n5] => (item=n1) skipping: [n5] => (item=n2) skipping: [n5] => (item=n3) skipping: [n5] => (item=n4) skipping: [n5] => (item=n5) TASK [ssh : Copy local public key to authorized_keys] ****************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [ssh : Slurp host keys from all nodes] **************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [ssh : Insert all nodes into global known_hosts] ****************************************************************************************** ok: [n1] => (item=n1) ok: [n2] => (item=n1) ok: [n3] => (item=n1) ok: [n4] => (item=n1) ok: [n5] => (item=n1) ok: [n2] => (item=n2) ok: [n3] => (item=n2) ok: [n1] => (item=n2) ok: [n4] => (item=n2) ok: [n5] => (item=n2) ok: [n2] => (item=n3) ok: [n3] => (item=n3) ok: [n4] => (item=n3) ok: [n1] => (item=n3) ok: [n5] => (item=n3) ok: [n2] => (item=n4) ok: [n3] => (item=n4) ok: [n4] => (item=n4) ok: [n1] => (item=n4) ok: [n5] => (item=n4) ok: [n2] => (item=n5) ok: [n3] => (item=n5) ok: [n4] => (item=n5) ok: [n1] => (item=n5) ok: [n5] => (item=n5) TASK [ssh : Secure SSH configuration] ********************************************************************************************************** ok: [n1] => (item={'regexp': '^[# \\\\t]*Port', 'line': 'Port 22'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*Port', 'line': 'Port 22'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*Port', 'line': 'Port 22'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*Port', 'line': 'Port 22'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*Port', 'line': 'Port 22'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*PasswordAuthentication', 'line': 'PasswordAuthentication yes'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*PasswordAuthentication', 'line': 'PasswordAuthentication yes'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*PasswordAuthentication', 'line': 'PasswordAuthentication yes'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*PasswordAuthentication', 'line': 'PasswordAuthentication yes'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*PasswordAuthentication', 'line': 'PasswordAuthentication yes'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*PermitRootLogin', 'line': 'PermitRootLogin no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*PermitRootLogin', 'line': 'PermitRootLogin no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*PermitRootLogin', 'line': 'PermitRootLogin no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*PermitRootLogin', 'line': 'PermitRootLogin no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*PermitRootLogin', 'line': 'PermitRootLogin no'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*UseDNS', 'line': 'UseDNS no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*UseDNS', 'line': 'UseDNS no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*UseDNS', 'line': 'UseDNS no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*UseDNS', 'line': 'UseDNS no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*UseDNS', 'line': 'UseDNS no'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*PermitEmptyPasswords', 'line': 'PermitEmptyPasswords no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*PermitEmptyPasswords', 'line': 'PermitEmptyPasswords no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*PermitEmptyPasswords', 'line': 'PermitEmptyPasswords no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*PermitEmptyPasswords', 'line': 'PermitEmptyPasswords no'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*ChallengeResponseAuthentication', 'line': 'ChallengeResponseAuthentication no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*PermitEmptyPasswords', 'line': 'PermitEmptyPasswords no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*ChallengeResponseAuthentication', 'line': 'ChallengeResponseAuthentication no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*ChallengeResponseAuthentication', 'line': 'ChallengeResponseAuthentication no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*ChallengeResponseAuthentication', 'line': 'ChallengeResponseAuthentication no'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*GSSAPIAuthentication', 'line': 'GSSAPIAuthentication no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*GSSAPIAuthentication', 'line': 'GSSAPIAuthentication no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*ChallengeResponseAuthentication', 'line': 'ChallengeResponseAuthentication no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*GSSAPIAuthentication', 'line': 'GSSAPIAuthentication no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*GSSAPIAuthentication', 'line': 'GSSAPIAuthentication no'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*X11Forwarding', 'line': 'X11Forwarding no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*X11Forwarding', 'line': 'X11Forwarding no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*GSSAPIAuthentication', 'line': 'GSSAPIAuthentication no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*X11Forwarding', 'line': 'X11Forwarding no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*X11Forwarding', 'line': 'X11Forwarding no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*X11Forwarding', 'line': 'X11Forwarding no'}) TASK [ssh : Secure SSH hosts configuration] **************************************************************************************************** ok: [n1] => (item={'regexp': '^[# \\\\t]*HashKnownHosts', 'line': 'HashKnownHosts no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*HashKnownHosts', 'line': 'HashKnownHosts no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*HashKnownHosts', 'line': 'HashKnownHosts no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*HashKnownHosts', 'line': 'HashKnownHosts no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*HashKnownHosts', 'line': 'HashKnownHosts no'}) PLAY RECAP ************************************************************************************************************************************* n1 : ok=38 changed=8 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 n2 : ok=38 changed=8 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 n3 : ok=38 changed=8 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 n4 : ok=38 changed=8 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 n5 : ok=37 changed=5 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0","title":"Run the ansible playbook provision.yml"},{"location":"pi-stories2/#references","text":"[1] Ansible pi4-cluster-ansible-roles playbook [2] Source code for ARM side libraries for interfacing to Raspberry Pi GPU","title":"References"},{"location":"pi-stories2/#edit_history","text":"initial post on 09/Sep/2020","title":"Edit history"},{"location":"pi-stories3/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Installing k3s software \u00b6 Why k3s? \u00b6 K3s is a fully compliant Kubernetes distribution in a single binary perfectly suitable for smaller edge devices such as the Raspberry PI4. Simply said, you can do almost exactly the same as with its big sister kubernetes (k8s). For these kind of devices it is the perfect match. To get ks3 installed with ansible clone the playbook k3s-ansible playbook [1]. K3s is a product from Rancher Labs and can be installed in different ways, such as with k3sup (read the nice article \" Deploying a highly-available K3s with K3sup \") or with a fork of k3s-ansible sources. We choose for the latter and made some customisation to fork of k3s-ansible playbook [1]. Do check the inventory/my-cluster/hosts.ini file and add you preferences. Also, do not forget to adjust the attributes yaml file inventory/my-cluster/group_vars/all.yml . Especially, check the k3s_version you want to install. To find the latest stable release of k3s see the github release page of ks3s . Run the ansible playbook site.yml \u00b6 gdha@n5:~/projects/k3s-ansible$ ansible-playbook site.yml -i inventory/my-cluster/hosts.ini [WARNING]: While constructing a mapping from /home/gdha/projects/k3s-ansible/roles/ubuntu/tasks/main.yml, line 4, column 5, found a duplicate dict key (backrefs). Using last defined value only. PLAY [k3s_cluster] ******************************************************************************************************************************************************** TASK [Gathering Facts] **************************************************************************************************************************************************** Wednesday 23 September 2020 16:16:13 +0200 (0:00:00.102) 0:00:00.102 *** ok: [192.168.0.202] ok: [192.168.0.204] ok: [192.168.0.203] ok: [192.168.0.201] ok: [192.168.0.205] TASK [prereq : Set SELinux to disabled state] ***************************************************************************************************************************** Wednesday 23 September 2020 16:16:24 +0200 (0:00:10.334) 0:00:10.437 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [prereq : Enable IPv4 forwarding] ************************************************************************************************************************************ Wednesday 23 September 2020 16:16:24 +0200 (0:00:00.913) 0:00:11.350 *** ok: [192.168.0.203] ok: [192.168.0.201] ok: [192.168.0.202] ok: [192.168.0.204] ok: [192.168.0.205] TASK [prereq : Enable IPv6 forwarding] ************************************************************************************************************************************ Wednesday 23 September 2020 16:16:26 +0200 (0:00:01.658) 0:00:13.009 *** ok: [192.168.0.201] ok: [192.168.0.202] ok: [192.168.0.203] ok: [192.168.0.204] ok: [192.168.0.205] TASK [prereq : Add br_netfilter to /etc/modules-load.d/] ****************************************************************************************************************** Wednesday 23 September 2020 16:16:28 +0200 (0:00:01.781) 0:00:14.790 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [prereq : Load br_netfilter] ***************************************************************************************************************************************** Wednesday 23 September 2020 16:16:29 +0200 (0:00:00.905) 0:00:15.696 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [prereq : Set bridge-nf-call-iptables (just to be sure)] ************************************************************************************************************* Wednesday 23 September 2020 16:16:30 +0200 (0:00:00.909) 0:00:16.605 *** skipping: [192.168.0.201] => (item=net.bridge.bridge-nf-call-iptables) skipping: [192.168.0.201] => (item=net.bridge.bridge-nf-call-ip6tables) skipping: [192.168.0.202] => (item=net.bridge.bridge-nf-call-iptables) skipping: [192.168.0.202] => (item=net.bridge.bridge-nf-call-ip6tables) skipping: [192.168.0.203] => (item=net.bridge.bridge-nf-call-iptables) skipping: [192.168.0.203] => (item=net.bridge.bridge-nf-call-ip6tables) skipping: [192.168.0.204] => (item=net.bridge.bridge-nf-call-iptables) skipping: [192.168.0.204] => (item=net.bridge.bridge-nf-call-ip6tables) skipping: [192.168.0.205] => (item=net.bridge.bridge-nf-call-iptables) skipping: [192.168.0.205] => (item=net.bridge.bridge-nf-call-ip6tables) TASK [prereq : Add /usr/local/bin to sudo secure_path] ******************************************************************************************************************** Wednesday 23 September 2020 16:16:31 +0200 (0:00:00.939) 0:00:17.544 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [download : Delete k3s if already present] *************************************************************************************************************************** Wednesday 23 September 2020 16:16:32 +0200 (0:00:01.169) 0:00:18.714 *** changed: [192.168.0.201] changed: [192.168.0.203] changed: [192.168.0.202] changed: [192.168.0.204] changed: [192.168.0.205] TASK [download : Download k3s binary x64] ********************************************************************************************************************************* Wednesday 23 September 2020 16:16:34 +0200 (0:00:01.707) 0:00:20.422 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [download : Download k3s binary arm64] ******************************************************************************************************************************* Wednesday 23 September 2020 16:16:34 +0200 (0:00:00.943) 0:00:21.365 *** changed: [192.168.0.202] changed: [192.168.0.204] changed: [192.168.0.201] changed: [192.168.0.205] changed: [192.168.0.203] TASK [download : Download k3s binary armhf] ******************************************************************************************************************************* Wednesday 23 September 2020 16:17:34 +0200 (0:00:59.059) 0:01:20.425 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [raspbian : Test for Raspbian] *************************************************************************************************************************************** Wednesday 23 September 2020 16:17:34 +0200 (0:00:00.940) 0:01:21.365 *** ok: [192.168.0.201] ok: [192.168.0.202] ok: [192.168.0.203] ok: [192.168.0.204] ok: [192.168.0.205] TASK [raspbian : Activating cgroup support] ******************************************************************************************************************************* Wednesday 23 September 2020 16:17:36 +0200 (0:00:01.165) 0:01:22.531 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [raspbian : Flush iptables before changing to iptables-legacy] ******************************************************************************************************* Wednesday 23 September 2020 16:17:36 +0200 (0:00:00.796) 0:01:23.327 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [raspbian : Changing to iptables-legacy] ***************************************************************************************************************************** Wednesday 23 September 2020 16:17:37 +0200 (0:00:00.795) 0:01:24.123 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [raspbian : Changing to ip6tables-legacy] **************************************************************************************************************************** Wednesday 23 September 2020 16:17:38 +0200 (0:00:00.798) 0:01:24.922 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [raspbian : Rebooting] *********************************************************************************************************************************************** Wednesday 23 September 2020 16:17:39 +0200 (0:00:00.940) 0:01:25.862 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [ubuntu : Enable cgroup via boot commandline if not already enabled for Ubuntu on ARM] ******************************************************************************* Wednesday 23 September 2020 16:17:40 +0200 (0:00:00.800) 0:01:26.663 *** ok: [192.168.0.203] ok: [192.168.0.202] ok: [192.168.0.201] ok: [192.168.0.204] ok: [192.168.0.205] PLAY [master] ************************************************************************************************************************************************************* TASK [Gathering Facts] **************************************************************************************************************************************************** Wednesday 23 September 2020 16:17:42 +0200 (0:00:01.887) 0:01:28.550 *** ok: [192.168.0.201] TASK [k3s/master : Copy K3s service file] ********************************************************************************************************************************* Wednesday 23 September 2020 16:17:49 +0200 (0:00:07.512) 0:01:36.063 *** ok: [192.168.0.201] TASK [k3s/master : Enable and check K3s service] ************************************************************************************************************************** Wednesday 23 September 2020 16:17:51 +0200 (0:00:02.229) 0:01:38.293 *** changed: [192.168.0.201] TASK [k3s/master : Wait for node-token] *********************************************************************************************************************************** Wednesday 23 September 2020 16:18:11 +0200 (0:00:19.552) 0:01:57.846 *** ok: [192.168.0.201] TASK [k3s/master : Register node-token file access mode] ****************************************************************************************************************** Wednesday 23 September 2020 16:18:13 +0200 (0:00:01.661) 0:01:59.508 *** ok: [192.168.0.201] TASK [k3s/master : Change file access node-token] ************************************************************************************************************************* Wednesday 23 September 2020 16:18:14 +0200 (0:00:01.250) 0:02:00.758 *** changed: [192.168.0.201] TASK [k3s/master : Read node-token from master] *************************************************************************************************************************** Wednesday 23 September 2020 16:18:15 +0200 (0:00:01.036) 0:02:01.794 *** ok: [192.168.0.201] TASK [k3s/master : Store Master node-token] ******************************************************************************************************************************* Wednesday 23 September 2020 16:18:16 +0200 (0:00:01.293) 0:02:03.087 *** ok: [192.168.0.201] TASK [k3s/master : Restore node-token file access] ************************************************************************************************************************ Wednesday 23 September 2020 16:18:17 +0200 (0:00:00.277) 0:02:03.365 *** changed: [192.168.0.201] TASK [k3s/master : Create directory .kube] ******************************************************************************************************************************** Wednesday 23 September 2020 16:18:18 +0200 (0:00:01.079) 0:02:04.445 *** ok: [192.168.0.201] TASK [k3s/master : Copy config file to user home directory] *************************************************************************************************************** Wednesday 23 September 2020 16:18:19 +0200 (0:00:01.083) 0:02:05.528 *** changed: [192.168.0.201] TASK [k3s/master : Replace https://localhost:6443 by https://master-ip:6443] ********************************************************************************************** Wednesday 23 September 2020 16:18:20 +0200 (0:00:01.586) 0:02:07.115 *** changed: [192.168.0.201] TASK [k3s/master : Create kubectl symlink] ******************************************************************************************************************************** Wednesday 23 September 2020 16:18:22 +0200 (0:00:02.239) 0:02:09.354 *** ok: [192.168.0.201] TASK [k3s/master : Create crictl symlink] ********************************************************************************************************************************* Wednesday 23 September 2020 16:18:23 +0200 (0:00:00.973) 0:02:10.328 *** ok: [192.168.0.201] PLAY [node] *************************************************************************************************************************************************************** TASK [Gathering Facts] **************************************************************************************************************************************************** Wednesday 23 September 2020 16:18:25 +0200 (0:00:01.351) 0:02:11.680 *** ok: [192.168.0.203] ok: [192.168.0.205] ok: [192.168.0.202] ok: [192.168.0.204] TASK [k3s/node : Copy K3s service file] *********************************************************************************************************************************** Wednesday 23 September 2020 16:18:35 +0200 (0:00:09.865) 0:02:21.546 *** ok: [192.168.0.202] ok: [192.168.0.203] ok: [192.168.0.204] ok: [192.168.0.205] TASK [k3s/node : Enable and check K3s service] **************************************************************************************************************************** Wednesday 23 September 2020 16:18:37 +0200 (0:00:02.715) 0:02:24.261 *** changed: [192.168.0.202] changed: [192.168.0.204] changed: [192.168.0.205] changed: [192.168.0.203] TASK [k3s/node : Create directory .kube] ********************************************************************************************************************************** Wednesday 23 September 2020 16:18:46 +0200 (0:00:08.523) 0:02:32.785 *** ok: [192.168.0.202] ok: [192.168.0.203] ok: [192.168.0.204] ok: [192.168.0.205] TASK [k3s/node : Create kubectl/crictl symlinks] ************************************************************************************************************************** Wednesday 23 September 2020 16:18:48 +0200 (0:00:01.660) 0:02:34.446 *** ok: [192.168.0.202] => (item=kubectl) ok: [192.168.0.203] => (item=kubectl) ok: [192.168.0.204] => (item=kubectl) ok: [192.168.0.202] => (item=crictl) ok: [192.168.0.205] => (item=kubectl) ok: [192.168.0.203] => (item=crictl) ok: [192.168.0.204] => (item=crictl) ok: [192.168.0.205] => (item=crictl) TASK [k3s/node : fetch the ~/.kube/config file] *************************************************************************************************************************** Wednesday 23 September 2020 16:18:50 +0200 (0:00:02.746) 0:02:37.192 *** skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] PLAY RECAP **************************************************************************************************************************************************************** 192.168.0.201 : ok=21 changed=7 unreachable=0 failed=0 skipped=12 rescued=0 ignored=0 192.168.0.202 : ok=12 changed=3 unreachable=0 failed=0 skipped=13 rescued=0 ignored=0 192.168.0.203 : ok=12 changed=3 unreachable=0 failed=0 skipped=13 rescued=0 ignored=0 192.168.0.204 : ok=12 changed=3 unreachable=0 failed=0 skipped=13 rescued=0 ignored=0 192.168.0.205 : ok=12 changed=3 unreachable=0 failed=0 skipped=13 rescued=0 ignored=0 Wednesday 23 September 2020 16:18:51 +0200 (0:00:00.630) 0:02:37.823 *** =============================================================================== download : Download k3s binary arm64 ------------------------------------------------------------------------------------------------------------------------------ 59.06s k3s/master : Enable and check K3s service ------------------------------------------------------------------------------------------------------------------------- 19.55s Gathering Facts --------------------------------------------------------------------------------------------------------------------------------------------------- 10.33s Gathering Facts ---------------------------------------------------------------------------------------------------------------------------------------------------- 9.87s k3s/node : Enable and check K3s service ---------------------------------------------------------------------------------------------------------------------------- 8.52s Gathering Facts ---------------------------------------------------------------------------------------------------------------------------------------------------- 7.51s k3s/node : Create kubectl/crictl symlinks -------------------------------------------------------------------------------------------------------------------------- 2.75s k3s/node : Copy K3s service file ----------------------------------------------------------------------------------------------------------------------------------- 2.72s k3s/master : Replace https://localhost:6443 by https://master-ip:6443 ---------------------------------------------------------------------------------------------- 2.24s k3s/master : Copy K3s service file --------------------------------------------------------------------------------------------------------------------------------- 2.23s ubuntu : Enable cgroup via boot commandline if not already enabled for Ubuntu on ARM ------------------------------------------------------------------------------- 1.89s prereq : Enable IPv6 forwarding ------------------------------------------------------------------------------------------------------------------------------------ 1.78s download : Delete k3s if already present --------------------------------------------------------------------------------------------------------------------------- 1.71s k3s/master : Wait for node-token ----------------------------------------------------------------------------------------------------------------------------------- 1.66s k3s/node : Create directory .kube ---------------------------------------------------------------------------------------------------------------------------------- 1.66s prereq : Enable IPv4 forwarding ------------------------------------------------------------------------------------------------------------------------------------ 1.66s k3s/master : Copy config file to user home directory --------------------------------------------------------------------------------------------------------------- 1.59s k3s/master : Create crictl symlink --------------------------------------------------------------------------------------------------------------------------------- 1.35s k3s/master : Read node-token from master --------------------------------------------------------------------------------------------------------------------------- 1.29s k3s/master : Register node-token file access mode ------------------------------------------------------------------------------------------------------------------ 1.25s k3s is up and running? \u00b6 Wow the installation went rather fast - an exciting moment - is k3s working fine? gdha@n5:~/projects/k3s-ansible$ k3s --version k3s version v1.26.0+k3s2 (f0ec6a4c) go version go1.19.4 gdha@n5:~/projects/k3s-ansible$ k3s kubectl cluster-info kubernetes control plane is running at https://127.0.0.1:6443 CoreDNS is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. The connection to the server localhost:8080 was refused - did you specify the right host or port? Ok - k3s needs the cluster configuration file via variable KUBECONFIG or the file ~/kube/config . We are on a worker node (n5) and choose as master node n1, therefore, the cluster configuration was created and resides on the master node. Or, we copy this to each node or decide only the work from the master node. It is a much better choice to copy the configuration to each node instead. To do so first copy the cluster configuration file from the master node (n1) to this node (n5): gdha@n5:~/projects/k3s-ansible$ scp n1:.kube/config ~/.kube/ config 100% 2793 2.0MB/s 00:00 Now see if we have more luck with a kubernetes command to get all the nodes in this cluster and dumping the cluster-info? gdha@n5:~/projects/k3s-ansible$ kubectl get nodes NAME STATUS ROLES AGE VERSION n4 Ready <none> 9m39s v1.26.0+k3s2 n2 Ready <none> 9m39s v1.26.0+k3s2 n3 Ready <none> 9m39s v1.26.0+k3s2 n5 Ready <none> 9m24s v1.26.0+k3s2 n1 Ready control-plane,master 10m v1.26.0+k3s2 gdha@n5:~/projects/k3s-ansible$ k3s kubectl cluster-info Kubernetes master is running at https://192.168.0.201:6443 CoreDNS is running at https://192.168.0.201:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://192.168.0.201:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. Yes, that works fine, so lets copy the configuration file to the other nodes as well: gdha@n5:~/projects/k3s-ansible$ scp ~/.kube/config n2:~/.kube/ config 100% 2793 1.9MB/s 00:00 gdha@n5:~/projects/k3s-ansible$ scp ~/.kube/config n3:~/.kube/ config 100% 2793 1.7MB/s 00:00 gdha@n5:~/projects/k3s-ansible$ scp ~/.kube/config n4:~/.kube/ config And finally, to conclude this story, what pods are running within a basic k3s setup? gdha@n5:~/projects/k3s-ansible$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system local-path-provisioner-5d56847996-mdlw9 1/1 Running 0 10m kube-system coredns-5c6b6c5476-h2p6s 1/1 Running 0 10m kube-system metrics-server-7b67f64457-bk84t 1/1 Running 0 10m Remove k3s (cleanup nodes) \u00b6 In case you want to get rid of k3s then just run the reset.yml file with ansible-playbook: $ ansible-playbook reset.yml -i inventory/my-cluster/hosts.ini PLAY [k3s_cluster] **************************************************************************************************************************************** TASK [Gathering Facts] ************************************************************************************************************************************ Tuesday 24 January 2023 10:15:20 +0100 (0:00:00.153) 0:00:00.153 ******* ok: [192.168.0.205] ok: [192.168.0.204] ok: [192.168.0.201] ok: [192.168.0.203] ok: [192.168.0.202] TASK [reset : Disable services] *************************************************************************************************************************** Tuesday 24 January 2023 10:15:30 +0100 (0:00:09.803) 0:00:09.957 ******* ok: [192.168.0.205] => (item=k3s) ok: [192.168.0.204] => (item=k3s) ok: [192.168.0.202] => (item=k3s) changed: [192.168.0.201] => (item=k3s) ok: [192.168.0.201] => (item=k3s-node) ok: [192.168.0.203] => (item=k3s) changed: [192.168.0.205] => (item=k3s-node) changed: [192.168.0.204] => (item=k3s-node) changed: [192.168.0.203] => (item=k3s-node) changed: [192.168.0.202] => (item=k3s-node) TASK [reset : pkill -9 -f \"k3s/data/[^/]+/bin/containerd-shim-runc\"] ************************************************************************************** Tuesday 24 January 2023 10:15:40 +0100 (0:00:10.608) 0:00:20.565 ******* changed: [192.168.0.205] changed: [192.168.0.204] changed: [192.168.0.202] changed: [192.168.0.201] changed: [192.168.0.203] TASK [reset : Umount k3s filesystems] ********************************************************************************************************************* Tuesday 24 January 2023 10:15:45 +0100 (0:00:04.298) 0:00:24.864 ******* included: /home/gdha/projects/k3s-ansible/roles/reset/tasks/umount_with_childeren.yml for 192.168.0.201, 192.168.0.202, 192.168.0.203, 192.168.0.204, 192.168.0.205 included: /home/gdha/projects/k3s-ansible/roles/reset/tasks/umount_with_childeren.yml for 192.168.0.201, 192.168.0.202, 192.168.0.203, 192.168.0.204, 192.168.0.205 included: /home/gdha/projects/k3s-ansible/roles/reset/tasks/umount_with_childeren.yml for 192.168.0.201, 192.168.0.202, 192.168.0.203, 192.168.0.204, 192.168.0.205 included: /home/gdha/projects/k3s-ansible/roles/reset/tasks/umount_with_childeren.yml for 192.168.0.201, 192.168.0.202, 192.168.0.203, 192.168.0.204, 192.168.0.205 TASK [reset : Get the list of mounted filesystems] ******************************************************************************************************** Tuesday 24 January 2023 10:15:46 +0100 (0:00:00.838) 0:00:25.702 ******* changed: [192.168.0.201] changed: [192.168.0.205] changed: [192.168.0.202] changed: [192.168.0.203] changed: [192.168.0.204] TASK [reset : Umount filesystem] ************************************************************************************************************************** Tuesday 24 January 2023 10:15:48 +0100 (0:00:02.230) 0:00:27.933 ******* changed: [192.168.0.201] => (item=/run/k3s/containerd/io.containerd.grpc.v1.cri/sandboxes/a9f438036d703b39a747ef7615057b32310f437f97c4b82afe93a2e96a14b051/shm) changed: [192.168.0.202] => (item=/run/k3s/containerd/io.containerd.grpc.v1.cri/sandboxes/4086374a4e4dbd65831b95c5b0edc6f0a7d41fb6372c2f03b5e577ef695d1f36/shm) ... TASK [reset : Get the list of mounted filesystems] ******************************************************************************************************** Tuesday 24 January 2023 10:16:29 +0100 (0:00:41.594) 0:01:09.527 ******* changed: [192.168.0.201] changed: [192.168.0.205] changed: [192.168.0.202] changed: [192.168.0.203] changed: [192.168.0.204] TASK [reset : Umount filesystem] ************************************************************************************************************************** Tuesday 24 January 2023 10:16:32 +0100 (0:00:02.237) 0:01:11.764 ******* changed: [192.168.0.201] => (item=/var/lib/kubelet/pods/11aece04-899a-4312-b2ee-52fc334fff7a/volumes/kubernetes.io~projected/kube-api-access-672xr) changed: [192.168.0.205] => (item=/var/lib/kubelet/pods/8eb78e3d-c429-40b2-b40e-8cdd6cb4ac2a/volumes/kubernetes.io~projected/kube-api-access-5hqn7) ... TASK [reset : Get the list of mounted filesystems] ******************************************************************************************************** Tuesday 24 January 2023 10:16:48 +0100 (0:00:16.830) 0:01:28.595 ******* changed: [192.168.0.201] changed: [192.168.0.202] changed: [192.168.0.203] changed: [192.168.0.204] ok: [192.168.0.205] TASK [reset : Umount filesystem] ************************************************************************************************************************** Tuesday 24 January 2023 10:16:52 +0100 (0:00:03.382) 0:01:31.978 ******* changed: [192.168.0.202] => (item=/run/netns/cni-a04edc88-c0e2-9e73-e490-e574240e46ad) changed: [192.168.0.203] => (item=/run/netns/cni-1383df7a-2708-4359-52ff-1c83d7909765) changed: [192.168.0.204] => (item=/run/netns/cni-4b65f454-42dd-0e3c-b67e-2d0111e17669) changed: [192.168.0.201] => (item=/run/netns/cni-6600c459-286b-161e-3466-90a461c7fe3d) changed: [192.168.0.202] => (item=/run/netns/cni-a02f0dd6-800b-b560-7a44-5e7d5cfac9e2) changed: [192.168.0.203] => (item=/run/netns/cni-b69e3eb5-c128-c70f-f7f9-e3790103730d) changed: [192.168.0.201] => (item=/run/netns/cni-c9d8c3d1-0d36-41ba-11e7-f621b2dfe11b) changed: [192.168.0.201] => (item=/run/netns/cni-7d3cdeef-d898-48b7-e305-68d43e59d0b0) changed: [192.168.0.202] => (item=/run/netns/cni-c9e0d034-a5bb-6306-64f6-24d17b0dd1f1) changed: [192.168.0.202] => (item=/run/netns/cni-a0393025-d663-28f3-3bf8-3cf3620f9076) changed: [192.168.0.202] => (item=/run/netns/cni-1322e940-a422-2c2f-3a0d-1bcbb9aef2f9) changed: [192.168.0.202] => (item=/run/netns/cni-84cfb60a-24e1-013b-c727-b43a659785c0) TASK [reset : Get the list of mounted filesystems] ******************************************************************************************************** Tuesday 24 January 2023 10:17:03 +0100 (0:00:11.126) 0:01:43.104 ******* ok: [192.168.0.201] ok: [192.168.0.205] ok: [192.168.0.202] ok: [192.168.0.203] ok: [192.168.0.204] TASK [reset : Umount filesystem] ************************************************************************************************************************** Tuesday 24 January 2023 10:17:05 +0100 (0:00:02.246) 0:01:45.351 ******* TASK [reset : Remove service files, binaries and data] **************************************************************************************************** Tuesday 24 January 2023 10:17:06 +0100 (0:00:00.405) 0:01:45.757 ******* ok: [192.168.0.205] => (item=/etc/systemd/system/k3s.service) changed: [192.168.0.201] => (item=/etc/systemd/system/k3s.service) ok: [192.168.0.202] => (item=/etc/systemd/system/k3s.service) ok: [192.168.0.203] => (item=/etc/systemd/system/k3s.service) ok: [192.168.0.204] => (item=/etc/systemd/system/k3s.service) changed: [192.168.0.205] => (item=/etc/systemd/system/k3s-node.service) ok: [192.168.0.201] => (item=/etc/systemd/system/k3s-node.service) ok: [192.168.0.205] => (item=/etc/rancher/k3s) changed: [192.168.0.201] => (item=/etc/rancher/k3s) changed: [192.168.0.203] => (item=/etc/systemd/system/k3s-node.service) changed: [192.168.0.202] => (item=/etc/systemd/system/k3s-node.service) changed: [192.168.0.204] => (item=/etc/systemd/system/k3s-node.service) ok: [192.168.0.203] => (item=/etc/rancher/k3s) ok: [192.168.0.204] => (item=/etc/rancher/k3s) ok: [192.168.0.202] => (item=/etc/rancher/k3s) changed: [192.168.0.201] => (item=/var/lib/rancher/k3s) changed: [192.168.0.201] => (item=/var/lib/kubelet) changed: [192.168.0.201] => (item=/usr/local/bin/k3s) changed: [192.168.0.202] => (item=/var/lib/rancher/k3s) changed: [192.168.0.202] => (item=/var/lib/kubelet) changed: [192.168.0.202] => (item=/usr/local/bin/k3s) changed: [192.168.0.204] => (item=/var/lib/rancher/k3s) changed: [192.168.0.204] => (item=/var/lib/kubelet) changed: [192.168.0.204] => (item=/usr/local/bin/k3s) changed: [192.168.0.203] => (item=/var/lib/rancher/k3s) changed: [192.168.0.203] => (item=/var/lib/kubelet) changed: [192.168.0.203] => (item=/usr/local/bin/k3s) changed: [192.168.0.205] => (item=/var/lib/rancher/k3s) changed: [192.168.0.205] => (item=/var/lib/kubelet) changed: [192.168.0.205] => (item=/usr/local/bin/k3s) TASK [reset : daemon_reload] ****************************************************************************************************************************** Tuesday 24 January 2023 10:19:15 +0100 (0:02:09.797) 0:03:55.555 ******* ok: [192.168.0.205] ok: [192.168.0.201] ok: [192.168.0.203] ok: [192.168.0.204] ok: [192.168.0.202] PLAY RECAP ************************************************************************************************************************************************ 192.168.0.201 : ok=16 changed=9 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 192.168.0.202 : ok=16 changed=9 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 192.168.0.203 : ok=16 changed=9 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 192.168.0.204 : ok=16 changed=9 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 192.168.0.205 : ok=15 changed=7 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0 Tuesday 24 January 2023 10:19:23 +0100 (0:00:07.744) 0:04:03.299 ******* =============================================================================== reset : Remove service files, binaries and data -------------------------------------------------------------------------------------------------- 129.80s reset : Umount filesystem ------------------------------------------------------------------------------------------------------------------------- 41.59s reset : Umount filesystem ------------------------------------------------------------------------------------------------------------------------- 16.83s reset : Umount filesystem ------------------------------------------------------------------------------------------------------------------------- 11.13s reset : Disable services -------------------------------------------------------------------------------------------------------------------------- 10.61s Gathering Facts ------------------------------------------------------------------------------------------------------------------------------------ 9.80s reset : daemon_reload ------------------------------------------------------------------------------------------------------------------------------ 7.74s reset : pkill -9 -f \"k3s/data/[^/]+/bin/containerd-shim-runc\" -------------------------------------------------------------------------------------- 4.30s reset : Get the list of mounted filesystems -------------------------------------------------------------------------------------------------------- 3.38s reset : Get the list of mounted filesystems -------------------------------------------------------------------------------------------------------- 2.25s reset : Get the list of mounted filesystems -------------------------------------------------------------------------------------------------------- 2.24s reset : Get the list of mounted filesystems -------------------------------------------------------------------------------------------------------- 2.23s reset : Umount k3s filesystems --------------------------------------------------------------------------------------------------------------------- 0.84s reset : Umount filesystem -------------------------------------------------------------------------------------------------------------------------- 0.41s References \u00b6 [1] Ansible k3s-ansible playbook [2] Deploying a highly-available K3s with K3sup Edit history \u00b6 added running reset (remove k3s) on 24/Jan/2023 initial post on 28/Sep/2020","title":"Raspberry Pi 4 Installing k3s software"},{"location":"pi-stories3/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories3/#raspberry_pi_4_cluster_series_-_installing_k3s_software","text":"","title":"Raspberry Pi 4 cluster Series - Installing k3s software"},{"location":"pi-stories3/#why_k3s","text":"K3s is a fully compliant Kubernetes distribution in a single binary perfectly suitable for smaller edge devices such as the Raspberry PI4. Simply said, you can do almost exactly the same as with its big sister kubernetes (k8s). For these kind of devices it is the perfect match. To get ks3 installed with ansible clone the playbook k3s-ansible playbook [1]. K3s is a product from Rancher Labs and can be installed in different ways, such as with k3sup (read the nice article \" Deploying a highly-available K3s with K3sup \") or with a fork of k3s-ansible sources. We choose for the latter and made some customisation to fork of k3s-ansible playbook [1]. Do check the inventory/my-cluster/hosts.ini file and add you preferences. Also, do not forget to adjust the attributes yaml file inventory/my-cluster/group_vars/all.yml . Especially, check the k3s_version you want to install. To find the latest stable release of k3s see the github release page of ks3s .","title":"Why k3s?"},{"location":"pi-stories3/#run_the_ansible_playbook_siteyml","text":"gdha@n5:~/projects/k3s-ansible$ ansible-playbook site.yml -i inventory/my-cluster/hosts.ini [WARNING]: While constructing a mapping from /home/gdha/projects/k3s-ansible/roles/ubuntu/tasks/main.yml, line 4, column 5, found a duplicate dict key (backrefs). Using last defined value only. PLAY [k3s_cluster] ******************************************************************************************************************************************************** TASK [Gathering Facts] **************************************************************************************************************************************************** Wednesday 23 September 2020 16:16:13 +0200 (0:00:00.102) 0:00:00.102 *** ok: [192.168.0.202] ok: [192.168.0.204] ok: [192.168.0.203] ok: [192.168.0.201] ok: [192.168.0.205] TASK [prereq : Set SELinux to disabled state] ***************************************************************************************************************************** Wednesday 23 September 2020 16:16:24 +0200 (0:00:10.334) 0:00:10.437 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [prereq : Enable IPv4 forwarding] ************************************************************************************************************************************ Wednesday 23 September 2020 16:16:24 +0200 (0:00:00.913) 0:00:11.350 *** ok: [192.168.0.203] ok: [192.168.0.201] ok: [192.168.0.202] ok: [192.168.0.204] ok: [192.168.0.205] TASK [prereq : Enable IPv6 forwarding] ************************************************************************************************************************************ Wednesday 23 September 2020 16:16:26 +0200 (0:00:01.658) 0:00:13.009 *** ok: [192.168.0.201] ok: [192.168.0.202] ok: [192.168.0.203] ok: [192.168.0.204] ok: [192.168.0.205] TASK [prereq : Add br_netfilter to /etc/modules-load.d/] ****************************************************************************************************************** Wednesday 23 September 2020 16:16:28 +0200 (0:00:01.781) 0:00:14.790 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [prereq : Load br_netfilter] ***************************************************************************************************************************************** Wednesday 23 September 2020 16:16:29 +0200 (0:00:00.905) 0:00:15.696 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [prereq : Set bridge-nf-call-iptables (just to be sure)] ************************************************************************************************************* Wednesday 23 September 2020 16:16:30 +0200 (0:00:00.909) 0:00:16.605 *** skipping: [192.168.0.201] => (item=net.bridge.bridge-nf-call-iptables) skipping: [192.168.0.201] => (item=net.bridge.bridge-nf-call-ip6tables) skipping: [192.168.0.202] => (item=net.bridge.bridge-nf-call-iptables) skipping: [192.168.0.202] => (item=net.bridge.bridge-nf-call-ip6tables) skipping: [192.168.0.203] => (item=net.bridge.bridge-nf-call-iptables) skipping: [192.168.0.203] => (item=net.bridge.bridge-nf-call-ip6tables) skipping: [192.168.0.204] => (item=net.bridge.bridge-nf-call-iptables) skipping: [192.168.0.204] => (item=net.bridge.bridge-nf-call-ip6tables) skipping: [192.168.0.205] => (item=net.bridge.bridge-nf-call-iptables) skipping: [192.168.0.205] => (item=net.bridge.bridge-nf-call-ip6tables) TASK [prereq : Add /usr/local/bin to sudo secure_path] ******************************************************************************************************************** Wednesday 23 September 2020 16:16:31 +0200 (0:00:00.939) 0:00:17.544 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [download : Delete k3s if already present] *************************************************************************************************************************** Wednesday 23 September 2020 16:16:32 +0200 (0:00:01.169) 0:00:18.714 *** changed: [192.168.0.201] changed: [192.168.0.203] changed: [192.168.0.202] changed: [192.168.0.204] changed: [192.168.0.205] TASK [download : Download k3s binary x64] ********************************************************************************************************************************* Wednesday 23 September 2020 16:16:34 +0200 (0:00:01.707) 0:00:20.422 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [download : Download k3s binary arm64] ******************************************************************************************************************************* Wednesday 23 September 2020 16:16:34 +0200 (0:00:00.943) 0:00:21.365 *** changed: [192.168.0.202] changed: [192.168.0.204] changed: [192.168.0.201] changed: [192.168.0.205] changed: [192.168.0.203] TASK [download : Download k3s binary armhf] ******************************************************************************************************************************* Wednesday 23 September 2020 16:17:34 +0200 (0:00:59.059) 0:01:20.425 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [raspbian : Test for Raspbian] *************************************************************************************************************************************** Wednesday 23 September 2020 16:17:34 +0200 (0:00:00.940) 0:01:21.365 *** ok: [192.168.0.201] ok: [192.168.0.202] ok: [192.168.0.203] ok: [192.168.0.204] ok: [192.168.0.205] TASK [raspbian : Activating cgroup support] ******************************************************************************************************************************* Wednesday 23 September 2020 16:17:36 +0200 (0:00:01.165) 0:01:22.531 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [raspbian : Flush iptables before changing to iptables-legacy] ******************************************************************************************************* Wednesday 23 September 2020 16:17:36 +0200 (0:00:00.796) 0:01:23.327 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [raspbian : Changing to iptables-legacy] ***************************************************************************************************************************** Wednesday 23 September 2020 16:17:37 +0200 (0:00:00.795) 0:01:24.123 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [raspbian : Changing to ip6tables-legacy] **************************************************************************************************************************** Wednesday 23 September 2020 16:17:38 +0200 (0:00:00.798) 0:01:24.922 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [raspbian : Rebooting] *********************************************************************************************************************************************** Wednesday 23 September 2020 16:17:39 +0200 (0:00:00.940) 0:01:25.862 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [ubuntu : Enable cgroup via boot commandline if not already enabled for Ubuntu on ARM] ******************************************************************************* Wednesday 23 September 2020 16:17:40 +0200 (0:00:00.800) 0:01:26.663 *** ok: [192.168.0.203] ok: [192.168.0.202] ok: [192.168.0.201] ok: [192.168.0.204] ok: [192.168.0.205] PLAY [master] ************************************************************************************************************************************************************* TASK [Gathering Facts] **************************************************************************************************************************************************** Wednesday 23 September 2020 16:17:42 +0200 (0:00:01.887) 0:01:28.550 *** ok: [192.168.0.201] TASK [k3s/master : Copy K3s service file] ********************************************************************************************************************************* Wednesday 23 September 2020 16:17:49 +0200 (0:00:07.512) 0:01:36.063 *** ok: [192.168.0.201] TASK [k3s/master : Enable and check K3s service] ************************************************************************************************************************** Wednesday 23 September 2020 16:17:51 +0200 (0:00:02.229) 0:01:38.293 *** changed: [192.168.0.201] TASK [k3s/master : Wait for node-token] *********************************************************************************************************************************** Wednesday 23 September 2020 16:18:11 +0200 (0:00:19.552) 0:01:57.846 *** ok: [192.168.0.201] TASK [k3s/master : Register node-token file access mode] ****************************************************************************************************************** Wednesday 23 September 2020 16:18:13 +0200 (0:00:01.661) 0:01:59.508 *** ok: [192.168.0.201] TASK [k3s/master : Change file access node-token] ************************************************************************************************************************* Wednesday 23 September 2020 16:18:14 +0200 (0:00:01.250) 0:02:00.758 *** changed: [192.168.0.201] TASK [k3s/master : Read node-token from master] *************************************************************************************************************************** Wednesday 23 September 2020 16:18:15 +0200 (0:00:01.036) 0:02:01.794 *** ok: [192.168.0.201] TASK [k3s/master : Store Master node-token] ******************************************************************************************************************************* Wednesday 23 September 2020 16:18:16 +0200 (0:00:01.293) 0:02:03.087 *** ok: [192.168.0.201] TASK [k3s/master : Restore node-token file access] ************************************************************************************************************************ Wednesday 23 September 2020 16:18:17 +0200 (0:00:00.277) 0:02:03.365 *** changed: [192.168.0.201] TASK [k3s/master : Create directory .kube] ******************************************************************************************************************************** Wednesday 23 September 2020 16:18:18 +0200 (0:00:01.079) 0:02:04.445 *** ok: [192.168.0.201] TASK [k3s/master : Copy config file to user home directory] *************************************************************************************************************** Wednesday 23 September 2020 16:18:19 +0200 (0:00:01.083) 0:02:05.528 *** changed: [192.168.0.201] TASK [k3s/master : Replace https://localhost:6443 by https://master-ip:6443] ********************************************************************************************** Wednesday 23 September 2020 16:18:20 +0200 (0:00:01.586) 0:02:07.115 *** changed: [192.168.0.201] TASK [k3s/master : Create kubectl symlink] ******************************************************************************************************************************** Wednesday 23 September 2020 16:18:22 +0200 (0:00:02.239) 0:02:09.354 *** ok: [192.168.0.201] TASK [k3s/master : Create crictl symlink] ********************************************************************************************************************************* Wednesday 23 September 2020 16:18:23 +0200 (0:00:00.973) 0:02:10.328 *** ok: [192.168.0.201] PLAY [node] *************************************************************************************************************************************************************** TASK [Gathering Facts] **************************************************************************************************************************************************** Wednesday 23 September 2020 16:18:25 +0200 (0:00:01.351) 0:02:11.680 *** ok: [192.168.0.203] ok: [192.168.0.205] ok: [192.168.0.202] ok: [192.168.0.204] TASK [k3s/node : Copy K3s service file] *********************************************************************************************************************************** Wednesday 23 September 2020 16:18:35 +0200 (0:00:09.865) 0:02:21.546 *** ok: [192.168.0.202] ok: [192.168.0.203] ok: [192.168.0.204] ok: [192.168.0.205] TASK [k3s/node : Enable and check K3s service] **************************************************************************************************************************** Wednesday 23 September 2020 16:18:37 +0200 (0:00:02.715) 0:02:24.261 *** changed: [192.168.0.202] changed: [192.168.0.204] changed: [192.168.0.205] changed: [192.168.0.203] TASK [k3s/node : Create directory .kube] ********************************************************************************************************************************** Wednesday 23 September 2020 16:18:46 +0200 (0:00:08.523) 0:02:32.785 *** ok: [192.168.0.202] ok: [192.168.0.203] ok: [192.168.0.204] ok: [192.168.0.205] TASK [k3s/node : Create kubectl/crictl symlinks] ************************************************************************************************************************** Wednesday 23 September 2020 16:18:48 +0200 (0:00:01.660) 0:02:34.446 *** ok: [192.168.0.202] => (item=kubectl) ok: [192.168.0.203] => (item=kubectl) ok: [192.168.0.204] => (item=kubectl) ok: [192.168.0.202] => (item=crictl) ok: [192.168.0.205] => (item=kubectl) ok: [192.168.0.203] => (item=crictl) ok: [192.168.0.204] => (item=crictl) ok: [192.168.0.205] => (item=crictl) TASK [k3s/node : fetch the ~/.kube/config file] *************************************************************************************************************************** Wednesday 23 September 2020 16:18:50 +0200 (0:00:02.746) 0:02:37.192 *** skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] PLAY RECAP **************************************************************************************************************************************************************** 192.168.0.201 : ok=21 changed=7 unreachable=0 failed=0 skipped=12 rescued=0 ignored=0 192.168.0.202 : ok=12 changed=3 unreachable=0 failed=0 skipped=13 rescued=0 ignored=0 192.168.0.203 : ok=12 changed=3 unreachable=0 failed=0 skipped=13 rescued=0 ignored=0 192.168.0.204 : ok=12 changed=3 unreachable=0 failed=0 skipped=13 rescued=0 ignored=0 192.168.0.205 : ok=12 changed=3 unreachable=0 failed=0 skipped=13 rescued=0 ignored=0 Wednesday 23 September 2020 16:18:51 +0200 (0:00:00.630) 0:02:37.823 *** =============================================================================== download : Download k3s binary arm64 ------------------------------------------------------------------------------------------------------------------------------ 59.06s k3s/master : Enable and check K3s service ------------------------------------------------------------------------------------------------------------------------- 19.55s Gathering Facts --------------------------------------------------------------------------------------------------------------------------------------------------- 10.33s Gathering Facts ---------------------------------------------------------------------------------------------------------------------------------------------------- 9.87s k3s/node : Enable and check K3s service ---------------------------------------------------------------------------------------------------------------------------- 8.52s Gathering Facts ---------------------------------------------------------------------------------------------------------------------------------------------------- 7.51s k3s/node : Create kubectl/crictl symlinks -------------------------------------------------------------------------------------------------------------------------- 2.75s k3s/node : Copy K3s service file ----------------------------------------------------------------------------------------------------------------------------------- 2.72s k3s/master : Replace https://localhost:6443 by https://master-ip:6443 ---------------------------------------------------------------------------------------------- 2.24s k3s/master : Copy K3s service file --------------------------------------------------------------------------------------------------------------------------------- 2.23s ubuntu : Enable cgroup via boot commandline if not already enabled for Ubuntu on ARM ------------------------------------------------------------------------------- 1.89s prereq : Enable IPv6 forwarding ------------------------------------------------------------------------------------------------------------------------------------ 1.78s download : Delete k3s if already present --------------------------------------------------------------------------------------------------------------------------- 1.71s k3s/master : Wait for node-token ----------------------------------------------------------------------------------------------------------------------------------- 1.66s k3s/node : Create directory .kube ---------------------------------------------------------------------------------------------------------------------------------- 1.66s prereq : Enable IPv4 forwarding ------------------------------------------------------------------------------------------------------------------------------------ 1.66s k3s/master : Copy config file to user home directory --------------------------------------------------------------------------------------------------------------- 1.59s k3s/master : Create crictl symlink --------------------------------------------------------------------------------------------------------------------------------- 1.35s k3s/master : Read node-token from master --------------------------------------------------------------------------------------------------------------------------- 1.29s k3s/master : Register node-token file access mode ------------------------------------------------------------------------------------------------------------------ 1.25s","title":"Run the ansible playbook site.yml"},{"location":"pi-stories3/#k3s_is_up_and_running","text":"Wow the installation went rather fast - an exciting moment - is k3s working fine? gdha@n5:~/projects/k3s-ansible$ k3s --version k3s version v1.26.0+k3s2 (f0ec6a4c) go version go1.19.4 gdha@n5:~/projects/k3s-ansible$ k3s kubectl cluster-info kubernetes control plane is running at https://127.0.0.1:6443 CoreDNS is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. The connection to the server localhost:8080 was refused - did you specify the right host or port? Ok - k3s needs the cluster configuration file via variable KUBECONFIG or the file ~/kube/config . We are on a worker node (n5) and choose as master node n1, therefore, the cluster configuration was created and resides on the master node. Or, we copy this to each node or decide only the work from the master node. It is a much better choice to copy the configuration to each node instead. To do so first copy the cluster configuration file from the master node (n1) to this node (n5): gdha@n5:~/projects/k3s-ansible$ scp n1:.kube/config ~/.kube/ config 100% 2793 2.0MB/s 00:00 Now see if we have more luck with a kubernetes command to get all the nodes in this cluster and dumping the cluster-info? gdha@n5:~/projects/k3s-ansible$ kubectl get nodes NAME STATUS ROLES AGE VERSION n4 Ready <none> 9m39s v1.26.0+k3s2 n2 Ready <none> 9m39s v1.26.0+k3s2 n3 Ready <none> 9m39s v1.26.0+k3s2 n5 Ready <none> 9m24s v1.26.0+k3s2 n1 Ready control-plane,master 10m v1.26.0+k3s2 gdha@n5:~/projects/k3s-ansible$ k3s kubectl cluster-info Kubernetes master is running at https://192.168.0.201:6443 CoreDNS is running at https://192.168.0.201:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://192.168.0.201:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. Yes, that works fine, so lets copy the configuration file to the other nodes as well: gdha@n5:~/projects/k3s-ansible$ scp ~/.kube/config n2:~/.kube/ config 100% 2793 1.9MB/s 00:00 gdha@n5:~/projects/k3s-ansible$ scp ~/.kube/config n3:~/.kube/ config 100% 2793 1.7MB/s 00:00 gdha@n5:~/projects/k3s-ansible$ scp ~/.kube/config n4:~/.kube/ config And finally, to conclude this story, what pods are running within a basic k3s setup? gdha@n5:~/projects/k3s-ansible$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system local-path-provisioner-5d56847996-mdlw9 1/1 Running 0 10m kube-system coredns-5c6b6c5476-h2p6s 1/1 Running 0 10m kube-system metrics-server-7b67f64457-bk84t 1/1 Running 0 10m","title":"k3s is up and running?"},{"location":"pi-stories3/#remove_k3s_cleanup_nodes","text":"In case you want to get rid of k3s then just run the reset.yml file with ansible-playbook: $ ansible-playbook reset.yml -i inventory/my-cluster/hosts.ini PLAY [k3s_cluster] **************************************************************************************************************************************** TASK [Gathering Facts] ************************************************************************************************************************************ Tuesday 24 January 2023 10:15:20 +0100 (0:00:00.153) 0:00:00.153 ******* ok: [192.168.0.205] ok: [192.168.0.204] ok: [192.168.0.201] ok: [192.168.0.203] ok: [192.168.0.202] TASK [reset : Disable services] *************************************************************************************************************************** Tuesday 24 January 2023 10:15:30 +0100 (0:00:09.803) 0:00:09.957 ******* ok: [192.168.0.205] => (item=k3s) ok: [192.168.0.204] => (item=k3s) ok: [192.168.0.202] => (item=k3s) changed: [192.168.0.201] => (item=k3s) ok: [192.168.0.201] => (item=k3s-node) ok: [192.168.0.203] => (item=k3s) changed: [192.168.0.205] => (item=k3s-node) changed: [192.168.0.204] => (item=k3s-node) changed: [192.168.0.203] => (item=k3s-node) changed: [192.168.0.202] => (item=k3s-node) TASK [reset : pkill -9 -f \"k3s/data/[^/]+/bin/containerd-shim-runc\"] ************************************************************************************** Tuesday 24 January 2023 10:15:40 +0100 (0:00:10.608) 0:00:20.565 ******* changed: [192.168.0.205] changed: [192.168.0.204] changed: [192.168.0.202] changed: [192.168.0.201] changed: [192.168.0.203] TASK [reset : Umount k3s filesystems] ********************************************************************************************************************* Tuesday 24 January 2023 10:15:45 +0100 (0:00:04.298) 0:00:24.864 ******* included: /home/gdha/projects/k3s-ansible/roles/reset/tasks/umount_with_childeren.yml for 192.168.0.201, 192.168.0.202, 192.168.0.203, 192.168.0.204, 192.168.0.205 included: /home/gdha/projects/k3s-ansible/roles/reset/tasks/umount_with_childeren.yml for 192.168.0.201, 192.168.0.202, 192.168.0.203, 192.168.0.204, 192.168.0.205 included: /home/gdha/projects/k3s-ansible/roles/reset/tasks/umount_with_childeren.yml for 192.168.0.201, 192.168.0.202, 192.168.0.203, 192.168.0.204, 192.168.0.205 included: /home/gdha/projects/k3s-ansible/roles/reset/tasks/umount_with_childeren.yml for 192.168.0.201, 192.168.0.202, 192.168.0.203, 192.168.0.204, 192.168.0.205 TASK [reset : Get the list of mounted filesystems] ******************************************************************************************************** Tuesday 24 January 2023 10:15:46 +0100 (0:00:00.838) 0:00:25.702 ******* changed: [192.168.0.201] changed: [192.168.0.205] changed: [192.168.0.202] changed: [192.168.0.203] changed: [192.168.0.204] TASK [reset : Umount filesystem] ************************************************************************************************************************** Tuesday 24 January 2023 10:15:48 +0100 (0:00:02.230) 0:00:27.933 ******* changed: [192.168.0.201] => (item=/run/k3s/containerd/io.containerd.grpc.v1.cri/sandboxes/a9f438036d703b39a747ef7615057b32310f437f97c4b82afe93a2e96a14b051/shm) changed: [192.168.0.202] => (item=/run/k3s/containerd/io.containerd.grpc.v1.cri/sandboxes/4086374a4e4dbd65831b95c5b0edc6f0a7d41fb6372c2f03b5e577ef695d1f36/shm) ... TASK [reset : Get the list of mounted filesystems] ******************************************************************************************************** Tuesday 24 January 2023 10:16:29 +0100 (0:00:41.594) 0:01:09.527 ******* changed: [192.168.0.201] changed: [192.168.0.205] changed: [192.168.0.202] changed: [192.168.0.203] changed: [192.168.0.204] TASK [reset : Umount filesystem] ************************************************************************************************************************** Tuesday 24 January 2023 10:16:32 +0100 (0:00:02.237) 0:01:11.764 ******* changed: [192.168.0.201] => (item=/var/lib/kubelet/pods/11aece04-899a-4312-b2ee-52fc334fff7a/volumes/kubernetes.io~projected/kube-api-access-672xr) changed: [192.168.0.205] => (item=/var/lib/kubelet/pods/8eb78e3d-c429-40b2-b40e-8cdd6cb4ac2a/volumes/kubernetes.io~projected/kube-api-access-5hqn7) ... TASK [reset : Get the list of mounted filesystems] ******************************************************************************************************** Tuesday 24 January 2023 10:16:48 +0100 (0:00:16.830) 0:01:28.595 ******* changed: [192.168.0.201] changed: [192.168.0.202] changed: [192.168.0.203] changed: [192.168.0.204] ok: [192.168.0.205] TASK [reset : Umount filesystem] ************************************************************************************************************************** Tuesday 24 January 2023 10:16:52 +0100 (0:00:03.382) 0:01:31.978 ******* changed: [192.168.0.202] => (item=/run/netns/cni-a04edc88-c0e2-9e73-e490-e574240e46ad) changed: [192.168.0.203] => (item=/run/netns/cni-1383df7a-2708-4359-52ff-1c83d7909765) changed: [192.168.0.204] => (item=/run/netns/cni-4b65f454-42dd-0e3c-b67e-2d0111e17669) changed: [192.168.0.201] => (item=/run/netns/cni-6600c459-286b-161e-3466-90a461c7fe3d) changed: [192.168.0.202] => (item=/run/netns/cni-a02f0dd6-800b-b560-7a44-5e7d5cfac9e2) changed: [192.168.0.203] => (item=/run/netns/cni-b69e3eb5-c128-c70f-f7f9-e3790103730d) changed: [192.168.0.201] => (item=/run/netns/cni-c9d8c3d1-0d36-41ba-11e7-f621b2dfe11b) changed: [192.168.0.201] => (item=/run/netns/cni-7d3cdeef-d898-48b7-e305-68d43e59d0b0) changed: [192.168.0.202] => (item=/run/netns/cni-c9e0d034-a5bb-6306-64f6-24d17b0dd1f1) changed: [192.168.0.202] => (item=/run/netns/cni-a0393025-d663-28f3-3bf8-3cf3620f9076) changed: [192.168.0.202] => (item=/run/netns/cni-1322e940-a422-2c2f-3a0d-1bcbb9aef2f9) changed: [192.168.0.202] => (item=/run/netns/cni-84cfb60a-24e1-013b-c727-b43a659785c0) TASK [reset : Get the list of mounted filesystems] ******************************************************************************************************** Tuesday 24 January 2023 10:17:03 +0100 (0:00:11.126) 0:01:43.104 ******* ok: [192.168.0.201] ok: [192.168.0.205] ok: [192.168.0.202] ok: [192.168.0.203] ok: [192.168.0.204] TASK [reset : Umount filesystem] ************************************************************************************************************************** Tuesday 24 January 2023 10:17:05 +0100 (0:00:02.246) 0:01:45.351 ******* TASK [reset : Remove service files, binaries and data] **************************************************************************************************** Tuesday 24 January 2023 10:17:06 +0100 (0:00:00.405) 0:01:45.757 ******* ok: [192.168.0.205] => (item=/etc/systemd/system/k3s.service) changed: [192.168.0.201] => (item=/etc/systemd/system/k3s.service) ok: [192.168.0.202] => (item=/etc/systemd/system/k3s.service) ok: [192.168.0.203] => (item=/etc/systemd/system/k3s.service) ok: [192.168.0.204] => (item=/etc/systemd/system/k3s.service) changed: [192.168.0.205] => (item=/etc/systemd/system/k3s-node.service) ok: [192.168.0.201] => (item=/etc/systemd/system/k3s-node.service) ok: [192.168.0.205] => (item=/etc/rancher/k3s) changed: [192.168.0.201] => (item=/etc/rancher/k3s) changed: [192.168.0.203] => (item=/etc/systemd/system/k3s-node.service) changed: [192.168.0.202] => (item=/etc/systemd/system/k3s-node.service) changed: [192.168.0.204] => (item=/etc/systemd/system/k3s-node.service) ok: [192.168.0.203] => (item=/etc/rancher/k3s) ok: [192.168.0.204] => (item=/etc/rancher/k3s) ok: [192.168.0.202] => (item=/etc/rancher/k3s) changed: [192.168.0.201] => (item=/var/lib/rancher/k3s) changed: [192.168.0.201] => (item=/var/lib/kubelet) changed: [192.168.0.201] => (item=/usr/local/bin/k3s) changed: [192.168.0.202] => (item=/var/lib/rancher/k3s) changed: [192.168.0.202] => (item=/var/lib/kubelet) changed: [192.168.0.202] => (item=/usr/local/bin/k3s) changed: [192.168.0.204] => (item=/var/lib/rancher/k3s) changed: [192.168.0.204] => (item=/var/lib/kubelet) changed: [192.168.0.204] => (item=/usr/local/bin/k3s) changed: [192.168.0.203] => (item=/var/lib/rancher/k3s) changed: [192.168.0.203] => (item=/var/lib/kubelet) changed: [192.168.0.203] => (item=/usr/local/bin/k3s) changed: [192.168.0.205] => (item=/var/lib/rancher/k3s) changed: [192.168.0.205] => (item=/var/lib/kubelet) changed: [192.168.0.205] => (item=/usr/local/bin/k3s) TASK [reset : daemon_reload] ****************************************************************************************************************************** Tuesday 24 January 2023 10:19:15 +0100 (0:02:09.797) 0:03:55.555 ******* ok: [192.168.0.205] ok: [192.168.0.201] ok: [192.168.0.203] ok: [192.168.0.204] ok: [192.168.0.202] PLAY RECAP ************************************************************************************************************************************************ 192.168.0.201 : ok=16 changed=9 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 192.168.0.202 : ok=16 changed=9 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 192.168.0.203 : ok=16 changed=9 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 192.168.0.204 : ok=16 changed=9 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 192.168.0.205 : ok=15 changed=7 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0 Tuesday 24 January 2023 10:19:23 +0100 (0:00:07.744) 0:04:03.299 ******* =============================================================================== reset : Remove service files, binaries and data -------------------------------------------------------------------------------------------------- 129.80s reset : Umount filesystem ------------------------------------------------------------------------------------------------------------------------- 41.59s reset : Umount filesystem ------------------------------------------------------------------------------------------------------------------------- 16.83s reset : Umount filesystem ------------------------------------------------------------------------------------------------------------------------- 11.13s reset : Disable services -------------------------------------------------------------------------------------------------------------------------- 10.61s Gathering Facts ------------------------------------------------------------------------------------------------------------------------------------ 9.80s reset : daemon_reload ------------------------------------------------------------------------------------------------------------------------------ 7.74s reset : pkill -9 -f \"k3s/data/[^/]+/bin/containerd-shim-runc\" -------------------------------------------------------------------------------------- 4.30s reset : Get the list of mounted filesystems -------------------------------------------------------------------------------------------------------- 3.38s reset : Get the list of mounted filesystems -------------------------------------------------------------------------------------------------------- 2.25s reset : Get the list of mounted filesystems -------------------------------------------------------------------------------------------------------- 2.24s reset : Get the list of mounted filesystems -------------------------------------------------------------------------------------------------------- 2.23s reset : Umount k3s filesystems --------------------------------------------------------------------------------------------------------------------- 0.84s reset : Umount filesystem -------------------------------------------------------------------------------------------------------------------------- 0.41s","title":"Remove k3s (cleanup nodes)"},{"location":"pi-stories3/#references","text":"[1] Ansible k3s-ansible playbook [2] Deploying a highly-available K3s with K3sup","title":"References"},{"location":"pi-stories3/#edit_history","text":"added running reset (remove k3s) on 24/Jan/2023 initial post on 28/Sep/2020","title":"Edit history"},{"location":"pi-stories4/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Install awesome kubectl aliases \u00b6 The article \" Awesome Kubernetes Command-Line Hacks \" points out some interesting items to take in account. In summary activate the autocompletion and generate the kubectl aliases to make your life better. Enable kubectl autocompletion \u00b6 Enable kubectl autocompletion is one of the first things you have to do to make your life a bit easier. There are two ways in which you can do this: Source the completion script in your ~/.bashrc file: echo 'source <(kubectl completion bash)' >>~/.bashrc Add the completion script to the /etc/bash_completion.d directory: kubectl completion bash >/etc/bash_completion.d/kubectl If you have an alias for kubectl, you can extend shell completion to work with that alias: echo 'alias k=kubectl' >>~/.bashrc echo 'complete -F __start_kubectl k' >>~/.bashrc Above information come from kubernetes kubectl page . If you are alone then go for the first option, if more users require these autocompletion then go for the latter option. Kubectl aliases \u00b6 Kubectl commands can be long and hard to type over and over again, therefore, why not apply the kubectl-aliases github project . References \u00b6 [1] Awesome Kubernetes Command-Line Hacks [2] kubectl-aliases GitHub Source Edit history \u00b6 initial post on 30/Sep/2020","title":"Raspberry Pi 4 Install awesome kubectl aliases"},{"location":"pi-stories4/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories4/#raspberry_pi_4_cluster_series_-_install_awesome_kubectl_aliases","text":"The article \" Awesome Kubernetes Command-Line Hacks \" points out some interesting items to take in account. In summary activate the autocompletion and generate the kubectl aliases to make your life better.","title":"Raspberry Pi 4 cluster Series - Install awesome kubectl aliases"},{"location":"pi-stories4/#enable_kubectl_autocompletion","text":"Enable kubectl autocompletion is one of the first things you have to do to make your life a bit easier. There are two ways in which you can do this: Source the completion script in your ~/.bashrc file: echo 'source <(kubectl completion bash)' >>~/.bashrc Add the completion script to the /etc/bash_completion.d directory: kubectl completion bash >/etc/bash_completion.d/kubectl If you have an alias for kubectl, you can extend shell completion to work with that alias: echo 'alias k=kubectl' >>~/.bashrc echo 'complete -F __start_kubectl k' >>~/.bashrc Above information come from kubernetes kubectl page . If you are alone then go for the first option, if more users require these autocompletion then go for the latter option.","title":"Enable kubectl autocompletion"},{"location":"pi-stories4/#kubectl_aliases","text":"Kubectl commands can be long and hard to type over and over again, therefore, why not apply the kubectl-aliases github project .","title":"Kubectl aliases"},{"location":"pi-stories4/#references","text":"[1] Awesome Kubernetes Command-Line Hacks [2] kubectl-aliases GitHub Source","title":"References"},{"location":"pi-stories4/#edit_history","text":"initial post on 30/Sep/2020","title":"Edit history"},{"location":"pi-stories5/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Installing cert-manager on the k3s cluster \u00b6 As certificates are crucial in a kuberbetes cluster one of the first pods that one should install is cert-manager . Installing cert-manager \u00b6 Installation is extremelt easy with the following command: kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.11.0/cert-manager.yaml In the time of writing this article the current version was v1.11.0 - you can change that to the latest release available of course. Here follows an example of the instalaltion of cert-manager: gdha@n1:~$ kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.11.0/cert-manager.yaml namespace/cert-manager created customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created serviceaccount/cert-manager-cainjector created serviceaccount/cert-manager created serviceaccount/cert-manager-webhook created configmap/cert-manager-webhook created clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created clusterrole.rbac.authorization.k8s.io/cert-manager-view created clusterrole.rbac.authorization.k8s.io/cert-manager-edit created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created clusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created role.rbac.authorization.k8s.io/cert-manager:leaderelection created role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection created rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created service/cert-manager created service/cert-manager-webhook created deployment.apps/cert-manager-cainjector created deployment.apps/cert-manager created deployment.apps/cert-manager-webhook created mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created Just after previous command check if the cert-manager pods are created: $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system helm-install-traefik-p7jkh 0/1 Completed 0 49d kube-system metrics-server-7b4f8b595-bldsd 1/1 Running 3 49d kube-system local-path-provisioner-7ff9579c6-l6t6s 1/1 Running 3 49d kube-system svclb-traefik-r9q6r 2/2 Running 6 49d kube-system svclb-traefik-n6srr 2/2 Running 6 49d kube-system svclb-traefik-kxxn4 2/2 Running 6 49d kube-system coredns-66c464876b-vqrd6 1/1 Running 3 49d kube-system svclb-traefik-74k9f 2/2 Running 6 49d kube-system svclb-traefik-qlgn9 2/2 Running 6 49d kube-system traefik-5dd496474-4fwdm 1/1 Running 3 49d cert-manager cert-manager-86548b886-4xrbj 0/1 ContainerCreating 0 9s cert-manager cert-manager-cainjector-6d59c8d4f7-b2vdc 0/1 ContainerCreating 0 9s cert-manager cert-manager-webhook-578954cdd-lg5m4 0/1 ContainerCreating 0 9s After a minute or so check again with the wide option to see on which worker nodes the cert-managers pods are running: $ kubectl get pods -n cert-manager -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cert-manager-cainjector-6d59c8d4f7-b2vdc 1/1 Running 0 21m 10.42.1.17 n5 <none> <none> cert-manager-webhook-578954cdd-lg5m4 1/1 Running 0 21m 10.42.0.19 n1 <none> <none> cert-manager-86548b886-4xrbj 1/1 Running 0 21m 10.42.5.10 n4 <none> <none> References \u00b6 cert-manager documentation cert-manager sources Edit history \u00b6 24/Jan/2023: update with the installation of version v1.11.0","title":"Raspberry Pi 4 Installing cert-manager on our cluster"},{"location":"pi-stories5/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories5/#raspberry_pi_4_cluster_series_-_installing_cert-manager_on_the_k3s_cluster","text":"As certificates are crucial in a kuberbetes cluster one of the first pods that one should install is cert-manager .","title":"Raspberry Pi 4 cluster Series - Installing cert-manager on the k3s cluster"},{"location":"pi-stories5/#installing_cert-manager","text":"Installation is extremelt easy with the following command: kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.11.0/cert-manager.yaml In the time of writing this article the current version was v1.11.0 - you can change that to the latest release available of course. Here follows an example of the instalaltion of cert-manager: gdha@n1:~$ kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.11.0/cert-manager.yaml namespace/cert-manager created customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created serviceaccount/cert-manager-cainjector created serviceaccount/cert-manager created serviceaccount/cert-manager-webhook created configmap/cert-manager-webhook created clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created clusterrole.rbac.authorization.k8s.io/cert-manager-view created clusterrole.rbac.authorization.k8s.io/cert-manager-edit created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created clusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created role.rbac.authorization.k8s.io/cert-manager:leaderelection created role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection created rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created service/cert-manager created service/cert-manager-webhook created deployment.apps/cert-manager-cainjector created deployment.apps/cert-manager created deployment.apps/cert-manager-webhook created mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created Just after previous command check if the cert-manager pods are created: $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system helm-install-traefik-p7jkh 0/1 Completed 0 49d kube-system metrics-server-7b4f8b595-bldsd 1/1 Running 3 49d kube-system local-path-provisioner-7ff9579c6-l6t6s 1/1 Running 3 49d kube-system svclb-traefik-r9q6r 2/2 Running 6 49d kube-system svclb-traefik-n6srr 2/2 Running 6 49d kube-system svclb-traefik-kxxn4 2/2 Running 6 49d kube-system coredns-66c464876b-vqrd6 1/1 Running 3 49d kube-system svclb-traefik-74k9f 2/2 Running 6 49d kube-system svclb-traefik-qlgn9 2/2 Running 6 49d kube-system traefik-5dd496474-4fwdm 1/1 Running 3 49d cert-manager cert-manager-86548b886-4xrbj 0/1 ContainerCreating 0 9s cert-manager cert-manager-cainjector-6d59c8d4f7-b2vdc 0/1 ContainerCreating 0 9s cert-manager cert-manager-webhook-578954cdd-lg5m4 0/1 ContainerCreating 0 9s After a minute or so check again with the wide option to see on which worker nodes the cert-managers pods are running: $ kubectl get pods -n cert-manager -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cert-manager-cainjector-6d59c8d4f7-b2vdc 1/1 Running 0 21m 10.42.1.17 n5 <none> <none> cert-manager-webhook-578954cdd-lg5m4 1/1 Running 0 21m 10.42.0.19 n1 <none> <none> cert-manager-86548b886-4xrbj 1/1 Running 0 21m 10.42.5.10 n4 <none> <none>","title":"Installing cert-manager"},{"location":"pi-stories5/#references","text":"cert-manager documentation cert-manager sources","title":"References"},{"location":"pi-stories5/#edit_history","text":"24/Jan/2023: update with the installation of version v1.11.0","title":"Edit history"},{"location":"pi-stories6/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Upgrading k3s software on your cluster \u00b6 It is advisable to track the security vulnerabilities published by Rancher Labs around k3s. For example, in November 2020 a critical bug was detected in k3s (see [1]). Therefore, it is quite important to be able to update k3s without interrupting the k3s cluster, hence this procedure from Rancher Labs. $ kubectl get nodes NAME STATUS ROLES AGE VERSION n3 Ready <none> 117d v1.19.2+k3s1 n2 Ready <none> 117d v1.19.2+k3s1 n4 Ready <none> 117d v1.19.2+k3s1 n1 Ready master 117d v1.19.2+k3s1 n5 Ready <none> 117d v1.19.2+k3s1 CRD installation \u00b6 We will follow the procedure described in [2] and are required first to install a kubernetes Custom Resource Definition (CRD) [3] followed by creating a Plan. To find the latest version of the system upgrade controller use the following command (sources of Rancher [6]): VERSION=$(curl -s \"https://api.github.com/repos/rancher/system-upgrade-controller/releases/latest\" | awk -F '\"' '/tag_name/{print $4}') echo $VERSION To download the CRD locally run the following command: $ wget https://raw.githubusercontent.com/rancher/system-upgrade-controller/${VERSION}/manifests/system-upgrade-controller.yaml Now get it applied by: $ kubectl apply -f ./system-upgrade-controller.yaml namespace/system-upgrade created serviceaccount/system-upgrade created clusterrolebinding.rbac.authorization.k8s.io/system-upgrade created configmap/default-controller-env created deployment.apps/system-upgrade-controller created $ kubectl get all -n system-upgrade NAME READY STATUS RESTARTS AGE pod/system-upgrade-controller-556df575dd-2qfrs 1/1 Running 0 17s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/system-upgrade-controller 1/1 1 1 17s NAME DESIRED CURRENT READY AGE replicaset.apps/system-upgrade-controller-556df575dd 1 1 1 18s How to Upgrade the CRD \u00b6 VERSION=$(curl -s \"https://api.github.com/repos/rancher/system-upgrade-controller/releases/latest\" | awk -F '\"' '/tag_name/{print $4}') echo $VERSION $ wget https://raw.githubusercontent.com/rancher/system-upgrade-controller/${VERSION}/manifests/system-upgrade-controller.yaml $ kubectl replace -f ./system-upgrade-controller.yaml Making a k3s upgrade plan \u00b6 Before making a plan we need to decide to which k3s version we need to upgrade, therefore, check out the GitHub release page of k3s . The latest release of this writing was v1.19.4+k3s1 (30 November 2020). The upgrade plan will upgrade the k3s server node (called k3s-server in the plan) and the k3s worker nodes (called k3s-agent in the plan). For that reason we must first label our master node (in our case n1 ) if that was not yet done: $ kubectl get node --selector='node-role.kubernetes.io/master' NAME STATUS ROLES AGE VERSION n1 Ready master 117d v1.19.2+k3s1 Here we see that node n1 was already labelled 'master', however, if that was not yet the case we could realize this by: kubectl label node n1 node-role.kubernetes.io/master=true Edit the plan to update the k3s version (use latest stable release found on GitHub release page of k3s ): vi ./k3s-upgrade-plan.yaml Apply the plan: $ kubectl apply -f ./k3s-upgrade-plan.yaml plan.upgrade.cattle.io/k3s-server created plan.upgrade.cattle.io/k3s-agent created Check out if the Plans were added correctly: $ kubectl describe plans.upgrade.cattle.io -n system-upgrade Name: k3s-server Namespace: system-upgrade Labels: k3s-upgrade=server Annotations: <none> API Version: upgrade.cattle.io/v1 Kind: Plan Metadata: Creation Timestamp: 2020-11-30T10:53:24Z Generation: 1 Managed Fields: API Version: upgrade.cattle.io/v1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:labels: .: f:k3s-upgrade: f:spec: .: f:concurrency: f:cordon: f:nodeSelector: .: f:matchExpressions: f:serviceAccountName: f:upgrade: .: f:image: f:version: Manager: kubectl-client-side-apply Operation: Update Time: 2020-11-30T10:53:24Z API Version: upgrade.cattle.io/v1 Fields Type: FieldsV1 fieldsV1: f:status: .: f:conditions: f:latestHash: f:latestVersion: Manager: system-upgrade-controller Operation: Update Time: 2020-11-30T10:53:24Z Resource Version: 190476 Self Link: /apis/upgrade.cattle.io/v1/namespaces/system-upgrade/plans/k3s-server UID: d8bb36e7-7602-4e8f-bda7-b75947752eb1 Spec: Concurrency: 1 Cordon: true Node Selector: Match Expressions: Key: k3s-upgrade Operator: Exists Key: k3s-upgrade Operator: NotIn Values: disabled false Key: k3s.io/hostname Operator: Exists Key: k3os.io/mode Operator: DoesNotExist Key: node-role.kubernetes.io/master Operator: In Values: true Service Account Name: system-upgrade Upgrade: Image: rancher/k3s-upgrade Version: v1.19.4+k3s1 Status: Conditions: Last Update Time: 2020-11-30T10:53:24Z Reason: Version Status: True Type: LatestResolved Latest Hash: e50d232791db24fa7ce5039d6f9cf61238b420aacf2ecd32db7cfce3 Latest Version: v1.19.4-k3s1 Events: <none> Name: k3s-agent Namespace: system-upgrade Labels: k3s-upgrade=agent Annotations: <none> API Version: upgrade.cattle.io/v1 Kind: Plan Metadata: Creation Timestamp: 2020-11-30T10:53:24Z Generation: 1 Managed Fields: API Version: upgrade.cattle.io/v1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:labels: .: f:k3s-upgrade: f:spec: .: f:concurrency: f:drain: .: f:force: f:nodeSelector: .: f:matchExpressions: f:prepare: .: f:args: f:image: f:serviceAccountName: f:upgrade: .: f:image: f:version: Manager: kubectl-client-side-apply Operation: Update Time: 2020-11-30T10:53:24Z API Version: upgrade.cattle.io/v1 Fields Type: FieldsV1 fieldsV1: f:status: .: f:conditions: f:latestHash: f:latestVersion: Manager: system-upgrade-controller Operation: Update Time: 2020-11-30T10:53:24Z Resource Version: 190477 Self Link: /apis/upgrade.cattle.io/v1/namespaces/system-upgrade/plans/k3s-agent UID: db4567a7-f80c-44eb-b5dd-0505159ace87 Spec: Concurrency: 2 Drain: Force: true Node Selector: Match Expressions: Key: k3s-upgrade Operator: Exists Key: k3s-upgrade Operator: NotIn Values: disabled false Key: k3s.io/hostname Operator: Exists Key: k3os.io/mode Operator: DoesNotExist Key: node-role.kubernetes.io/master Operator: NotIn Values: true Prepare: Args: prepare k3s-server Image: rancher/k3s-upgrade Service Account Name: system-upgrade Upgrade: Image: rancher/k3s-upgrade Version: v1.19.4+k3s1 Status: Conditions: Last Update Time: 2020-11-30T10:53:24Z Reason: Version Status: True Type: LatestResolved Latest Hash: e50d232791db24fa7ce5039d6f9cf61238b420aacf2ecd32db7cfce3 Latest Version: v1.19.4-k3s1 Events: <none> And, for the magic to happen we just have to enable to k3s upgrade with command: $ kubectl label node --all k3s-upgrade=enabled node/n2 labeled node/n4 labeled node/n3 labeled node/n1 labeled node/n5 labeled To check if the k3s upgrades are kicking in watch with: $ kubectl get pods -n system-upgrade -w NAME READY STATUS RESTARTS AGE system-upgrade-controller-556df575dd-2qfrs 1/1 Running 0 109m apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Init:0/2 0 73s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Init:0/2 0 73s apply-k3s-server-on-n1-with-e50d232791db24fa7ce5039d6f9cf-2j42m 1/1 Running 0 73s apply-k3s-server-on-n1-with-e50d232791db24fa7ce5039d6f9cf-2j42m 0/1 Completed 0 75s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Init:1/2 0 89s apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Init:1/2 0 92s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Init:1/2 0 104s apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Init:1/2 0 107s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 PodInitializing 0 108s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 1/1 Running 0 110s apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 PodInitializing 0 110s apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 1/1 Running 0 112s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Completed 0 2m19s apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Completed 0 2m21s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Pending 0 0s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Pending 0 0s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Pending 0 0s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Pending 0 0s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Init:0/2 0 0s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Init:0/2 0 0s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Init:0/2 0 18s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Init:0/2 0 30s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Init:1/2 0 33s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Init:1/2 0 42s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Init:1/2 0 47s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Init:1/2 0 61s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 PodInitializing 0 70s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 PodInitializing 0 77s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 1/1 Running 0 79s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 1/1 Running 0 79s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Completed 0 107s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Completed 0 109s apply-k3s-server-on-n1-with-e50d232791db24fa7ce5039d6f9cf-2j42m 0/1 Terminating 0 16m apply-k3s-server-on-n1-with-e50d232791db24fa7ce5039d6f9cf-2j42m 0/1 Terminating 0 16m apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Terminating 0 17m apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Terminating 0 17m apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Terminating 0 17m apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Terminating 0 17m apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Terminating 0 16m apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Terminating 0 16m apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Terminating 0 16m apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Terminating 0 16m As you can see from the sequence first the k3s-server gets updated and thereafter, 2 worker nodes at a time as we requested in the plan description. After a couple of minutes we see: $ kubectl get nodes NAME STATUS ROLES AGE VERSION n1 Ready master 117d v1.19.4+k3s1 n2 Ready <none> 117d v1.19.4+k3s1 n4 Ready <none> 117d v1.19.4+k3s1 n3 Ready <none> 117d v1.19.4+k3s1 n5 Ready <none> 117d v1.19.4+k3s1 We believe it is better to disable the k3s upgrades once it is done with the command: $ kubectl label node --all --overwrite k3s-upgrade=disabled node/n5 labeled node/n1 labeled node/n2 labeled node/n4 labeled node/n3 labeled If we want to upgrade again just edit the plan [4] again with the correct version of k3s and replace the plan with the command: $ kubectl replace -f ./k3s-upgrade-plan.yaml To start the k3s version upgrade overwrite the label k3s-upgrade again with keyword enabled . References \u00b6 [1] Rancher Operational Advisory: Attention All Rancher K3s Customers, DB bug requires upgrade [2] Automate K3s Upgrades with System Upgrade Controller [3] CRD system-upgrade-controller [4] k3s Upgrade Plan [5] GitHub Sources of our k3s upgrade controller [6] GitHub rancher/system-upgrade-controller","title":"Raspberry Pi 4 Upgrading k3s software on your cluster"},{"location":"pi-stories6/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories6/#raspberry_pi_4_cluster_series_-_upgrading_k3s_software_on_your_cluster","text":"It is advisable to track the security vulnerabilities published by Rancher Labs around k3s. For example, in November 2020 a critical bug was detected in k3s (see [1]). Therefore, it is quite important to be able to update k3s without interrupting the k3s cluster, hence this procedure from Rancher Labs. $ kubectl get nodes NAME STATUS ROLES AGE VERSION n3 Ready <none> 117d v1.19.2+k3s1 n2 Ready <none> 117d v1.19.2+k3s1 n4 Ready <none> 117d v1.19.2+k3s1 n1 Ready master 117d v1.19.2+k3s1 n5 Ready <none> 117d v1.19.2+k3s1","title":"Raspberry Pi 4 cluster Series - Upgrading k3s software on your cluster"},{"location":"pi-stories6/#crd_installation","text":"We will follow the procedure described in [2] and are required first to install a kubernetes Custom Resource Definition (CRD) [3] followed by creating a Plan. To find the latest version of the system upgrade controller use the following command (sources of Rancher [6]): VERSION=$(curl -s \"https://api.github.com/repos/rancher/system-upgrade-controller/releases/latest\" | awk -F '\"' '/tag_name/{print $4}') echo $VERSION To download the CRD locally run the following command: $ wget https://raw.githubusercontent.com/rancher/system-upgrade-controller/${VERSION}/manifests/system-upgrade-controller.yaml Now get it applied by: $ kubectl apply -f ./system-upgrade-controller.yaml namespace/system-upgrade created serviceaccount/system-upgrade created clusterrolebinding.rbac.authorization.k8s.io/system-upgrade created configmap/default-controller-env created deployment.apps/system-upgrade-controller created $ kubectl get all -n system-upgrade NAME READY STATUS RESTARTS AGE pod/system-upgrade-controller-556df575dd-2qfrs 1/1 Running 0 17s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/system-upgrade-controller 1/1 1 1 17s NAME DESIRED CURRENT READY AGE replicaset.apps/system-upgrade-controller-556df575dd 1 1 1 18s","title":"CRD installation"},{"location":"pi-stories6/#how_to_upgrade_the_crd","text":"VERSION=$(curl -s \"https://api.github.com/repos/rancher/system-upgrade-controller/releases/latest\" | awk -F '\"' '/tag_name/{print $4}') echo $VERSION $ wget https://raw.githubusercontent.com/rancher/system-upgrade-controller/${VERSION}/manifests/system-upgrade-controller.yaml $ kubectl replace -f ./system-upgrade-controller.yaml","title":"How to Upgrade the CRD"},{"location":"pi-stories6/#making_a_k3s_upgrade_plan","text":"Before making a plan we need to decide to which k3s version we need to upgrade, therefore, check out the GitHub release page of k3s . The latest release of this writing was v1.19.4+k3s1 (30 November 2020). The upgrade plan will upgrade the k3s server node (called k3s-server in the plan) and the k3s worker nodes (called k3s-agent in the plan). For that reason we must first label our master node (in our case n1 ) if that was not yet done: $ kubectl get node --selector='node-role.kubernetes.io/master' NAME STATUS ROLES AGE VERSION n1 Ready master 117d v1.19.2+k3s1 Here we see that node n1 was already labelled 'master', however, if that was not yet the case we could realize this by: kubectl label node n1 node-role.kubernetes.io/master=true Edit the plan to update the k3s version (use latest stable release found on GitHub release page of k3s ): vi ./k3s-upgrade-plan.yaml Apply the plan: $ kubectl apply -f ./k3s-upgrade-plan.yaml plan.upgrade.cattle.io/k3s-server created plan.upgrade.cattle.io/k3s-agent created Check out if the Plans were added correctly: $ kubectl describe plans.upgrade.cattle.io -n system-upgrade Name: k3s-server Namespace: system-upgrade Labels: k3s-upgrade=server Annotations: <none> API Version: upgrade.cattle.io/v1 Kind: Plan Metadata: Creation Timestamp: 2020-11-30T10:53:24Z Generation: 1 Managed Fields: API Version: upgrade.cattle.io/v1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:labels: .: f:k3s-upgrade: f:spec: .: f:concurrency: f:cordon: f:nodeSelector: .: f:matchExpressions: f:serviceAccountName: f:upgrade: .: f:image: f:version: Manager: kubectl-client-side-apply Operation: Update Time: 2020-11-30T10:53:24Z API Version: upgrade.cattle.io/v1 Fields Type: FieldsV1 fieldsV1: f:status: .: f:conditions: f:latestHash: f:latestVersion: Manager: system-upgrade-controller Operation: Update Time: 2020-11-30T10:53:24Z Resource Version: 190476 Self Link: /apis/upgrade.cattle.io/v1/namespaces/system-upgrade/plans/k3s-server UID: d8bb36e7-7602-4e8f-bda7-b75947752eb1 Spec: Concurrency: 1 Cordon: true Node Selector: Match Expressions: Key: k3s-upgrade Operator: Exists Key: k3s-upgrade Operator: NotIn Values: disabled false Key: k3s.io/hostname Operator: Exists Key: k3os.io/mode Operator: DoesNotExist Key: node-role.kubernetes.io/master Operator: In Values: true Service Account Name: system-upgrade Upgrade: Image: rancher/k3s-upgrade Version: v1.19.4+k3s1 Status: Conditions: Last Update Time: 2020-11-30T10:53:24Z Reason: Version Status: True Type: LatestResolved Latest Hash: e50d232791db24fa7ce5039d6f9cf61238b420aacf2ecd32db7cfce3 Latest Version: v1.19.4-k3s1 Events: <none> Name: k3s-agent Namespace: system-upgrade Labels: k3s-upgrade=agent Annotations: <none> API Version: upgrade.cattle.io/v1 Kind: Plan Metadata: Creation Timestamp: 2020-11-30T10:53:24Z Generation: 1 Managed Fields: API Version: upgrade.cattle.io/v1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:labels: .: f:k3s-upgrade: f:spec: .: f:concurrency: f:drain: .: f:force: f:nodeSelector: .: f:matchExpressions: f:prepare: .: f:args: f:image: f:serviceAccountName: f:upgrade: .: f:image: f:version: Manager: kubectl-client-side-apply Operation: Update Time: 2020-11-30T10:53:24Z API Version: upgrade.cattle.io/v1 Fields Type: FieldsV1 fieldsV1: f:status: .: f:conditions: f:latestHash: f:latestVersion: Manager: system-upgrade-controller Operation: Update Time: 2020-11-30T10:53:24Z Resource Version: 190477 Self Link: /apis/upgrade.cattle.io/v1/namespaces/system-upgrade/plans/k3s-agent UID: db4567a7-f80c-44eb-b5dd-0505159ace87 Spec: Concurrency: 2 Drain: Force: true Node Selector: Match Expressions: Key: k3s-upgrade Operator: Exists Key: k3s-upgrade Operator: NotIn Values: disabled false Key: k3s.io/hostname Operator: Exists Key: k3os.io/mode Operator: DoesNotExist Key: node-role.kubernetes.io/master Operator: NotIn Values: true Prepare: Args: prepare k3s-server Image: rancher/k3s-upgrade Service Account Name: system-upgrade Upgrade: Image: rancher/k3s-upgrade Version: v1.19.4+k3s1 Status: Conditions: Last Update Time: 2020-11-30T10:53:24Z Reason: Version Status: True Type: LatestResolved Latest Hash: e50d232791db24fa7ce5039d6f9cf61238b420aacf2ecd32db7cfce3 Latest Version: v1.19.4-k3s1 Events: <none> And, for the magic to happen we just have to enable to k3s upgrade with command: $ kubectl label node --all k3s-upgrade=enabled node/n2 labeled node/n4 labeled node/n3 labeled node/n1 labeled node/n5 labeled To check if the k3s upgrades are kicking in watch with: $ kubectl get pods -n system-upgrade -w NAME READY STATUS RESTARTS AGE system-upgrade-controller-556df575dd-2qfrs 1/1 Running 0 109m apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Init:0/2 0 73s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Init:0/2 0 73s apply-k3s-server-on-n1-with-e50d232791db24fa7ce5039d6f9cf-2j42m 1/1 Running 0 73s apply-k3s-server-on-n1-with-e50d232791db24fa7ce5039d6f9cf-2j42m 0/1 Completed 0 75s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Init:1/2 0 89s apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Init:1/2 0 92s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Init:1/2 0 104s apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Init:1/2 0 107s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 PodInitializing 0 108s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 1/1 Running 0 110s apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 PodInitializing 0 110s apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 1/1 Running 0 112s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Completed 0 2m19s apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Completed 0 2m21s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Pending 0 0s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Pending 0 0s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Pending 0 0s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Pending 0 0s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Init:0/2 0 0s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Init:0/2 0 0s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Init:0/2 0 18s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Init:0/2 0 30s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Init:1/2 0 33s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Init:1/2 0 42s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Init:1/2 0 47s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Init:1/2 0 61s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 PodInitializing 0 70s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 PodInitializing 0 77s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 1/1 Running 0 79s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 1/1 Running 0 79s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Completed 0 107s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Completed 0 109s apply-k3s-server-on-n1-with-e50d232791db24fa7ce5039d6f9cf-2j42m 0/1 Terminating 0 16m apply-k3s-server-on-n1-with-e50d232791db24fa7ce5039d6f9cf-2j42m 0/1 Terminating 0 16m apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Terminating 0 17m apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Terminating 0 17m apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Terminating 0 17m apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Terminating 0 17m apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Terminating 0 16m apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Terminating 0 16m apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Terminating 0 16m apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Terminating 0 16m As you can see from the sequence first the k3s-server gets updated and thereafter, 2 worker nodes at a time as we requested in the plan description. After a couple of minutes we see: $ kubectl get nodes NAME STATUS ROLES AGE VERSION n1 Ready master 117d v1.19.4+k3s1 n2 Ready <none> 117d v1.19.4+k3s1 n4 Ready <none> 117d v1.19.4+k3s1 n3 Ready <none> 117d v1.19.4+k3s1 n5 Ready <none> 117d v1.19.4+k3s1 We believe it is better to disable the k3s upgrades once it is done with the command: $ kubectl label node --all --overwrite k3s-upgrade=disabled node/n5 labeled node/n1 labeled node/n2 labeled node/n4 labeled node/n3 labeled If we want to upgrade again just edit the plan [4] again with the correct version of k3s and replace the plan with the command: $ kubectl replace -f ./k3s-upgrade-plan.yaml To start the k3s version upgrade overwrite the label k3s-upgrade again with keyword enabled .","title":"Making a k3s upgrade plan"},{"location":"pi-stories6/#references","text":"[1] Rancher Operational Advisory: Attention All Rancher K3s Customers, DB bug requires upgrade [2] Automate K3s Upgrades with System Upgrade Controller [3] CRD system-upgrade-controller [4] k3s Upgrade Plan [5] GitHub Sources of our k3s upgrade controller [6] GitHub rancher/system-upgrade-controller","title":"References"},{"location":"pi-stories7/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - YAML everywhere - what about correctness? \u00b6 When you dive deep into Kubernetes you will notice you cannot go around YAML [1] language. You hate it or love it, however, you better get used to it as it is part of the core of kubernetes. Writing YAML code from scratch is not a real pleasure, therefore, having a linter would be nice to avoid the low hanging fruit errors. We found a kubernetes linter tool called \" KubeLinter \" written in the Go language, however, there is no binary available for the Raspberry Pi4 architecture aarch64 on the release page of kube-linter . Therefore, we decided to build it ourselves from the sources. Installing the Go Language binaries \u00b6 On our node n1 we installed the Go Language with the commands: $ sudo apt install golang-go $ sudo apt install make Compiling the KubeLinter code \u00b6 Download the code from the Kube-Linter Github page and according the \" Building from source \" documentation it would be as easy as make build to generate the binary: $ git clone git@github.com:gdha/kube-linter.git $ cd kube-linter $ make build ... all modules verified + /home/gdha/projects/kube-linter/.gobin/packr go install github.com/gobuffalo/packr/packr packr Compiling Go source in ./cmd/kube-linter to bin/darwin/kube-linter # golang.stackrox.io/kube-linter/cmd/kube-linter /usr/lib/go-1.13/pkg/tool/linux_arm64/link: running gcc failed: exit status 1 /usr/bin/ld: unrecognized -a option `gezero_size' collect2: error: ld returned 1 exit status make: *** [Makefile:100: build] Error 2 As usual it didn't work as expected. Mind the bin/darwin/kube-linter compile line mentions darwin and that is not our architecture for RPI4. We figured out that by tweaking the Makefile [2] we could build the kube-linter to an executable. Installing kube-linter as /usr/local/bin/kube-linter \u00b6 We update the Makefile with an install rule for our binary so that we do not have to add the PATH to out .bashrc file. Just run make install to copy the compiled binary to /usr/local/bin/kube-linter . To verify it works try the command: $ kube-linter version 0.1.4-10-g5c30a676d3-dirty Testing it out on a real example \u00b6 In a previous post we discussed and explained the rolling upgrade of the k3s kubernetes software on our pods. [3] Therefore, what kind of information will the kube-linter produce on these YAML files? $ kube-linter lint ../k3s-upgrade-controller/ ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" does not have a read-only root file system (check: no-read-only-root-fs, remediation: Set readOnlyRootFilesystem to true in your container's securityContext.) ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" is not set to runAsNonRoot (check: run-as-non-root, remediation: Set runAsUser to a non-zero number, and runAsNonRoot to true, in your pod or container securityContext. See https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for more details.) ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" has cpu request 0 (check: unset-cpu-requirements, remediation: Set your container's CPU requests and limits depending on its requirements. See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for more details.) ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" has cpu limit 0 (check: unset-cpu-requirements, remediation: Set your container's CPU requests and limits depending on its requirements. See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for more details.) ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" has memory request 0 (check: unset-memory-requirements, remediation: Set your container's memory requests and limits depending on its requirements. See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for more details.) ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" has memory limit 0 (check: unset-memory-requirements, remediation: Set your container's memory requests and limits depending on its requirements. See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for more details.) Error: found 6 lint errors The recommendations are perhaps not really perfect for this example as we do need a writable file system to be able to perform an update and root permissions will be required as well. However, the test itself was successful as it produces a meaningful output. For a more profound usage of kube-linter see the \" KubeLinter documentation \" [4]. References \u00b6 [1] YAML Ain't Markup Language [2] Kube-Linter GitHub fork [3] CRD system-upgrade-controller [4] KubeLinter Documentation","title":"Raspberry Pi 4 YAML everywhere -what about correctness?"},{"location":"pi-stories7/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories7/#raspberry_pi_4_cluster_series_-_yaml_everywhere_-_what_about_correctness","text":"When you dive deep into Kubernetes you will notice you cannot go around YAML [1] language. You hate it or love it, however, you better get used to it as it is part of the core of kubernetes. Writing YAML code from scratch is not a real pleasure, therefore, having a linter would be nice to avoid the low hanging fruit errors. We found a kubernetes linter tool called \" KubeLinter \" written in the Go language, however, there is no binary available for the Raspberry Pi4 architecture aarch64 on the release page of kube-linter . Therefore, we decided to build it ourselves from the sources.","title":"Raspberry Pi 4 cluster Series - YAML everywhere - what about correctness?"},{"location":"pi-stories7/#installing_the_go_language_binaries","text":"On our node n1 we installed the Go Language with the commands: $ sudo apt install golang-go $ sudo apt install make","title":"Installing the Go Language binaries"},{"location":"pi-stories7/#compiling_the_kubelinter_code","text":"Download the code from the Kube-Linter Github page and according the \" Building from source \" documentation it would be as easy as make build to generate the binary: $ git clone git@github.com:gdha/kube-linter.git $ cd kube-linter $ make build ... all modules verified + /home/gdha/projects/kube-linter/.gobin/packr go install github.com/gobuffalo/packr/packr packr Compiling Go source in ./cmd/kube-linter to bin/darwin/kube-linter # golang.stackrox.io/kube-linter/cmd/kube-linter /usr/lib/go-1.13/pkg/tool/linux_arm64/link: running gcc failed: exit status 1 /usr/bin/ld: unrecognized -a option `gezero_size' collect2: error: ld returned 1 exit status make: *** [Makefile:100: build] Error 2 As usual it didn't work as expected. Mind the bin/darwin/kube-linter compile line mentions darwin and that is not our architecture for RPI4. We figured out that by tweaking the Makefile [2] we could build the kube-linter to an executable.","title":"Compiling the KubeLinter code"},{"location":"pi-stories7/#installing_kube-linter_as_usrlocalbinkube-linter","text":"We update the Makefile with an install rule for our binary so that we do not have to add the PATH to out .bashrc file. Just run make install to copy the compiled binary to /usr/local/bin/kube-linter . To verify it works try the command: $ kube-linter version 0.1.4-10-g5c30a676d3-dirty","title":"Installing kube-linter as /usr/local/bin/kube-linter"},{"location":"pi-stories7/#testing_it_out_on_a_real_example","text":"In a previous post we discussed and explained the rolling upgrade of the k3s kubernetes software on our pods. [3] Therefore, what kind of information will the kube-linter produce on these YAML files? $ kube-linter lint ../k3s-upgrade-controller/ ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" does not have a read-only root file system (check: no-read-only-root-fs, remediation: Set readOnlyRootFilesystem to true in your container's securityContext.) ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" is not set to runAsNonRoot (check: run-as-non-root, remediation: Set runAsUser to a non-zero number, and runAsNonRoot to true, in your pod or container securityContext. See https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for more details.) ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" has cpu request 0 (check: unset-cpu-requirements, remediation: Set your container's CPU requests and limits depending on its requirements. See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for more details.) ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" has cpu limit 0 (check: unset-cpu-requirements, remediation: Set your container's CPU requests and limits depending on its requirements. See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for more details.) ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" has memory request 0 (check: unset-memory-requirements, remediation: Set your container's memory requests and limits depending on its requirements. See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for more details.) ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" has memory limit 0 (check: unset-memory-requirements, remediation: Set your container's memory requests and limits depending on its requirements. See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for more details.) Error: found 6 lint errors The recommendations are perhaps not really perfect for this example as we do need a writable file system to be able to perform an update and root permissions will be required as well. However, the test itself was successful as it produces a meaningful output. For a more profound usage of kube-linter see the \" KubeLinter documentation \" [4].","title":"Testing it out on a real example"},{"location":"pi-stories7/#references","text":"[1] YAML Ain't Markup Language [2] Kube-Linter GitHub fork [3] CRD system-upgrade-controller [4] KubeLinter Documentation","title":"References"},{"location":"pi-stories8/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Keep your Operating System updated \u00b6 When you login on one of your nodes you often see a message like the following: Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 5.4.0-1026-raspi aarch64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage System information as of Thu 28 Jan 2021 12:05:56 PM CET System load: 0.55 Usage of /: 18.3% of 28.40GB Memory usage: 14% Swap usage: 0% Temperature: 34.6 C Processes: 157 Users logged in: 1 IPv4 address for cni0: 10.42.1.1 IPv4 address for docker0: 172.17.0.1 IPv4 address for eth0: 192.168.0.205 * Introducing self-healing high availability clusters in MicroK8s. Simple, hardened, Kubernetes for production, from RaspberryPi to DC. https://microk8s.io/high-availability 62 updates can be installed immediately. 0 of these updates are security updates. To see these additional updates run: apt list --upgradable Last login: Thu Jan 28 12:01:54 2021 from 192.168.0.41 Pay attention to the amount of packages ready to update: See line \"62 updates can be installed immediately\" Doing that for a bunch of systems is tiring/boring so let get this done via ansible. First we need to download package information from all configured sources. $ ansible pi -m shell -b -a \"apt update\" n1 | SUCCESS | rc=0 >> Hit:1 http://ports.ubuntu.com/ubuntu-ports focal InRelease Get:2 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease [114 kB] Get:3 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease [101 kB] Get:4 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease [109 kB] Fetched 324 kB in 2s (187 kB/s) Reading package lists... Building dependency tree... Reading state information... 49 packages can be upgraded. Run 'apt list --upgradable' to see them. WARNING: apt does not have a stable CLI interface. Use with caution in scripts. ... n5 | SUCCESS | rc=0 >> Hit:1 http://ports.ubuntu.com/ubuntu-ports focal InRelease Get:2 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease [114 kB] Get:3 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease [101 kB] Get:4 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease [109 kB] Fetched 324 kB in 2s (196 kB/s) Reading package lists... Building dependency tree... Reading state information... 58 packages can be upgraded. Run 'apt list --upgradable' to see them. WARNING: apt does not have a stable CLI interface. Use with caution in scripts And, finally, to automate the installation of the packages: $ ansible pi -m shell -b -a \"apt --yes upgrade\"","title":"Raspberry Pi 4 Keep your Operating System updated"},{"location":"pi-stories8/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories8/#raspberry_pi_4_cluster_series_-_keep_your_operating_system_updated","text":"When you login on one of your nodes you often see a message like the following: Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 5.4.0-1026-raspi aarch64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage System information as of Thu 28 Jan 2021 12:05:56 PM CET System load: 0.55 Usage of /: 18.3% of 28.40GB Memory usage: 14% Swap usage: 0% Temperature: 34.6 C Processes: 157 Users logged in: 1 IPv4 address for cni0: 10.42.1.1 IPv4 address for docker0: 172.17.0.1 IPv4 address for eth0: 192.168.0.205 * Introducing self-healing high availability clusters in MicroK8s. Simple, hardened, Kubernetes for production, from RaspberryPi to DC. https://microk8s.io/high-availability 62 updates can be installed immediately. 0 of these updates are security updates. To see these additional updates run: apt list --upgradable Last login: Thu Jan 28 12:01:54 2021 from 192.168.0.41 Pay attention to the amount of packages ready to update: See line \"62 updates can be installed immediately\" Doing that for a bunch of systems is tiring/boring so let get this done via ansible. First we need to download package information from all configured sources. $ ansible pi -m shell -b -a \"apt update\" n1 | SUCCESS | rc=0 >> Hit:1 http://ports.ubuntu.com/ubuntu-ports focal InRelease Get:2 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease [114 kB] Get:3 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease [101 kB] Get:4 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease [109 kB] Fetched 324 kB in 2s (187 kB/s) Reading package lists... Building dependency tree... Reading state information... 49 packages can be upgraded. Run 'apt list --upgradable' to see them. WARNING: apt does not have a stable CLI interface. Use with caution in scripts. ... n5 | SUCCESS | rc=0 >> Hit:1 http://ports.ubuntu.com/ubuntu-ports focal InRelease Get:2 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease [114 kB] Get:3 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease [101 kB] Get:4 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease [109 kB] Fetched 324 kB in 2s (196 kB/s) Reading package lists... Building dependency tree... Reading state information... 58 packages can be upgraded. Run 'apt list --upgradable' to see them. WARNING: apt does not have a stable CLI interface. Use with caution in scripts And, finally, to automate the installation of the packages: $ ansible pi -m shell -b -a \"apt --yes upgrade\"","title":"Raspberry Pi 4 cluster Series - Keep your Operating System updated"},{"location":"pi-stories9/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Installation of Longhorn \u00b6 Prepare our external USB block devices \u00b6 On all our pi systems we added an USB block device of the same size and we are sure that they all put in the same USB port so that we are sure the all have the same block device name, e.g. /dev/sda We create an ansible playbook to prepare the USB block devices and have it mounted on /app/longhorn on each node. E.g. $ df /app/longhorn/ Filesystem 1K-blocks Used Available Use% Mounted on /dev/sda1 117715864 61472 111631688 1% /app/longhorn Using helm to perform the installation \u00b6 If we want to use helm to performt the installation of longhorn we first need to install it as it isn't standard avaibale on these systems. Getting helm from URL https://helm.sh/docs/intro/install/ . $ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 $ chmod 700 get_helm.sh $ ./get_helm.sh Helm v3.11.0 is available. Changing from version v3.7.1. Downloading https://get.helm.sh/helm-v3.11.0-linux-arm64.tar.gz Verifying checksum... Done. Preparing to install helm into /usr/local/bin helm installed into /usr/local/bin/helm Alright, now we have the helm executable available on our local system (e.g. node n1). We can now download the helm longhorn chart repository: $ helm repo add longhorn https://charts.longhorn.io \"longhorn\" has been added to your repositories $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"longhorn\" chart repository Update Complete. \u2388Happy Helming!\u2388 We need to be careful when we run the helm installer as we only want longhorn to use the USB block devices mounted at /app/longhorn and it should not be using the default location /var/lib/longhorn (as this might fill up the root partition). Information on how we can actually do this can be found at \"Adding Node Tags to New Nodes\" : $ helm install longhorn longhorn/longhorn --namespace longhorn-system --set defaultSettings.defaultDataPath=\"/app/longhorn/\" --create-namespace NAME: longhorn LAST DEPLOYED: Tue Jan 24 14:38:41 2023 NAMESPACE: longhorn-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Longhorn is now installed on the cluster! Please wait a few minutes for other Longhorn components such as CSI deployments, Engine Images, and Instance Managers to be initialized. Visit our documentation at https://longhorn.io/docs/ Prepare the longhorn-ingress (required for UI) \u00b6 In-depth information about accessing the longhorn UI can be found at longhorn-ingress . In short this is the procedure we followed: $ USER=gdha; PASSWORD=*******; echo \"${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})\" >> auth $ cat auth gdha:$apr1$XXXXXXXXXXXXXXXXXXXXXXXX $ kubectl -n longhorn-system create secret generic basic-auth --from-file=auth secret/basic-auth created $ kubectl -n longhorn-system get secret basic-auth -o yaml apiVersion: v1 data: auth: Z2RoYTokYXByMSRSTW11bU5oRSQyd3ZCMzNFM0hyLjY4aGZvL2xkVGsuCg== kind: Secret metadata: creationTimestamp: \"2023-01-24T13:40:16Z\" name: basic-auth namespace: longhorn-system resourceVersion: \"13336\" uid: eeb2bd1f-9cae-4460-916d-b26c7eb97b7e type: Opaque Then we paste the following set of yaml command lines into kubectl to create the longhorn-ingress: $ cat longhorn-ingress.yaml apiVersion: v1 items: - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required ' nginx.ingress.kubernetes.io/auth-secret: basic-auth nginx.ingress.kubernetes.io/auth-type: basic nginx.ingress.kubernetes.io/proxy-body-size: 10000m nginx.ingress.kubernetes.io/ssl-redirect: \"false\" creationTimestamp: \"2022-02-01T11:41:47Z\" generation: 4 name: longhorn-ingress namespace: longhorn-system resourceVersion: \"3312028\" uid: bb838795-97d8-44e7-a3f1-4f74dd0854ee spec: rules: - http: paths: - backend: service: name: longhorn-frontend port: number: 80 path: / pathType: ImplementationSpecific status: loadBalancer: {} kind: List metadata: resourceVersion: \"\" $ kubectl create -f longhorn-ingress.yaml $ kubectl describe ingress -n longhorn-system Name: longhorn-ingress Labels: <none> Namespace: longhorn-system Address: Ingress Class: traefik Default backend: <default> Rules: Host Path Backends ---- ---- -------- * / longhorn-frontend:80 (10.42.3.5:8000,10.42.4.6:8000) Annotations: nginx.ingress.kubernetes.io/auth-realm: Authentication Required nginx.ingress.kubernetes.io/auth-secret: basic-auth nginx.ingress.kubernetes.io/auth-type: basic nginx.ingress.kubernetes.io/proxy-body-size: 10000m nginx.ingress.kubernetes.io/ssl-redirect: false Events: <none> Use the longhorn UI \u00b6 As we have setup metallb with traefik [5] the longhorn-ingress is running is now available at IP address 192.168.0.230 and to test the connectivity we can use curl : $ curl -v 192.168.0.230:80 * Trying 192.168.0.230:80... * TCP_NODELAY set * Connected to 192.168.0.230 (192.168.0.230) port 80 (#0) > GET / HTTP/1.1 > Host: 192.168.0.230 > User-Agent: curl/7.68.0 > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Cache-Control: max-age=0 < Content-Type: text/html < Date: Tue, 24 Jan 2023 16:11:19 GMT < Etag: W/\"63adbe68-401\" < Last-Modified: Thu, 29 Dec 2022 16:20:56 GMT < Server: nginx/1.21.5 < Vary: Accept-Encoding < Transfer-Encoding: chunked < <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"> <!--[if lte IE 10]> <script src=\"https://as.alipayobjects.com/g/component/??console-polyfill/0.2.2/index.js,media-match/2.0.2/media.match.min.js\"></script> <![endif]--> <style> ::-webkit-scrollbar { width: 10px; height: 1px; } ::-webkit-scrollbar-thumb { border-radius: 10px; -webkit-box-shadow: inset 0 0 5px rgba(0,0,0,0.1); background: #535353; } </style> <link href=\"./styles.css?e0c09a3f2aae9c069d0c\" rel=\"stylesheet\"></head> <body> <div id=\"root\"></div> <script type=\"text/javascript\" src=\"./runtime~main.6d7bda24.js?e0c09a3f2aae9c069d0c\"></script><script type=\"text/javascript\" src=\"./styles.985bf912.async.js?e0c09a3f2aae9c069d0c\"></script><script type=\"text/javascript\" src=\"./main.c5723e73.async.js?e0c09a3f2aae9c069d0c\"></script></body> </html> * Connection #0 to host 192.168.0.230 left intact However, we a browser pointing to http://192.168.0.230/#/dashboard we get a better overview: Or, when selecting the node tab: And, the details of one node: Upgrading longhorn version with the help of helm \u00b6 We were running version 1.4.0 of longhorn and we wanted to upgrade to version 1.5.1 . Here are the steps how we did this: $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"metallb\" chart repository ...Successfully got an update from the \"longhorn\" chart repository ...Successfully got an update from the \"kiwigrid\" chart repository ...Successfully got an update from the \"traefik\" chart repository ...Successfully got an update from the \"grafana\" chart repository Update Complete. \u2388Happy Helming!\u2388 $ helm search repo longhorn NAME CHART VERSION APP VERSION DESCRIPTION longhorn/longhorn 1.5.1 v1.5.1 Longhorn is a distributed block storage system ... $ helm upgrade longhorn longhorn/longhorn --namespace longhorn-system --version 1.5.1 Release \"longhorn\" has been upgraded. Happy Helming! NAME: longhorn LAST DEPLOYED: Fri Oct 6 08:31:36 2023 NAMESPACE: longhorn-system STATUS: deployed REVISION: 2 TEST SUITE: None NOTES: Longhorn is now installed on the cluster! Please wait a few minutes for other Longhorn components such as CSI deployments, Engine Images, and Instance Managers to be initialized. Visit our documentation at https://longhorn.io/docs/ $ kubectl get pods -n longhorn-system -w It will take a couple of minutes to upgrade the longhorn manager and then you can refresh the longhorn GUI window on your browser and you will see in the left down corner the correct (new) version number. However, please note that each volume will show on the left side an upper arrow indicating that there is an action waiting: Select a volume and on the right side you can pick the \"upgrade engine\" option; select the available longhorn engine version and click ok. Do this for each volume you have: References \u00b6 [1] Ansible playbook to prepare USB devices [2] Longhorn [3] Adding Node Tags to New Nodes [4] Accessing Loghorn through UI [5] Replacing internal traefik with Metallb Edit history \u00b6 update for longhorn version 2.4.0 (24/Jan/2023)","title":"Raspberry Pi 4 Installation of Longhorn"},{"location":"pi-stories9/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories9/#raspberry_pi_4_cluster_series_-_installation_of_longhorn","text":"","title":"Raspberry Pi 4 cluster Series - Installation of Longhorn"},{"location":"pi-stories9/#prepare_our_external_usb_block_devices","text":"On all our pi systems we added an USB block device of the same size and we are sure that they all put in the same USB port so that we are sure the all have the same block device name, e.g. /dev/sda We create an ansible playbook to prepare the USB block devices and have it mounted on /app/longhorn on each node. E.g. $ df /app/longhorn/ Filesystem 1K-blocks Used Available Use% Mounted on /dev/sda1 117715864 61472 111631688 1% /app/longhorn","title":"Prepare our external USB block devices"},{"location":"pi-stories9/#using_helm_to_perform_the_installation","text":"If we want to use helm to performt the installation of longhorn we first need to install it as it isn't standard avaibale on these systems. Getting helm from URL https://helm.sh/docs/intro/install/ . $ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 $ chmod 700 get_helm.sh $ ./get_helm.sh Helm v3.11.0 is available. Changing from version v3.7.1. Downloading https://get.helm.sh/helm-v3.11.0-linux-arm64.tar.gz Verifying checksum... Done. Preparing to install helm into /usr/local/bin helm installed into /usr/local/bin/helm Alright, now we have the helm executable available on our local system (e.g. node n1). We can now download the helm longhorn chart repository: $ helm repo add longhorn https://charts.longhorn.io \"longhorn\" has been added to your repositories $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"longhorn\" chart repository Update Complete. \u2388Happy Helming!\u2388 We need to be careful when we run the helm installer as we only want longhorn to use the USB block devices mounted at /app/longhorn and it should not be using the default location /var/lib/longhorn (as this might fill up the root partition). Information on how we can actually do this can be found at \"Adding Node Tags to New Nodes\" : $ helm install longhorn longhorn/longhorn --namespace longhorn-system --set defaultSettings.defaultDataPath=\"/app/longhorn/\" --create-namespace NAME: longhorn LAST DEPLOYED: Tue Jan 24 14:38:41 2023 NAMESPACE: longhorn-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Longhorn is now installed on the cluster! Please wait a few minutes for other Longhorn components such as CSI deployments, Engine Images, and Instance Managers to be initialized. Visit our documentation at https://longhorn.io/docs/","title":"Using helm to perform the installation"},{"location":"pi-stories9/#prepare_the_longhorn-ingress_required_for_ui","text":"In-depth information about accessing the longhorn UI can be found at longhorn-ingress . In short this is the procedure we followed: $ USER=gdha; PASSWORD=*******; echo \"${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})\" >> auth $ cat auth gdha:$apr1$XXXXXXXXXXXXXXXXXXXXXXXX $ kubectl -n longhorn-system create secret generic basic-auth --from-file=auth secret/basic-auth created $ kubectl -n longhorn-system get secret basic-auth -o yaml apiVersion: v1 data: auth: Z2RoYTokYXByMSRSTW11bU5oRSQyd3ZCMzNFM0hyLjY4aGZvL2xkVGsuCg== kind: Secret metadata: creationTimestamp: \"2023-01-24T13:40:16Z\" name: basic-auth namespace: longhorn-system resourceVersion: \"13336\" uid: eeb2bd1f-9cae-4460-916d-b26c7eb97b7e type: Opaque Then we paste the following set of yaml command lines into kubectl to create the longhorn-ingress: $ cat longhorn-ingress.yaml apiVersion: v1 items: - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required ' nginx.ingress.kubernetes.io/auth-secret: basic-auth nginx.ingress.kubernetes.io/auth-type: basic nginx.ingress.kubernetes.io/proxy-body-size: 10000m nginx.ingress.kubernetes.io/ssl-redirect: \"false\" creationTimestamp: \"2022-02-01T11:41:47Z\" generation: 4 name: longhorn-ingress namespace: longhorn-system resourceVersion: \"3312028\" uid: bb838795-97d8-44e7-a3f1-4f74dd0854ee spec: rules: - http: paths: - backend: service: name: longhorn-frontend port: number: 80 path: / pathType: ImplementationSpecific status: loadBalancer: {} kind: List metadata: resourceVersion: \"\" $ kubectl create -f longhorn-ingress.yaml $ kubectl describe ingress -n longhorn-system Name: longhorn-ingress Labels: <none> Namespace: longhorn-system Address: Ingress Class: traefik Default backend: <default> Rules: Host Path Backends ---- ---- -------- * / longhorn-frontend:80 (10.42.3.5:8000,10.42.4.6:8000) Annotations: nginx.ingress.kubernetes.io/auth-realm: Authentication Required nginx.ingress.kubernetes.io/auth-secret: basic-auth nginx.ingress.kubernetes.io/auth-type: basic nginx.ingress.kubernetes.io/proxy-body-size: 10000m nginx.ingress.kubernetes.io/ssl-redirect: false Events: <none>","title":"Prepare the longhorn-ingress (required for UI)"},{"location":"pi-stories9/#use_the_longhorn_ui","text":"As we have setup metallb with traefik [5] the longhorn-ingress is running is now available at IP address 192.168.0.230 and to test the connectivity we can use curl : $ curl -v 192.168.0.230:80 * Trying 192.168.0.230:80... * TCP_NODELAY set * Connected to 192.168.0.230 (192.168.0.230) port 80 (#0) > GET / HTTP/1.1 > Host: 192.168.0.230 > User-Agent: curl/7.68.0 > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Cache-Control: max-age=0 < Content-Type: text/html < Date: Tue, 24 Jan 2023 16:11:19 GMT < Etag: W/\"63adbe68-401\" < Last-Modified: Thu, 29 Dec 2022 16:20:56 GMT < Server: nginx/1.21.5 < Vary: Accept-Encoding < Transfer-Encoding: chunked < <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"> <!--[if lte IE 10]> <script src=\"https://as.alipayobjects.com/g/component/??console-polyfill/0.2.2/index.js,media-match/2.0.2/media.match.min.js\"></script> <![endif]--> <style> ::-webkit-scrollbar { width: 10px; height: 1px; } ::-webkit-scrollbar-thumb { border-radius: 10px; -webkit-box-shadow: inset 0 0 5px rgba(0,0,0,0.1); background: #535353; } </style> <link href=\"./styles.css?e0c09a3f2aae9c069d0c\" rel=\"stylesheet\"></head> <body> <div id=\"root\"></div> <script type=\"text/javascript\" src=\"./runtime~main.6d7bda24.js?e0c09a3f2aae9c069d0c\"></script><script type=\"text/javascript\" src=\"./styles.985bf912.async.js?e0c09a3f2aae9c069d0c\"></script><script type=\"text/javascript\" src=\"./main.c5723e73.async.js?e0c09a3f2aae9c069d0c\"></script></body> </html> * Connection #0 to host 192.168.0.230 left intact However, we a browser pointing to http://192.168.0.230/#/dashboard we get a better overview: Or, when selecting the node tab: And, the details of one node:","title":"Use the longhorn UI"},{"location":"pi-stories9/#upgrading_longhorn_version_with_the_help_of_helm","text":"We were running version 1.4.0 of longhorn and we wanted to upgrade to version 1.5.1 . Here are the steps how we did this: $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"metallb\" chart repository ...Successfully got an update from the \"longhorn\" chart repository ...Successfully got an update from the \"kiwigrid\" chart repository ...Successfully got an update from the \"traefik\" chart repository ...Successfully got an update from the \"grafana\" chart repository Update Complete. \u2388Happy Helming!\u2388 $ helm search repo longhorn NAME CHART VERSION APP VERSION DESCRIPTION longhorn/longhorn 1.5.1 v1.5.1 Longhorn is a distributed block storage system ... $ helm upgrade longhorn longhorn/longhorn --namespace longhorn-system --version 1.5.1 Release \"longhorn\" has been upgraded. Happy Helming! NAME: longhorn LAST DEPLOYED: Fri Oct 6 08:31:36 2023 NAMESPACE: longhorn-system STATUS: deployed REVISION: 2 TEST SUITE: None NOTES: Longhorn is now installed on the cluster! Please wait a few minutes for other Longhorn components such as CSI deployments, Engine Images, and Instance Managers to be initialized. Visit our documentation at https://longhorn.io/docs/ $ kubectl get pods -n longhorn-system -w It will take a couple of minutes to upgrade the longhorn manager and then you can refresh the longhorn GUI window on your browser and you will see in the left down corner the correct (new) version number. However, please note that each volume will show on the left side an upper arrow indicating that there is an action waiting: Select a volume and on the right side you can pick the \"upgrade engine\" option; select the available longhorn engine version and click ok. Do this for each volume you have:","title":"Upgrading longhorn version with the help of helm"},{"location":"pi-stories9/#references","text":"[1] Ansible playbook to prepare USB devices [2] Longhorn [3] Adding Node Tags to New Nodes [4] Accessing Loghorn through UI [5] Replacing internal traefik with Metallb","title":"References"},{"location":"pi-stories9/#edit_history","text":"update for longhorn version 2.4.0 (24/Jan/2023)","title":"Edit history"}]}