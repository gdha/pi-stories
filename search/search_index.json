{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Raspberry PI k3s stories \u00b6 You can use the editor on GitHub to maintain and preview the content for your website in Markdown files. Whenever you commit to this repository, GitHub Pages will run Jekyll to rebuild the pages in your site, from the content in your Markdown files. Markdown Syntax \u00b6 Markdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for Syntax highlighted code block # Header 1 ## Header 2 ### Header 3 - Bulleted - List 1. Numbered 2. List **Bold** and _Italic_ and `Code` text [Link](url) and ![Image](src) For more details see GitHub Flavored Markdown . Mkdocs Themes with GitHub Pages \u00b6 Read how to set-up the site with GitHub pages Support or Contact \u00b6 Having trouble with Pages? Check out our documentation or contact support and we\u2019ll help you sort it out.","title":"WELCOME"},{"location":"#welcome_to_raspberry_pi_k3s_stories","text":"You can use the editor on GitHub to maintain and preview the content for your website in Markdown files. Whenever you commit to this repository, GitHub Pages will run Jekyll to rebuild the pages in your site, from the content in your Markdown files.","title":"Welcome to Raspberry PI k3s stories"},{"location":"#markdown_syntax","text":"Markdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for Syntax highlighted code block # Header 1 ## Header 2 ### Header 3 - Bulleted - List 1. Numbered 2. List **Bold** and _Italic_ and `Code` text [Link](url) and ![Image](src) For more details see GitHub Flavored Markdown .","title":"Markdown Syntax"},{"location":"#mkdocs_themes_with_github_pages","text":"Read how to set-up the site with GitHub pages","title":"Mkdocs Themes with GitHub Pages"},{"location":"#support_or_contact","text":"Having trouble with Pages? Check out our documentation or contact support and we\u2019ll help you sort it out.","title":"Support or Contact"},{"location":"pi-stories1/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Basic OS configuration \u00b6 Before we can build a PI 4 cluster using kubernetes software of some kind (most likely k3s) we need to buy the required hardware to begin with. In our case we decided to go for 5 Raspberry Pi's type 4 with 4 GB RAM. We leave the exercise to you what size of SD card you want (more GB cost more money, but gives you some room for expansion later on). We also bought some small USB sticks (type SanDisk Ultra Fit USB 3.1 flash drive 128 GB) for building an object oriented file system (but that is for a later series). Also, we bought some PI cases with fans built-in to keep the processor cool as we know kubernetes may heat up the processor [1]. We noticed that the temperature of a case with fans is around 40 degrees Celsius where with a metal case only (without fans) it is around 60 degrees Celsius. Decided to buy quickly an extra case with fans as it is worth its money. We were charmed with Ubuntu 20 series software and downloaded the Pi4 64-bit version and used the dd command to burn it onto the SD cards [2]. Link it all together to start a Pi computer one at the time. The first time we hooked the micro-HDMI to a TV-screen so we could watch the first kick off and to see everything looked right. Also, we needed to reset the default password of the built-in account named 'ubuntu'. By default the Pi computer is using DHCP for retrieving IP addresses, but we want to assign a static IPv4 address. Therefore, think ahaid and use something like this: cat >> /etc/hosts <<EOF # PI cluster 192.168.0.201 n1 192.168.0.202 n2 192.168.0.203 n3 192.168.0.204 n4 192.168.0.205 n5 EOF Of course, change the IPv4 addresses to your local taste. It is also a good idea to hard-code your local timezone, in our case we choose for Europe/Brussels, e.g. timedatectl set-timezone Europe/Brussels The kubernetes cluster (not yet of course) prefers not having IPv6 active, therefore, disable it via: echo \"net.ipv6.conf.all.disable_ipv6 = 1\" >> /etc/sysctl.conf sysctl -p And, check the time settings: systemctl status systemd-timesyncd To disable DHCP at next restart execute the following: cat > /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg <<EOF # network: {config: disabled} network: ethernets: eth0: dhcp4: true optional: true version: 2 EOF And, lets hide the original netplan yaml file: mkdir /etc/netplan/.hide mv /etc/netplan/50-cloud-init.yaml /etc/netplan/.hide The following steps differ node per node as we will define the hostname and fix the IPv4 address: hostnamectl set-hostname n1 Be careful, to use the correct hostname in above command. To define your permanent IPv4 address create your own netplan configuration as follow: cat > /etc/netplan/01-netcfg.yaml <<EOF # This file describes the network interfaces available on your system # For more information, see netplan(5). network: version: 2 renderer: networkd ethernets: eth0: dhcp4: no # IP address/subnet mask addresses: [192.168.0.201/24] # default gateway gateway4: 192.168.0.1 nameservers: # name server this host refers addresses: [192.168.0.1,8.8.8.8] dhcp6: no EOF Then, edit the file /etc/netplan/01-netcfg.yaml to adjust the correct IPv4 address of the node and also modify the gateway IPv4 address to your needs. On your laptop (or control server) we also add the IPv4 addresses of our Pi systems to the /etc/hosts file and to make your live easy copy your public OpenSSH keys to the ubuntu account like: ssh-copy-id ubuntu@n[1-5] Reboot this Pi computer and try to login via your laptop using ssh ubuntu@n1 OK so far for the first part, but just want to share picture of our setup with 5 Raspberry Pi's 4 nodes n[1-5]: References: \u00b6 [1] Joy-it Armor case \"Block Active\" for Raspberry Pi 4 [2] Download your Ubuntu Pi image Edit history \u00b6 Initial post on 01/Jul/2020 Updated title on 09/Sep/2020","title":"Raspberry Pi 4 Basic OS Configuration"},{"location":"pi-stories1/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories1/#raspberry_pi_4_cluster_series_-_basic_os_configuration","text":"Before we can build a PI 4 cluster using kubernetes software of some kind (most likely k3s) we need to buy the required hardware to begin with. In our case we decided to go for 5 Raspberry Pi's type 4 with 4 GB RAM. We leave the exercise to you what size of SD card you want (more GB cost more money, but gives you some room for expansion later on). We also bought some small USB sticks (type SanDisk Ultra Fit USB 3.1 flash drive 128 GB) for building an object oriented file system (but that is for a later series). Also, we bought some PI cases with fans built-in to keep the processor cool as we know kubernetes may heat up the processor [1]. We noticed that the temperature of a case with fans is around 40 degrees Celsius where with a metal case only (without fans) it is around 60 degrees Celsius. Decided to buy quickly an extra case with fans as it is worth its money. We were charmed with Ubuntu 20 series software and downloaded the Pi4 64-bit version and used the dd command to burn it onto the SD cards [2]. Link it all together to start a Pi computer one at the time. The first time we hooked the micro-HDMI to a TV-screen so we could watch the first kick off and to see everything looked right. Also, we needed to reset the default password of the built-in account named 'ubuntu'. By default the Pi computer is using DHCP for retrieving IP addresses, but we want to assign a static IPv4 address. Therefore, think ahaid and use something like this: cat >> /etc/hosts <<EOF # PI cluster 192.168.0.201 n1 192.168.0.202 n2 192.168.0.203 n3 192.168.0.204 n4 192.168.0.205 n5 EOF Of course, change the IPv4 addresses to your local taste. It is also a good idea to hard-code your local timezone, in our case we choose for Europe/Brussels, e.g. timedatectl set-timezone Europe/Brussels The kubernetes cluster (not yet of course) prefers not having IPv6 active, therefore, disable it via: echo \"net.ipv6.conf.all.disable_ipv6 = 1\" >> /etc/sysctl.conf sysctl -p And, check the time settings: systemctl status systemd-timesyncd To disable DHCP at next restart execute the following: cat > /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg <<EOF # network: {config: disabled} network: ethernets: eth0: dhcp4: true optional: true version: 2 EOF And, lets hide the original netplan yaml file: mkdir /etc/netplan/.hide mv /etc/netplan/50-cloud-init.yaml /etc/netplan/.hide The following steps differ node per node as we will define the hostname and fix the IPv4 address: hostnamectl set-hostname n1 Be careful, to use the correct hostname in above command. To define your permanent IPv4 address create your own netplan configuration as follow: cat > /etc/netplan/01-netcfg.yaml <<EOF # This file describes the network interfaces available on your system # For more information, see netplan(5). network: version: 2 renderer: networkd ethernets: eth0: dhcp4: no # IP address/subnet mask addresses: [192.168.0.201/24] # default gateway gateway4: 192.168.0.1 nameservers: # name server this host refers addresses: [192.168.0.1,8.8.8.8] dhcp6: no EOF Then, edit the file /etc/netplan/01-netcfg.yaml to adjust the correct IPv4 address of the node and also modify the gateway IPv4 address to your needs. On your laptop (or control server) we also add the IPv4 addresses of our Pi systems to the /etc/hosts file and to make your live easy copy your public OpenSSH keys to the ubuntu account like: ssh-copy-id ubuntu@n[1-5] Reboot this Pi computer and try to login via your laptop using ssh ubuntu@n1 OK so far for the first part, but just want to share picture of our setup with 5 Raspberry Pi's 4 nodes n[1-5]:","title":"Raspberry Pi 4 cluster Series - Basic OS configuration"},{"location":"pi-stories1/#references","text":"[1] Joy-it Armor case \"Block Active\" for Raspberry Pi 4 [2] Download your Ubuntu Pi image","title":"References:"},{"location":"pi-stories1/#edit_history","text":"Initial post on 01/Jul/2020 Updated title on 09/Sep/2020","title":"Edit history"},{"location":"pi-stories10/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Issues after upgrading k3s \u00b6 We upgrade our k3s version v1.20.4+k3s1 to v1.22.4+k3s1 according the procedure ligned out in [1]. However, we noticed that after the k3s upgrade their where some issues with our cluster as seen with command kubectl get pods -A cert-manager cert-manager-cainjector-6d59c8d4f7-hjszd 0/1 CrashLoopBackOff 39 (102s ago) 270d longhorn-system longhorn-driver-deployer-666c84fbb7-2lqqm 0/1 CrashLoopBackOff 43 (3m13s ago) 3h28m graphite graphite-0 0/1 ContainerCreating 0 167m Fixing the cert-manager issue \u00b6 The first thing you need to do when you have issues with apod is looking at the logs, therefore, we execute the following: $ kubectl logs -n cert-manager cert-manager-cainjector-6d59c8d4f7-hjszd I1207 13:08:09.495826 1 start.go:89] \"starting\" version=\"v1.0.4\" revision=\"4d870e49b43960fad974487a262395e65da1373e\" I1207 13:08:11.316015 1 request.go:645] Throttling request took 1.036581941s, request: GET:https://10.43.0.1:443/apis/admissionregistration.k8s.io/v1?timeout=32s I1207 13:08:12.316195 1 request.go:645] Throttling request took 2.035257813s, request: GET:https://10.43.0.1:443/apis/node.k8s.io/v1beta1?timeout=32s E1207 13:08:12.325362 1 start.go:158] cert-manager/ca-injector \"msg\"=\"error registering core-only controllers\" \"error\"=\"no matches for kind \\\"MutatingWebhookConfiguration\\\" in version \\\"admissionregistration.k8s.io/v1beta1\\\"\" From above output we can see that the cert-manager version is v1.0.4. The best thing to do is going to see the documentation [2] of cert-manager whether there are known issues or other items of interest. At the page [2] we read the following sentence \"Following their deprecation in version 1.4, the cert-manager API versions v1alpha2, v1alpha3, and v1beta1 are no longer served.\" - okay - we need to upgrade cert-manager to fix this. We did found an excellent article [3] that serverd our purposes. We simply have to look at the GitHub page of cert-manager releases [4] to find the latest stable version of cert-manager (today 07 December 2021 it was v1.6.1). Then we just downloaded the yaml file of cert-manager for arm (as this is a pi cluster) with a simple trick described oon [3]: VER=$(curl -s \"https://github.com/jetstack/cert-manager/releases/latest\" | cut -d\\\" -f2 | awk -F '/' '{print $NF}') curl -sL \\ https://github.com/jetstack/cert-manager/releases/download/${VER}/cert-manager.yaml |\\ sed -r 's/(image:.*):(v.*)$/\\1-arm:\\2/g' > cert-manager-arm.yaml Finally, we can upgrade our cert-manager pods with the command: $ kubectl replace -f cert-manager-arm.yaml customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io replaced customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io replaced customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io replaced customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io replaced customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io replaced customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io replaced namespace/cert-manager replaced serviceaccount/cert-manager-cainjector replaced serviceaccount/cert-manager replaced serviceaccount/cert-manager-webhook replaced clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim replaced clusterrole.rbac.authorization.k8s.io/cert-manager-view replaced clusterrole.rbac.authorization.k8s.io/cert-manager-edit replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests replaced clusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews replaced role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection replaced role.rbac.authorization.k8s.io/cert-manager:leaderelection replaced role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving replaced rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection replaced rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection replaced rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving replaced service/cert-manager replaced service/cert-manager-webhook replaced deployment.apps/cert-manager-cainjector replaced deployment.apps/cert-manager replaced deployment.apps/cert-manager-webhook replaced mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook replaced validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook replaced $ kubectl get pods -n cert-manager NAME READY STATUS RESTARTS AGE cert-manager-5d4dc99cfc-mfmrp 1/1 Running 0 161m cert-manager-cainjector-598fcd9fdd-sn9cs 1/1 Running 0 161m cert-manager-webhook-746ff5ffb9-wmnvp 1/1 Running 0 161m References \u00b6 [1] Raspberry Pi 4 cluster Series - Upgrading k3s software on your cluster [2] Removing Deprecated API Resources in cert-manager [3] K3s on Raspberry Pi - cert-manager [4] GitHub Cert-manager release page","title":"Raspberry Pi 4 Issues after upgrading k3s"},{"location":"pi-stories10/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories10/#raspberry_pi_4_cluster_series_-_issues_after_upgrading_k3s","text":"We upgrade our k3s version v1.20.4+k3s1 to v1.22.4+k3s1 according the procedure ligned out in [1]. However, we noticed that after the k3s upgrade their where some issues with our cluster as seen with command kubectl get pods -A cert-manager cert-manager-cainjector-6d59c8d4f7-hjszd 0/1 CrashLoopBackOff 39 (102s ago) 270d longhorn-system longhorn-driver-deployer-666c84fbb7-2lqqm 0/1 CrashLoopBackOff 43 (3m13s ago) 3h28m graphite graphite-0 0/1 ContainerCreating 0 167m","title":"Raspberry Pi 4 cluster Series - Issues after upgrading k3s"},{"location":"pi-stories10/#fixing_the_cert-manager_issue","text":"The first thing you need to do when you have issues with apod is looking at the logs, therefore, we execute the following: $ kubectl logs -n cert-manager cert-manager-cainjector-6d59c8d4f7-hjszd I1207 13:08:09.495826 1 start.go:89] \"starting\" version=\"v1.0.4\" revision=\"4d870e49b43960fad974487a262395e65da1373e\" I1207 13:08:11.316015 1 request.go:645] Throttling request took 1.036581941s, request: GET:https://10.43.0.1:443/apis/admissionregistration.k8s.io/v1?timeout=32s I1207 13:08:12.316195 1 request.go:645] Throttling request took 2.035257813s, request: GET:https://10.43.0.1:443/apis/node.k8s.io/v1beta1?timeout=32s E1207 13:08:12.325362 1 start.go:158] cert-manager/ca-injector \"msg\"=\"error registering core-only controllers\" \"error\"=\"no matches for kind \\\"MutatingWebhookConfiguration\\\" in version \\\"admissionregistration.k8s.io/v1beta1\\\"\" From above output we can see that the cert-manager version is v1.0.4. The best thing to do is going to see the documentation [2] of cert-manager whether there are known issues or other items of interest. At the page [2] we read the following sentence \"Following their deprecation in version 1.4, the cert-manager API versions v1alpha2, v1alpha3, and v1beta1 are no longer served.\" - okay - we need to upgrade cert-manager to fix this. We did found an excellent article [3] that serverd our purposes. We simply have to look at the GitHub page of cert-manager releases [4] to find the latest stable version of cert-manager (today 07 December 2021 it was v1.6.1). Then we just downloaded the yaml file of cert-manager for arm (as this is a pi cluster) with a simple trick described oon [3]: VER=$(curl -s \"https://github.com/jetstack/cert-manager/releases/latest\" | cut -d\\\" -f2 | awk -F '/' '{print $NF}') curl -sL \\ https://github.com/jetstack/cert-manager/releases/download/${VER}/cert-manager.yaml |\\ sed -r 's/(image:.*):(v.*)$/\\1-arm:\\2/g' > cert-manager-arm.yaml Finally, we can upgrade our cert-manager pods with the command: $ kubectl replace -f cert-manager-arm.yaml customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io replaced customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io replaced customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io replaced customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io replaced customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io replaced customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io replaced namespace/cert-manager replaced serviceaccount/cert-manager-cainjector replaced serviceaccount/cert-manager replaced serviceaccount/cert-manager-webhook replaced clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim replaced clusterrole.rbac.authorization.k8s.io/cert-manager-view replaced clusterrole.rbac.authorization.k8s.io/cert-manager-edit replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io replaced clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests replaced clusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests replaced clusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews replaced role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection replaced role.rbac.authorization.k8s.io/cert-manager:leaderelection replaced role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving replaced rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection replaced rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection replaced rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving replaced service/cert-manager replaced service/cert-manager-webhook replaced deployment.apps/cert-manager-cainjector replaced deployment.apps/cert-manager replaced deployment.apps/cert-manager-webhook replaced mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook replaced validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook replaced $ kubectl get pods -n cert-manager NAME READY STATUS RESTARTS AGE cert-manager-5d4dc99cfc-mfmrp 1/1 Running 0 161m cert-manager-cainjector-598fcd9fdd-sn9cs 1/1 Running 0 161m cert-manager-webhook-746ff5ffb9-wmnvp 1/1 Running 0 161m","title":"Fixing the cert-manager issue"},{"location":"pi-stories10/#references","text":"[1] Raspberry Pi 4 cluster Series - Upgrading k3s software on your cluster [2] Removing Deprecated API Resources in cert-manager [3] K3s on Raspberry Pi - cert-manager [4] GitHub Cert-manager release page","title":"References"},{"location":"pi-stories11/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Replacing internal traefik with Metallb \u00b6 The problem with have is that with our home pi4 cluster we don't have a decnt external load-balancer. Therefore, it is hard to access pods via an external IP address, such as the ones we have on our hosts (in our case in the range of 192.168.0.200-254). The steps we need to perform are... Re-configure the pi4 cluster with ansible \u00b6 Our k3s-ansible project was updated with: $ cat inventory/my-cluster/group_vars/all.yml --- k3s_version: v1.23.6+k3s1 ansible_user: gdha systemd_dir: /etc/systemd/system master_ip: \"{{ hostvars[groups['master'][0]]['ansible_host'] | default(groups['master'][0]) }}\" extra_server_args: \"--write-kubeconfig-mode 644 --disable traefik --disable servicelb\" extra_agent_args: \"\" in such way by disabling the default traefik and internal load-balancer delivered with the standard k3s implementation. While we were busy we also used the latest k3s version available at this given moment. Then it is just a matter of re-running: ansible-playbook site.yml -i inventory/my-cluster/hosts.ini It will remove k3s and re-implement it with the internal traefik, but all pods already installed remain present. Excellent news. Install metalllb layer2 load-balancer \u00b6 The main documentation of metallb can be found at https://metallb.universe.tf/installation/ [1]. We used the following steps: $ helm repo add metallb https://metallb.github.io/metallb $ helm repo list NAME URL longhorn https://charts.longhorn.io kiwigrid https://kiwigrid.github.io metallb https://metallb.github.io/metallb $ helm install metallb metallb/metallb --namespace kube-system \\ --set configInline.address-pools[0].name=default \\ --set configInline.address-pools[0].protocol=layer2 \\ --set configInline.address-pools[0].addresses[0]=192.168.0.240-192.168.0.250 W0523 12:38:34.591089 84914 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W0523 12:38:34.600588 84914 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W0523 12:38:34.788874 84914 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W0523 12:38:34.790806 84914 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ NAME: metallb LAST DEPLOYED: Mon May 23 12:38:29 2022 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: MetalLB is now running in the cluster. LoadBalancer Services in your cluster are now available on the IPs you defined in MetalLB's configuration: config: address-pools: - addresses: - 192.168.0.240-192.168.0.250 name: default protocol: layer2 To see IP assignments, try `kubectl get services`. To verify the load-balancer is working we could execute: $ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller-admission ClusterIP 10.43.197.96 <none> 443/TCP 11d ingress-nginx-controller LoadBalancer 10.43.159.135 192.168.0.240 80:31719/TCP,443:32502/TCP 11d Okay, so far so good. Install traefik2 as replacement for the internal traefik of k3s \u00b6 Execute the following commands: $ helm repo add traefik https://helm.traefik.io/traefik $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"metallb\" chart repository ...Successfully got an update from the \"longhorn\" chart repository ...Successfully got an update from the \"traefik\" chart repository ...Successfully got an update from the \"kiwigrid\" chart repository Update Complete. \u2388Happy Helming!\u2388 We need to define a dummy (internal) name for our treafik2 application, therefore, create a file like the one shown below: $ cat traefik-values.yaml dashboard: enabled: true domain: traefik.example.com rbac: enabled: true And, finally use helm to install traefik2 with our hand-crafted values yaml file: $ helm install traefik traefik/traefik -n kube-system -f traefik-values.yaml NAME: traefik LAST DEPLOYED: Mon May 23 14:53:46 2022 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None Check if it is created properly: $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-d76bd69b-8z7dh 1/1 Running 0 4h25m local-path-provisioner-6c79684f77-jvdf6 1/1 Running 0 4h25m metrics-server-7cd5fcb6b7-d6wlx 1/1 Running 0 4h25m metallb-controller-777cbcf64f-vfz5v 1/1 Running 0 136m metallb-speaker-r7wbg 1/1 Running 0 136m metallb-speaker-5lxff 1/1 Running 0 136m metallb-speaker-cxskn 1/1 Running 0 136m metallb-speaker-24vgg 1/1 Running 0 136m metallb-speaker-wmzkg 1/1 Running 0 136m traefik-7b9cf77df9-cwp4l 1/1 Running 0 67s And, also very if the treafik service is present: $ kubectl get svc -n kube-system | grep traefik traefik LoadBalancer 10.43.21.173 192.168.0.241 80:30339/TCP,443:31587/TCP 2m27s We can also check the logs of traefik: $ kubectl -n kube-system logs $(kubectl -n kube-system get pods --selector \"app.kubernetes.io/name=traefik\" --output=name) time=\"2022-05-23T12:54:04Z\" level=info msg=\"Configuration loaded from flags.\" When we see above listed line the we are sure traefik is properly installed and configured. Now, we are ready to do some more tests with our new load-balancer and traefik. References \u00b6 [1] Metallb [2] Setting up your own k3s home cluster [3] Configuring Traefik 2 Ingress for Kubernetes","title":"Raspberry Pi 4 Replacing internal traefik with Metallb"},{"location":"pi-stories11/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories11/#raspberry_pi_4_cluster_series_-_replacing_internal_traefik_with_metallb","text":"The problem with have is that with our home pi4 cluster we don't have a decnt external load-balancer. Therefore, it is hard to access pods via an external IP address, such as the ones we have on our hosts (in our case in the range of 192.168.0.200-254). The steps we need to perform are...","title":"Raspberry Pi 4 cluster Series - Replacing internal traefik with Metallb"},{"location":"pi-stories11/#re-configure_the_pi4_cluster_with_ansible","text":"Our k3s-ansible project was updated with: $ cat inventory/my-cluster/group_vars/all.yml --- k3s_version: v1.23.6+k3s1 ansible_user: gdha systemd_dir: /etc/systemd/system master_ip: \"{{ hostvars[groups['master'][0]]['ansible_host'] | default(groups['master'][0]) }}\" extra_server_args: \"--write-kubeconfig-mode 644 --disable traefik --disable servicelb\" extra_agent_args: \"\" in such way by disabling the default traefik and internal load-balancer delivered with the standard k3s implementation. While we were busy we also used the latest k3s version available at this given moment. Then it is just a matter of re-running: ansible-playbook site.yml -i inventory/my-cluster/hosts.ini It will remove k3s and re-implement it with the internal traefik, but all pods already installed remain present. Excellent news.","title":"Re-configure the pi4 cluster with ansible"},{"location":"pi-stories11/#install_metalllb_layer2_load-balancer","text":"The main documentation of metallb can be found at https://metallb.universe.tf/installation/ [1]. We used the following steps: $ helm repo add metallb https://metallb.github.io/metallb $ helm repo list NAME URL longhorn https://charts.longhorn.io kiwigrid https://kiwigrid.github.io metallb https://metallb.github.io/metallb $ helm install metallb metallb/metallb --namespace kube-system \\ --set configInline.address-pools[0].name=default \\ --set configInline.address-pools[0].protocol=layer2 \\ --set configInline.address-pools[0].addresses[0]=192.168.0.240-192.168.0.250 W0523 12:38:34.591089 84914 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W0523 12:38:34.600588 84914 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W0523 12:38:34.788874 84914 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W0523 12:38:34.790806 84914 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ NAME: metallb LAST DEPLOYED: Mon May 23 12:38:29 2022 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: MetalLB is now running in the cluster. LoadBalancer Services in your cluster are now available on the IPs you defined in MetalLB's configuration: config: address-pools: - addresses: - 192.168.0.240-192.168.0.250 name: default protocol: layer2 To see IP assignments, try `kubectl get services`. To verify the load-balancer is working we could execute: $ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller-admission ClusterIP 10.43.197.96 <none> 443/TCP 11d ingress-nginx-controller LoadBalancer 10.43.159.135 192.168.0.240 80:31719/TCP,443:32502/TCP 11d Okay, so far so good.","title":"Install metalllb layer2 load-balancer"},{"location":"pi-stories11/#install_traefik2_as_replacement_for_the_internal_traefik_of_k3s","text":"Execute the following commands: $ helm repo add traefik https://helm.traefik.io/traefik $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"metallb\" chart repository ...Successfully got an update from the \"longhorn\" chart repository ...Successfully got an update from the \"traefik\" chart repository ...Successfully got an update from the \"kiwigrid\" chart repository Update Complete. \u2388Happy Helming!\u2388 We need to define a dummy (internal) name for our treafik2 application, therefore, create a file like the one shown below: $ cat traefik-values.yaml dashboard: enabled: true domain: traefik.example.com rbac: enabled: true And, finally use helm to install traefik2 with our hand-crafted values yaml file: $ helm install traefik traefik/traefik -n kube-system -f traefik-values.yaml NAME: traefik LAST DEPLOYED: Mon May 23 14:53:46 2022 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None Check if it is created properly: $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-d76bd69b-8z7dh 1/1 Running 0 4h25m local-path-provisioner-6c79684f77-jvdf6 1/1 Running 0 4h25m metrics-server-7cd5fcb6b7-d6wlx 1/1 Running 0 4h25m metallb-controller-777cbcf64f-vfz5v 1/1 Running 0 136m metallb-speaker-r7wbg 1/1 Running 0 136m metallb-speaker-5lxff 1/1 Running 0 136m metallb-speaker-cxskn 1/1 Running 0 136m metallb-speaker-24vgg 1/1 Running 0 136m metallb-speaker-wmzkg 1/1 Running 0 136m traefik-7b9cf77df9-cwp4l 1/1 Running 0 67s And, also very if the treafik service is present: $ kubectl get svc -n kube-system | grep traefik traefik LoadBalancer 10.43.21.173 192.168.0.241 80:30339/TCP,443:31587/TCP 2m27s We can also check the logs of traefik: $ kubectl -n kube-system logs $(kubectl -n kube-system get pods --selector \"app.kubernetes.io/name=traefik\" --output=name) time=\"2022-05-23T12:54:04Z\" level=info msg=\"Configuration loaded from flags.\" When we see above listed line the we are sure traefik is properly installed and configured. Now, we are ready to do some more tests with our new load-balancer and traefik.","title":"Install traefik2 as replacement for the internal traefik of k3s"},{"location":"pi-stories11/#references","text":"[1] Metallb [2] Setting up your own k3s home cluster [3] Configuring Traefik 2 Ingress for Kubernetes","title":"References"},{"location":"pi-stories2/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Prepare for kubernetes installation \u00b6 Add account with Secure Shell keys and common software packages \u00b6 It is important that my account is created on each host with the requires secure shell keys. Also, we install what we think are required software packages on each host: vim git rsync acl dnsutils dphys-swapfile python-is-python3 sshpass ca-certificates curl gnupg-agent software-properties-common jq To get going first install git on the system where you start ansible playbooks (in our case it is host n5) and afterwards, clone the playbook pi4-cluster-ansible-roles . Do check the inventory.yml file and add you preferences. The playbook will also disable swap on each host, which is a requirement for kubernetes. Compile ARM side libraries for interfacing to Raspberry Pi GPU \u00b6 Download the source code from 2 . Compile and install these binaries on the host from where you will use the playbook. In my case its was host n5 (do not forget to install ansible ). Run the ansible playbook provision.yml \u00b6 $ ansible-playbook provision.yml -k --vault-password-file .my_password SSH password: PLAY [all] ************************************************************************************************************************************* TASK [Gathering Facts] ************************************************************************************************************************* ok: [n5] ok: [n3] ok: [n2] ok: [n4] ok: [n1] TASK [user : debug] **************************************************************************************************************************** ok: [n1] => { \"msg\": \"creating user gdha and OpenSSH public key distribution\" } ok: [n2] => { \"msg\": \"creating user gdha and OpenSSH public key distribution\" } ok: [n3] => { \"msg\": \"creating user gdha and OpenSSH public key distribution\" } ok: [n4] => { \"msg\": \"creating user gdha and OpenSSH public key distribution\" } ok: [n5] => { \"msg\": \"creating user gdha and OpenSSH public key distribution\" } TASK [user : Creating admin group] ************************************************************************************************************* ok: [n3] ok: [n1] ok: [n4] ok: [n2] ok: [n5] TASK [user : Add group gdha (1001)] ************************************************************************************************************ ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [user : Add user 'gdha' with specific uid (1001) and group 'gdha' (1001) and secondary group 'admin'] ************************************* ok: [n5] ok: [n2] ok: [n3] ok: [n1] ok: [n4] TASK [user : Create the /home/gdha/.ssh directory] ********************************************************************************************* ok: [n2] ok: [n1] ok: [n3] ok: [n4] ok: [n5] TASK [user : Copy /home/gdha/.ssh/id_rsa.pub to remote nodes] ********************************************************************************** ok: [n2] ok: [n3] ok: [n1] ok: [n4] ok: [n5] TASK [user : Append public ssh key of gdha to authorized_keys] ********************************************************************************* changed: [n1] => (item=/home/gdha/.ssh/id_rsa.pub) changed: [n4] => (item=/home/gdha/.ssh/id_rsa.pub) changed: [n2] => (item=/home/gdha/.ssh/id_rsa.pub) changed: [n3] => (item=/home/gdha/.ssh/id_rsa.pub) ok: [n5] => (item=/home/gdha/.ssh/id_rsa.pub) TASK [user : Create /etc/sudoers.d/gdha-sudoers file] ****************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [base : Set /etc/hosts for other nodes] *************************************************************************************************** ok: [n1] => (item=n2) ok: [n2] => (item=n1) ok: [n3] => (item=n1) ok: [n4] => (item=n1) ok: [n5] => (item=n1) ok: [n1] => (item=n3) ok: [n2] => (item=n3) ok: [n3] => (item=n2) ok: [n4] => (item=n2) ok: [n5] => (item=n2) ok: [n1] => (item=n4) ok: [n2] => (item=n4) ok: [n4] => (item=n3) ok: [n3] => (item=n4) ok: [n5] => (item=n3) ok: [n1] => (item=n5) ok: [n2] => (item=n5) ok: [n4] => (item=n5) ok: [n3] => (item=n5) ok: [n5] => (item=n4) TASK [base : Install common packages] ********************************************************************************************************** ok: [n5] changed: [n2] changed: [n3] changed: [n4] changed: [n1] TASK [base : Set default locale] *************************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [base : Enable locale] ******************************************************************************************************************** ok: [n2] ok: [n1] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Create directory /opt/vc/bin] ****************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : copy tvservice to /opt/vc/bin] ***************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Create the /opt/vc/lib directory] ************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Copy /opt/vc/lib/libvchiq_arm.so] ************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Copy /opt/vc/lib/libvcos.so] ******************************************************************************************************* ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : add /opt/vc/lib to /etc/ld.so.conf file] ******************************************************************************************* ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Run ldconfig] ********************************************************************************************************************** changed: [n2] changed: [n1] changed: [n3] changed: [n4] changed: [n5] TASK [rpi : Check if HDMI is on] *************************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Switch off HDMI] ******************************************************************************************************************* changed: [n1] changed: [n2] changed: [n3] changed: [n4] changed: [n5] TASK [rpi : Ensure rc.local exists] ************************************************************************************************************ changed: [n1] changed: [n2] changed: [n3] changed: [n4] changed: [n5] TASK [rpi : Switch off HDMI on boot] *********************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [os : Set hostname] *********************************************************************************************************************** ok: [n2] ok: [n3] ok: [n1] ok: [n4] ok: [n5] TASK [os : Set /etc/hosts hostname] ************************************************************************************************************ ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [os : Check if swap is enabled] *********************************************************************************************************** changed: [n1] changed: [n2] changed: [n3] changed: [n4] changed: [n5] TASK [os : Disable swap] *********************************************************************************************************************** changed: [n1] => (item=dphys-swapfile swapoff) changed: [n2] => (item=dphys-swapfile swapoff) changed: [n3] => (item=dphys-swapfile swapoff) changed: [n4] => (item=dphys-swapfile swapoff) changed: [n5] => (item=dphys-swapfile swapoff) changed: [n1] => (item=dphys-swapfile uninstall) changed: [n2] => (item=dphys-swapfile uninstall) changed: [n3] => (item=dphys-swapfile uninstall) changed: [n4] => (item=dphys-swapfile uninstall) changed: [n5] => (item=dphys-swapfile uninstall) changed: [n2] => (item=update-rc.d dphys-swapfile disable) changed: [n3] => (item=update-rc.d dphys-swapfile disable) changed: [n1] => (item=update-rc.d dphys-swapfile disable) changed: [n4] => (item=update-rc.d dphys-swapfile disable) changed: [n5] => (item=update-rc.d dphys-swapfile disable) TASK [os : Add users to passwordless sudoers.] ************************************************************************************************* ok: [n1] => (item=gdha) ok: [n2] => (item=gdha) ok: [n3] => (item=gdha) ok: [n4] => (item=gdha) ok: [n5] => (item=gdha) ok: [n1] => (item=ubuntu) ok: [n2] => (item=ubuntu) ok: [n3] => (item=ubuntu) ok: [n4] => (item=ubuntu) ok: [n5] => (item=ubuntu) TASK [ssh : Check if ssh key exists] *********************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [ssh : Create .ssh directory] ************************************************************************************************************* ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [ssh : Generate ssh key] ****************************************************************************************************************** skipping: [n5] changed: [n1] changed: [n3] changed: [n2] changed: [n4] TASK [ssh : Slurp public keys from all nodes] ************************************************************************************************** ok: [n1] ok: [n3] ok: [n2] ok: [n4] ok: [n5] TASK [ssh : Copy public keys of all nodes into authorized_keys] ******************************************************************************** skipping: [n1] => (item=n1) skipping: [n1] => (item=n2) skipping: [n1] => (item=n3) skipping: [n1] => (item=n4) skipping: [n1] => (item=n5) skipping: [n2] => (item=n1) skipping: [n2] => (item=n2) skipping: [n2] => (item=n3) skipping: [n2] => (item=n4) skipping: [n2] => (item=n5) skipping: [n3] => (item=n1) skipping: [n3] => (item=n2) skipping: [n3] => (item=n3) skipping: [n3] => (item=n4) skipping: [n3] => (item=n5) skipping: [n4] => (item=n1) skipping: [n4] => (item=n2) skipping: [n4] => (item=n3) skipping: [n4] => (item=n4) skipping: [n4] => (item=n5) skipping: [n5] => (item=n1) skipping: [n5] => (item=n2) skipping: [n5] => (item=n3) skipping: [n5] => (item=n4) skipping: [n5] => (item=n5) TASK [ssh : Copy local public key to authorized_keys] ****************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [ssh : Slurp host keys from all nodes] **************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [ssh : Insert all nodes into global known_hosts] ****************************************************************************************** ok: [n1] => (item=n1) ok: [n2] => (item=n1) ok: [n3] => (item=n1) ok: [n4] => (item=n1) ok: [n5] => (item=n1) ok: [n2] => (item=n2) ok: [n3] => (item=n2) ok: [n1] => (item=n2) ok: [n4] => (item=n2) ok: [n5] => (item=n2) ok: [n2] => (item=n3) ok: [n3] => (item=n3) ok: [n4] => (item=n3) ok: [n1] => (item=n3) ok: [n5] => (item=n3) ok: [n2] => (item=n4) ok: [n3] => (item=n4) ok: [n4] => (item=n4) ok: [n1] => (item=n4) ok: [n5] => (item=n4) ok: [n2] => (item=n5) ok: [n3] => (item=n5) ok: [n4] => (item=n5) ok: [n1] => (item=n5) ok: [n5] => (item=n5) TASK [ssh : Secure SSH configuration] ********************************************************************************************************** ok: [n1] => (item={'regexp': '^[# \\\\t]*Port', 'line': 'Port 22'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*Port', 'line': 'Port 22'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*Port', 'line': 'Port 22'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*Port', 'line': 'Port 22'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*Port', 'line': 'Port 22'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*PasswordAuthentication', 'line': 'PasswordAuthentication yes'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*PasswordAuthentication', 'line': 'PasswordAuthentication yes'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*PasswordAuthentication', 'line': 'PasswordAuthentication yes'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*PasswordAuthentication', 'line': 'PasswordAuthentication yes'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*PasswordAuthentication', 'line': 'PasswordAuthentication yes'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*PermitRootLogin', 'line': 'PermitRootLogin no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*PermitRootLogin', 'line': 'PermitRootLogin no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*PermitRootLogin', 'line': 'PermitRootLogin no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*PermitRootLogin', 'line': 'PermitRootLogin no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*PermitRootLogin', 'line': 'PermitRootLogin no'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*UseDNS', 'line': 'UseDNS no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*UseDNS', 'line': 'UseDNS no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*UseDNS', 'line': 'UseDNS no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*UseDNS', 'line': 'UseDNS no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*UseDNS', 'line': 'UseDNS no'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*PermitEmptyPasswords', 'line': 'PermitEmptyPasswords no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*PermitEmptyPasswords', 'line': 'PermitEmptyPasswords no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*PermitEmptyPasswords', 'line': 'PermitEmptyPasswords no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*PermitEmptyPasswords', 'line': 'PermitEmptyPasswords no'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*ChallengeResponseAuthentication', 'line': 'ChallengeResponseAuthentication no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*PermitEmptyPasswords', 'line': 'PermitEmptyPasswords no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*ChallengeResponseAuthentication', 'line': 'ChallengeResponseAuthentication no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*ChallengeResponseAuthentication', 'line': 'ChallengeResponseAuthentication no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*ChallengeResponseAuthentication', 'line': 'ChallengeResponseAuthentication no'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*GSSAPIAuthentication', 'line': 'GSSAPIAuthentication no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*GSSAPIAuthentication', 'line': 'GSSAPIAuthentication no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*ChallengeResponseAuthentication', 'line': 'ChallengeResponseAuthentication no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*GSSAPIAuthentication', 'line': 'GSSAPIAuthentication no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*GSSAPIAuthentication', 'line': 'GSSAPIAuthentication no'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*X11Forwarding', 'line': 'X11Forwarding no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*X11Forwarding', 'line': 'X11Forwarding no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*GSSAPIAuthentication', 'line': 'GSSAPIAuthentication no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*X11Forwarding', 'line': 'X11Forwarding no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*X11Forwarding', 'line': 'X11Forwarding no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*X11Forwarding', 'line': 'X11Forwarding no'}) TASK [ssh : Secure SSH hosts configuration] **************************************************************************************************** ok: [n1] => (item={'regexp': '^[# \\\\t]*HashKnownHosts', 'line': 'HashKnownHosts no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*HashKnownHosts', 'line': 'HashKnownHosts no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*HashKnownHosts', 'line': 'HashKnownHosts no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*HashKnownHosts', 'line': 'HashKnownHosts no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*HashKnownHosts', 'line': 'HashKnownHosts no'}) PLAY RECAP ************************************************************************************************************************************* n1 : ok=38 changed=8 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 n2 : ok=38 changed=8 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 n3 : ok=38 changed=8 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 n4 : ok=38 changed=8 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 n5 : ok=37 changed=5 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0 References \u00b6 [1] Ansible pi4-cluster-ansible-roles playbook [2] Source code for ARM side libraries for interfacing to Raspberry Pi GPU Edit history \u00b6 initial post on 09/Sep/2020","title":"Raspberry Pi 4 Prepare for kubernetes"},{"location":"pi-stories2/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories2/#raspberry_pi_4_cluster_series_-_prepare_for_kubernetes_installation","text":"","title":"Raspberry Pi 4 cluster Series - Prepare for kubernetes installation"},{"location":"pi-stories2/#add_account_with_secure_shell_keys_and_common_software_packages","text":"It is important that my account is created on each host with the requires secure shell keys. Also, we install what we think are required software packages on each host: vim git rsync acl dnsutils dphys-swapfile python-is-python3 sshpass ca-certificates curl gnupg-agent software-properties-common jq To get going first install git on the system where you start ansible playbooks (in our case it is host n5) and afterwards, clone the playbook pi4-cluster-ansible-roles . Do check the inventory.yml file and add you preferences. The playbook will also disable swap on each host, which is a requirement for kubernetes.","title":"Add account with Secure Shell keys and common software packages"},{"location":"pi-stories2/#compile_arm_side_libraries_for_interfacing_to_raspberry_pi_gpu","text":"Download the source code from 2 . Compile and install these binaries on the host from where you will use the playbook. In my case its was host n5 (do not forget to install ansible ).","title":"Compile ARM side libraries for interfacing to Raspberry Pi GPU"},{"location":"pi-stories2/#run_the_ansible_playbook_provisionyml","text":"$ ansible-playbook provision.yml -k --vault-password-file .my_password SSH password: PLAY [all] ************************************************************************************************************************************* TASK [Gathering Facts] ************************************************************************************************************************* ok: [n5] ok: [n3] ok: [n2] ok: [n4] ok: [n1] TASK [user : debug] **************************************************************************************************************************** ok: [n1] => { \"msg\": \"creating user gdha and OpenSSH public key distribution\" } ok: [n2] => { \"msg\": \"creating user gdha and OpenSSH public key distribution\" } ok: [n3] => { \"msg\": \"creating user gdha and OpenSSH public key distribution\" } ok: [n4] => { \"msg\": \"creating user gdha and OpenSSH public key distribution\" } ok: [n5] => { \"msg\": \"creating user gdha and OpenSSH public key distribution\" } TASK [user : Creating admin group] ************************************************************************************************************* ok: [n3] ok: [n1] ok: [n4] ok: [n2] ok: [n5] TASK [user : Add group gdha (1001)] ************************************************************************************************************ ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [user : Add user 'gdha' with specific uid (1001) and group 'gdha' (1001) and secondary group 'admin'] ************************************* ok: [n5] ok: [n2] ok: [n3] ok: [n1] ok: [n4] TASK [user : Create the /home/gdha/.ssh directory] ********************************************************************************************* ok: [n2] ok: [n1] ok: [n3] ok: [n4] ok: [n5] TASK [user : Copy /home/gdha/.ssh/id_rsa.pub to remote nodes] ********************************************************************************** ok: [n2] ok: [n3] ok: [n1] ok: [n4] ok: [n5] TASK [user : Append public ssh key of gdha to authorized_keys] ********************************************************************************* changed: [n1] => (item=/home/gdha/.ssh/id_rsa.pub) changed: [n4] => (item=/home/gdha/.ssh/id_rsa.pub) changed: [n2] => (item=/home/gdha/.ssh/id_rsa.pub) changed: [n3] => (item=/home/gdha/.ssh/id_rsa.pub) ok: [n5] => (item=/home/gdha/.ssh/id_rsa.pub) TASK [user : Create /etc/sudoers.d/gdha-sudoers file] ****************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [base : Set /etc/hosts for other nodes] *************************************************************************************************** ok: [n1] => (item=n2) ok: [n2] => (item=n1) ok: [n3] => (item=n1) ok: [n4] => (item=n1) ok: [n5] => (item=n1) ok: [n1] => (item=n3) ok: [n2] => (item=n3) ok: [n3] => (item=n2) ok: [n4] => (item=n2) ok: [n5] => (item=n2) ok: [n1] => (item=n4) ok: [n2] => (item=n4) ok: [n4] => (item=n3) ok: [n3] => (item=n4) ok: [n5] => (item=n3) ok: [n1] => (item=n5) ok: [n2] => (item=n5) ok: [n4] => (item=n5) ok: [n3] => (item=n5) ok: [n5] => (item=n4) TASK [base : Install common packages] ********************************************************************************************************** ok: [n5] changed: [n2] changed: [n3] changed: [n4] changed: [n1] TASK [base : Set default locale] *************************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [base : Enable locale] ******************************************************************************************************************** ok: [n2] ok: [n1] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Create directory /opt/vc/bin] ****************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : copy tvservice to /opt/vc/bin] ***************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Create the /opt/vc/lib directory] ************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Copy /opt/vc/lib/libvchiq_arm.so] ************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Copy /opt/vc/lib/libvcos.so] ******************************************************************************************************* ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : add /opt/vc/lib to /etc/ld.so.conf file] ******************************************************************************************* ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Run ldconfig] ********************************************************************************************************************** changed: [n2] changed: [n1] changed: [n3] changed: [n4] changed: [n5] TASK [rpi : Check if HDMI is on] *************************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [rpi : Switch off HDMI] ******************************************************************************************************************* changed: [n1] changed: [n2] changed: [n3] changed: [n4] changed: [n5] TASK [rpi : Ensure rc.local exists] ************************************************************************************************************ changed: [n1] changed: [n2] changed: [n3] changed: [n4] changed: [n5] TASK [rpi : Switch off HDMI on boot] *********************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [os : Set hostname] *********************************************************************************************************************** ok: [n2] ok: [n3] ok: [n1] ok: [n4] ok: [n5] TASK [os : Set /etc/hosts hostname] ************************************************************************************************************ ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [os : Check if swap is enabled] *********************************************************************************************************** changed: [n1] changed: [n2] changed: [n3] changed: [n4] changed: [n5] TASK [os : Disable swap] *********************************************************************************************************************** changed: [n1] => (item=dphys-swapfile swapoff) changed: [n2] => (item=dphys-swapfile swapoff) changed: [n3] => (item=dphys-swapfile swapoff) changed: [n4] => (item=dphys-swapfile swapoff) changed: [n5] => (item=dphys-swapfile swapoff) changed: [n1] => (item=dphys-swapfile uninstall) changed: [n2] => (item=dphys-swapfile uninstall) changed: [n3] => (item=dphys-swapfile uninstall) changed: [n4] => (item=dphys-swapfile uninstall) changed: [n5] => (item=dphys-swapfile uninstall) changed: [n2] => (item=update-rc.d dphys-swapfile disable) changed: [n3] => (item=update-rc.d dphys-swapfile disable) changed: [n1] => (item=update-rc.d dphys-swapfile disable) changed: [n4] => (item=update-rc.d dphys-swapfile disable) changed: [n5] => (item=update-rc.d dphys-swapfile disable) TASK [os : Add users to passwordless sudoers.] ************************************************************************************************* ok: [n1] => (item=gdha) ok: [n2] => (item=gdha) ok: [n3] => (item=gdha) ok: [n4] => (item=gdha) ok: [n5] => (item=gdha) ok: [n1] => (item=ubuntu) ok: [n2] => (item=ubuntu) ok: [n3] => (item=ubuntu) ok: [n4] => (item=ubuntu) ok: [n5] => (item=ubuntu) TASK [ssh : Check if ssh key exists] *********************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [ssh : Create .ssh directory] ************************************************************************************************************* ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [ssh : Generate ssh key] ****************************************************************************************************************** skipping: [n5] changed: [n1] changed: [n3] changed: [n2] changed: [n4] TASK [ssh : Slurp public keys from all nodes] ************************************************************************************************** ok: [n1] ok: [n3] ok: [n2] ok: [n4] ok: [n5] TASK [ssh : Copy public keys of all nodes into authorized_keys] ******************************************************************************** skipping: [n1] => (item=n1) skipping: [n1] => (item=n2) skipping: [n1] => (item=n3) skipping: [n1] => (item=n4) skipping: [n1] => (item=n5) skipping: [n2] => (item=n1) skipping: [n2] => (item=n2) skipping: [n2] => (item=n3) skipping: [n2] => (item=n4) skipping: [n2] => (item=n5) skipping: [n3] => (item=n1) skipping: [n3] => (item=n2) skipping: [n3] => (item=n3) skipping: [n3] => (item=n4) skipping: [n3] => (item=n5) skipping: [n4] => (item=n1) skipping: [n4] => (item=n2) skipping: [n4] => (item=n3) skipping: [n4] => (item=n4) skipping: [n4] => (item=n5) skipping: [n5] => (item=n1) skipping: [n5] => (item=n2) skipping: [n5] => (item=n3) skipping: [n5] => (item=n4) skipping: [n5] => (item=n5) TASK [ssh : Copy local public key to authorized_keys] ****************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [ssh : Slurp host keys from all nodes] **************************************************************************************************** ok: [n1] ok: [n2] ok: [n3] ok: [n4] ok: [n5] TASK [ssh : Insert all nodes into global known_hosts] ****************************************************************************************** ok: [n1] => (item=n1) ok: [n2] => (item=n1) ok: [n3] => (item=n1) ok: [n4] => (item=n1) ok: [n5] => (item=n1) ok: [n2] => (item=n2) ok: [n3] => (item=n2) ok: [n1] => (item=n2) ok: [n4] => (item=n2) ok: [n5] => (item=n2) ok: [n2] => (item=n3) ok: [n3] => (item=n3) ok: [n4] => (item=n3) ok: [n1] => (item=n3) ok: [n5] => (item=n3) ok: [n2] => (item=n4) ok: [n3] => (item=n4) ok: [n4] => (item=n4) ok: [n1] => (item=n4) ok: [n5] => (item=n4) ok: [n2] => (item=n5) ok: [n3] => (item=n5) ok: [n4] => (item=n5) ok: [n1] => (item=n5) ok: [n5] => (item=n5) TASK [ssh : Secure SSH configuration] ********************************************************************************************************** ok: [n1] => (item={'regexp': '^[# \\\\t]*Port', 'line': 'Port 22'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*Port', 'line': 'Port 22'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*Port', 'line': 'Port 22'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*Port', 'line': 'Port 22'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*Port', 'line': 'Port 22'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*PasswordAuthentication', 'line': 'PasswordAuthentication yes'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*PasswordAuthentication', 'line': 'PasswordAuthentication yes'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*PasswordAuthentication', 'line': 'PasswordAuthentication yes'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*PasswordAuthentication', 'line': 'PasswordAuthentication yes'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*PasswordAuthentication', 'line': 'PasswordAuthentication yes'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*PermitRootLogin', 'line': 'PermitRootLogin no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*PermitRootLogin', 'line': 'PermitRootLogin no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*PermitRootLogin', 'line': 'PermitRootLogin no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*PermitRootLogin', 'line': 'PermitRootLogin no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*PermitRootLogin', 'line': 'PermitRootLogin no'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*UseDNS', 'line': 'UseDNS no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*UseDNS', 'line': 'UseDNS no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*UseDNS', 'line': 'UseDNS no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*UseDNS', 'line': 'UseDNS no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*UseDNS', 'line': 'UseDNS no'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*PermitEmptyPasswords', 'line': 'PermitEmptyPasswords no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*PermitEmptyPasswords', 'line': 'PermitEmptyPasswords no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*PermitEmptyPasswords', 'line': 'PermitEmptyPasswords no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*PermitEmptyPasswords', 'line': 'PermitEmptyPasswords no'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*ChallengeResponseAuthentication', 'line': 'ChallengeResponseAuthentication no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*PermitEmptyPasswords', 'line': 'PermitEmptyPasswords no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*ChallengeResponseAuthentication', 'line': 'ChallengeResponseAuthentication no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*ChallengeResponseAuthentication', 'line': 'ChallengeResponseAuthentication no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*ChallengeResponseAuthentication', 'line': 'ChallengeResponseAuthentication no'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*GSSAPIAuthentication', 'line': 'GSSAPIAuthentication no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*GSSAPIAuthentication', 'line': 'GSSAPIAuthentication no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*ChallengeResponseAuthentication', 'line': 'ChallengeResponseAuthentication no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*GSSAPIAuthentication', 'line': 'GSSAPIAuthentication no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*GSSAPIAuthentication', 'line': 'GSSAPIAuthentication no'}) ok: [n1] => (item={'regexp': '^[# \\\\t]*X11Forwarding', 'line': 'X11Forwarding no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*X11Forwarding', 'line': 'X11Forwarding no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*GSSAPIAuthentication', 'line': 'GSSAPIAuthentication no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*X11Forwarding', 'line': 'X11Forwarding no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*X11Forwarding', 'line': 'X11Forwarding no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*X11Forwarding', 'line': 'X11Forwarding no'}) TASK [ssh : Secure SSH hosts configuration] **************************************************************************************************** ok: [n1] => (item={'regexp': '^[# \\\\t]*HashKnownHosts', 'line': 'HashKnownHosts no'}) ok: [n2] => (item={'regexp': '^[# \\\\t]*HashKnownHosts', 'line': 'HashKnownHosts no'}) ok: [n3] => (item={'regexp': '^[# \\\\t]*HashKnownHosts', 'line': 'HashKnownHosts no'}) ok: [n4] => (item={'regexp': '^[# \\\\t]*HashKnownHosts', 'line': 'HashKnownHosts no'}) ok: [n5] => (item={'regexp': '^[# \\\\t]*HashKnownHosts', 'line': 'HashKnownHosts no'}) PLAY RECAP ************************************************************************************************************************************* n1 : ok=38 changed=8 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 n2 : ok=38 changed=8 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 n3 : ok=38 changed=8 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 n4 : ok=38 changed=8 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 n5 : ok=37 changed=5 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0","title":"Run the ansible playbook provision.yml"},{"location":"pi-stories2/#references","text":"[1] Ansible pi4-cluster-ansible-roles playbook [2] Source code for ARM side libraries for interfacing to Raspberry Pi GPU","title":"References"},{"location":"pi-stories2/#edit_history","text":"initial post on 09/Sep/2020","title":"Edit history"},{"location":"pi-stories3/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Installing k3s software \u00b6 Why k3s? \u00b6 K3s is a fully compliant Kubernetes distribution in a single binary perfectly suitable for smaller edge devices such as the Raspberry PI4. Simply said, you can do almost exactly the same as with its big sister kubernetes (k8s). For these kind of devices it is the perfect match. To get ks3 installed with ansible clone the playbook k3s-ansible playbook [1]. K3s is a product from Rancher Labs and can be installed in different ways, such as with k3sup (read the nice article \" Deploying a highly-available K3s with K3sup \") or with a fork of k3s-ansible sources. We choose for the latter and made some customisation to fork of k3s-ansible playbook [1]. Do check the inventory/my-cluster/hosts.ini file and add you preferences. Also, do not forget to adjust the attributes yaml file inventory/my-cluster/group_vars/all.yml . Especially, check the k3s_version you want to install. To find the latest stable release of k3s see the github release page of ks3s . Run the ansible playbook site.yml \u00b6 gdha@n5:~/projects/k3s-ansible$ ansible-playbook site.yml -i inventory/my-cluster/hosts.ini [WARNING]: While constructing a mapping from /home/gdha/projects/k3s-ansible/roles/ubuntu/tasks/main.yml, line 4, column 5, found a duplicate dict key (backrefs). Using last defined value only. PLAY [k3s_cluster] ******************************************************************************************************************************************************** TASK [Gathering Facts] **************************************************************************************************************************************************** Wednesday 23 September 2020 16:16:13 +0200 (0:00:00.102) 0:00:00.102 *** ok: [192.168.0.202] ok: [192.168.0.204] ok: [192.168.0.203] ok: [192.168.0.201] ok: [192.168.0.205] TASK [prereq : Set SELinux to disabled state] ***************************************************************************************************************************** Wednesday 23 September 2020 16:16:24 +0200 (0:00:10.334) 0:00:10.437 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [prereq : Enable IPv4 forwarding] ************************************************************************************************************************************ Wednesday 23 September 2020 16:16:24 +0200 (0:00:00.913) 0:00:11.350 *** ok: [192.168.0.203] ok: [192.168.0.201] ok: [192.168.0.202] ok: [192.168.0.204] ok: [192.168.0.205] TASK [prereq : Enable IPv6 forwarding] ************************************************************************************************************************************ Wednesday 23 September 2020 16:16:26 +0200 (0:00:01.658) 0:00:13.009 *** ok: [192.168.0.201] ok: [192.168.0.202] ok: [192.168.0.203] ok: [192.168.0.204] ok: [192.168.0.205] TASK [prereq : Add br_netfilter to /etc/modules-load.d/] ****************************************************************************************************************** Wednesday 23 September 2020 16:16:28 +0200 (0:00:01.781) 0:00:14.790 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [prereq : Load br_netfilter] ***************************************************************************************************************************************** Wednesday 23 September 2020 16:16:29 +0200 (0:00:00.905) 0:00:15.696 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [prereq : Set bridge-nf-call-iptables (just to be sure)] ************************************************************************************************************* Wednesday 23 September 2020 16:16:30 +0200 (0:00:00.909) 0:00:16.605 *** skipping: [192.168.0.201] => (item=net.bridge.bridge-nf-call-iptables) skipping: [192.168.0.201] => (item=net.bridge.bridge-nf-call-ip6tables) skipping: [192.168.0.202] => (item=net.bridge.bridge-nf-call-iptables) skipping: [192.168.0.202] => (item=net.bridge.bridge-nf-call-ip6tables) skipping: [192.168.0.203] => (item=net.bridge.bridge-nf-call-iptables) skipping: [192.168.0.203] => (item=net.bridge.bridge-nf-call-ip6tables) skipping: [192.168.0.204] => (item=net.bridge.bridge-nf-call-iptables) skipping: [192.168.0.204] => (item=net.bridge.bridge-nf-call-ip6tables) skipping: [192.168.0.205] => (item=net.bridge.bridge-nf-call-iptables) skipping: [192.168.0.205] => (item=net.bridge.bridge-nf-call-ip6tables) TASK [prereq : Add /usr/local/bin to sudo secure_path] ******************************************************************************************************************** Wednesday 23 September 2020 16:16:31 +0200 (0:00:00.939) 0:00:17.544 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [download : Delete k3s if already present] *************************************************************************************************************************** Wednesday 23 September 2020 16:16:32 +0200 (0:00:01.169) 0:00:18.714 *** changed: [192.168.0.201] changed: [192.168.0.203] changed: [192.168.0.202] changed: [192.168.0.204] changed: [192.168.0.205] TASK [download : Download k3s binary x64] ********************************************************************************************************************************* Wednesday 23 September 2020 16:16:34 +0200 (0:00:01.707) 0:00:20.422 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [download : Download k3s binary arm64] ******************************************************************************************************************************* Wednesday 23 September 2020 16:16:34 +0200 (0:00:00.943) 0:00:21.365 *** changed: [192.168.0.202] changed: [192.168.0.204] changed: [192.168.0.201] changed: [192.168.0.205] changed: [192.168.0.203] TASK [download : Download k3s binary armhf] ******************************************************************************************************************************* Wednesday 23 September 2020 16:17:34 +0200 (0:00:59.059) 0:01:20.425 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [raspbian : Test for Raspbian] *************************************************************************************************************************************** Wednesday 23 September 2020 16:17:34 +0200 (0:00:00.940) 0:01:21.365 *** ok: [192.168.0.201] ok: [192.168.0.202] ok: [192.168.0.203] ok: [192.168.0.204] ok: [192.168.0.205] TASK [raspbian : Activating cgroup support] ******************************************************************************************************************************* Wednesday 23 September 2020 16:17:36 +0200 (0:00:01.165) 0:01:22.531 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [raspbian : Flush iptables before changing to iptables-legacy] ******************************************************************************************************* Wednesday 23 September 2020 16:17:36 +0200 (0:00:00.796) 0:01:23.327 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [raspbian : Changing to iptables-legacy] ***************************************************************************************************************************** Wednesday 23 September 2020 16:17:37 +0200 (0:00:00.795) 0:01:24.123 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [raspbian : Changing to ip6tables-legacy] **************************************************************************************************************************** Wednesday 23 September 2020 16:17:38 +0200 (0:00:00.798) 0:01:24.922 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [raspbian : Rebooting] *********************************************************************************************************************************************** Wednesday 23 September 2020 16:17:39 +0200 (0:00:00.940) 0:01:25.862 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [ubuntu : Enable cgroup via boot commandline if not already enabled for Ubuntu on ARM] ******************************************************************************* Wednesday 23 September 2020 16:17:40 +0200 (0:00:00.800) 0:01:26.663 *** ok: [192.168.0.203] ok: [192.168.0.202] ok: [192.168.0.201] ok: [192.168.0.204] ok: [192.168.0.205] PLAY [master] ************************************************************************************************************************************************************* TASK [Gathering Facts] **************************************************************************************************************************************************** Wednesday 23 September 2020 16:17:42 +0200 (0:00:01.887) 0:01:28.550 *** ok: [192.168.0.201] TASK [k3s/master : Copy K3s service file] ********************************************************************************************************************************* Wednesday 23 September 2020 16:17:49 +0200 (0:00:07.512) 0:01:36.063 *** ok: [192.168.0.201] TASK [k3s/master : Enable and check K3s service] ************************************************************************************************************************** Wednesday 23 September 2020 16:17:51 +0200 (0:00:02.229) 0:01:38.293 *** changed: [192.168.0.201] TASK [k3s/master : Wait for node-token] *********************************************************************************************************************************** Wednesday 23 September 2020 16:18:11 +0200 (0:00:19.552) 0:01:57.846 *** ok: [192.168.0.201] TASK [k3s/master : Register node-token file access mode] ****************************************************************************************************************** Wednesday 23 September 2020 16:18:13 +0200 (0:00:01.661) 0:01:59.508 *** ok: [192.168.0.201] TASK [k3s/master : Change file access node-token] ************************************************************************************************************************* Wednesday 23 September 2020 16:18:14 +0200 (0:00:01.250) 0:02:00.758 *** changed: [192.168.0.201] TASK [k3s/master : Read node-token from master] *************************************************************************************************************************** Wednesday 23 September 2020 16:18:15 +0200 (0:00:01.036) 0:02:01.794 *** ok: [192.168.0.201] TASK [k3s/master : Store Master node-token] ******************************************************************************************************************************* Wednesday 23 September 2020 16:18:16 +0200 (0:00:01.293) 0:02:03.087 *** ok: [192.168.0.201] TASK [k3s/master : Restore node-token file access] ************************************************************************************************************************ Wednesday 23 September 2020 16:18:17 +0200 (0:00:00.277) 0:02:03.365 *** changed: [192.168.0.201] TASK [k3s/master : Create directory .kube] ******************************************************************************************************************************** Wednesday 23 September 2020 16:18:18 +0200 (0:00:01.079) 0:02:04.445 *** ok: [192.168.0.201] TASK [k3s/master : Copy config file to user home directory] *************************************************************************************************************** Wednesday 23 September 2020 16:18:19 +0200 (0:00:01.083) 0:02:05.528 *** changed: [192.168.0.201] TASK [k3s/master : Replace https://localhost:6443 by https://master-ip:6443] ********************************************************************************************** Wednesday 23 September 2020 16:18:20 +0200 (0:00:01.586) 0:02:07.115 *** changed: [192.168.0.201] TASK [k3s/master : Create kubectl symlink] ******************************************************************************************************************************** Wednesday 23 September 2020 16:18:22 +0200 (0:00:02.239) 0:02:09.354 *** ok: [192.168.0.201] TASK [k3s/master : Create crictl symlink] ********************************************************************************************************************************* Wednesday 23 September 2020 16:18:23 +0200 (0:00:00.973) 0:02:10.328 *** ok: [192.168.0.201] PLAY [node] *************************************************************************************************************************************************************** TASK [Gathering Facts] **************************************************************************************************************************************************** Wednesday 23 September 2020 16:18:25 +0200 (0:00:01.351) 0:02:11.680 *** ok: [192.168.0.203] ok: [192.168.0.205] ok: [192.168.0.202] ok: [192.168.0.204] TASK [k3s/node : Copy K3s service file] *********************************************************************************************************************************** Wednesday 23 September 2020 16:18:35 +0200 (0:00:09.865) 0:02:21.546 *** ok: [192.168.0.202] ok: [192.168.0.203] ok: [192.168.0.204] ok: [192.168.0.205] TASK [k3s/node : Enable and check K3s service] **************************************************************************************************************************** Wednesday 23 September 2020 16:18:37 +0200 (0:00:02.715) 0:02:24.261 *** changed: [192.168.0.202] changed: [192.168.0.204] changed: [192.168.0.205] changed: [192.168.0.203] TASK [k3s/node : Create directory .kube] ********************************************************************************************************************************** Wednesday 23 September 2020 16:18:46 +0200 (0:00:08.523) 0:02:32.785 *** ok: [192.168.0.202] ok: [192.168.0.203] ok: [192.168.0.204] ok: [192.168.0.205] TASK [k3s/node : Create kubectl/crictl symlinks] ************************************************************************************************************************** Wednesday 23 September 2020 16:18:48 +0200 (0:00:01.660) 0:02:34.446 *** ok: [192.168.0.202] => (item=kubectl) ok: [192.168.0.203] => (item=kubectl) ok: [192.168.0.204] => (item=kubectl) ok: [192.168.0.202] => (item=crictl) ok: [192.168.0.205] => (item=kubectl) ok: [192.168.0.203] => (item=crictl) ok: [192.168.0.204] => (item=crictl) ok: [192.168.0.205] => (item=crictl) TASK [k3s/node : fetch the ~/.kube/config file] *************************************************************************************************************************** Wednesday 23 September 2020 16:18:50 +0200 (0:00:02.746) 0:02:37.192 *** skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] PLAY RECAP **************************************************************************************************************************************************************** 192.168.0.201 : ok=21 changed=7 unreachable=0 failed=0 skipped=12 rescued=0 ignored=0 192.168.0.202 : ok=12 changed=3 unreachable=0 failed=0 skipped=13 rescued=0 ignored=0 192.168.0.203 : ok=12 changed=3 unreachable=0 failed=0 skipped=13 rescued=0 ignored=0 192.168.0.204 : ok=12 changed=3 unreachable=0 failed=0 skipped=13 rescued=0 ignored=0 192.168.0.205 : ok=12 changed=3 unreachable=0 failed=0 skipped=13 rescued=0 ignored=0 Wednesday 23 September 2020 16:18:51 +0200 (0:00:00.630) 0:02:37.823 *** =============================================================================== download : Download k3s binary arm64 ------------------------------------------------------------------------------------------------------------------------------ 59.06s k3s/master : Enable and check K3s service ------------------------------------------------------------------------------------------------------------------------- 19.55s Gathering Facts --------------------------------------------------------------------------------------------------------------------------------------------------- 10.33s Gathering Facts ---------------------------------------------------------------------------------------------------------------------------------------------------- 9.87s k3s/node : Enable and check K3s service ---------------------------------------------------------------------------------------------------------------------------- 8.52s Gathering Facts ---------------------------------------------------------------------------------------------------------------------------------------------------- 7.51s k3s/node : Create kubectl/crictl symlinks -------------------------------------------------------------------------------------------------------------------------- 2.75s k3s/node : Copy K3s service file ----------------------------------------------------------------------------------------------------------------------------------- 2.72s k3s/master : Replace https://localhost:6443 by https://master-ip:6443 ---------------------------------------------------------------------------------------------- 2.24s k3s/master : Copy K3s service file --------------------------------------------------------------------------------------------------------------------------------- 2.23s ubuntu : Enable cgroup via boot commandline if not already enabled for Ubuntu on ARM ------------------------------------------------------------------------------- 1.89s prereq : Enable IPv6 forwarding ------------------------------------------------------------------------------------------------------------------------------------ 1.78s download : Delete k3s if already present --------------------------------------------------------------------------------------------------------------------------- 1.71s k3s/master : Wait for node-token ----------------------------------------------------------------------------------------------------------------------------------- 1.66s k3s/node : Create directory .kube ---------------------------------------------------------------------------------------------------------------------------------- 1.66s prereq : Enable IPv4 forwarding ------------------------------------------------------------------------------------------------------------------------------------ 1.66s k3s/master : Copy config file to user home directory --------------------------------------------------------------------------------------------------------------- 1.59s k3s/master : Create crictl symlink --------------------------------------------------------------------------------------------------------------------------------- 1.35s k3s/master : Read node-token from master --------------------------------------------------------------------------------------------------------------------------- 1.29s k3s/master : Register node-token file access mode ------------------------------------------------------------------------------------------------------------------ 1.25s k3s is up and running? \u00b6 Wow the installation went rather fast - an exciting moment - is k3s working fine? gdha@n5:~/projects/k3s-ansible$ k3s --version k3s version v1.19.2+k3s1 (d38505b1) gdha@n5:~/projects/k3s-ansible$ k3s kubectl cluster-info To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. The connection to the server localhost:8080 was refused - did you specify the right host or port? Ok - k3s needs the cluster configuration file via variable KUBECONFIG or the file ~/kube/config . We are on a worker node (n5) and choose as master node n1, therefore, the cluster configuration was created and resides on the master node. Or, we copy this to each node or decide only the work from the master node. It is a much better choice to copy the configuration to each node instead. To do so first copy the cluster configuration file from the master node (n1) to this node (n5): gdha@n5:~/projects/k3s-ansible$ scp n1:.kube/config ~/.kube/ config 100% 2793 2.0MB/s 00:00 Now see if we have more luck with a kubernetes command to get all the nodes in this cluster and dumping the cluster-info? gdha@n5:~/projects/k3s-ansible$ kubectl get nodes NAME STATUS ROLES AGE VERSION n2 Ready <none> 49d v1.19.2+k3s1 n5 Ready <none> 49d v1.19.2+k3s1 n4 Ready <none> 49d v1.19.2+k3s1 n3 Ready <none> 49d v1.19.2+k3s1 n1 Ready master 49d v1.19.2+k3s1 gdha@n5:~/projects/k3s-ansible$ k3s kubectl cluster-info Kubernetes master is running at https://192.168.0.201:6443 CoreDNS is running at https://192.168.0.201:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://192.168.0.201:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. Yes, that works fine, so lets copy the configuration file to the other nodes as well: gdha@n5:~/projects/k3s-ansible$ scp ~/.kube/config n2:~/.kube/ config 100% 2793 1.9MB/s 00:00 gdha@n5:~/projects/k3s-ansible$ scp ~/.kube/config n3:~/.kube/ config 100% 2793 1.7MB/s 00:00 gdha@n5:~/projects/k3s-ansible$ scp ~/.kube/config n4:~/.kube/ config And finally, to conclude this story, what pods are running within a basic k3s setup? gdha@n5:~/projects/k3s-ansible$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system helm-install-traefik-p7jkh 0/1 Completed 0 4d22h kube-system svclb-traefik-kxxn4 2/2 Running 2 4d22h kube-system svclb-traefik-r9q6r 2/2 Running 2 4d22h kube-system svclb-traefik-qlgn9 2/2 Running 2 4d22h kube-system traefik-5dd496474-4fwdm 1/1 Running 1 4d22h kube-system local-path-provisioner-7ff9579c6-l6t6s 1/1 Running 1 4d22h kube-system svclb-traefik-n6srr 2/2 Running 2 4d22h kube-system coredns-66c464876b-vqrd6 1/1 Running 1 4d22h kube-system metrics-server-7b4f8b595-bldsd 1/1 Running 1 4d22h kube-system svclb-traefik-74k9f 2/2 Running 2 4d22h References \u00b6 [1] Ansible k3s-ansible playbook [2] Deploying a highly-available K3s with K3sup Edit history \u00b6 initial post on 28/Sep/2020","title":"Raspberry Pi 4 Installing k3s software"},{"location":"pi-stories3/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories3/#raspberry_pi_4_cluster_series_-_installing_k3s_software","text":"","title":"Raspberry Pi 4 cluster Series - Installing k3s software"},{"location":"pi-stories3/#why_k3s","text":"K3s is a fully compliant Kubernetes distribution in a single binary perfectly suitable for smaller edge devices such as the Raspberry PI4. Simply said, you can do almost exactly the same as with its big sister kubernetes (k8s). For these kind of devices it is the perfect match. To get ks3 installed with ansible clone the playbook k3s-ansible playbook [1]. K3s is a product from Rancher Labs and can be installed in different ways, such as with k3sup (read the nice article \" Deploying a highly-available K3s with K3sup \") or with a fork of k3s-ansible sources. We choose for the latter and made some customisation to fork of k3s-ansible playbook [1]. Do check the inventory/my-cluster/hosts.ini file and add you preferences. Also, do not forget to adjust the attributes yaml file inventory/my-cluster/group_vars/all.yml . Especially, check the k3s_version you want to install. To find the latest stable release of k3s see the github release page of ks3s .","title":"Why k3s?"},{"location":"pi-stories3/#run_the_ansible_playbook_siteyml","text":"gdha@n5:~/projects/k3s-ansible$ ansible-playbook site.yml -i inventory/my-cluster/hosts.ini [WARNING]: While constructing a mapping from /home/gdha/projects/k3s-ansible/roles/ubuntu/tasks/main.yml, line 4, column 5, found a duplicate dict key (backrefs). Using last defined value only. PLAY [k3s_cluster] ******************************************************************************************************************************************************** TASK [Gathering Facts] **************************************************************************************************************************************************** Wednesday 23 September 2020 16:16:13 +0200 (0:00:00.102) 0:00:00.102 *** ok: [192.168.0.202] ok: [192.168.0.204] ok: [192.168.0.203] ok: [192.168.0.201] ok: [192.168.0.205] TASK [prereq : Set SELinux to disabled state] ***************************************************************************************************************************** Wednesday 23 September 2020 16:16:24 +0200 (0:00:10.334) 0:00:10.437 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [prereq : Enable IPv4 forwarding] ************************************************************************************************************************************ Wednesday 23 September 2020 16:16:24 +0200 (0:00:00.913) 0:00:11.350 *** ok: [192.168.0.203] ok: [192.168.0.201] ok: [192.168.0.202] ok: [192.168.0.204] ok: [192.168.0.205] TASK [prereq : Enable IPv6 forwarding] ************************************************************************************************************************************ Wednesday 23 September 2020 16:16:26 +0200 (0:00:01.658) 0:00:13.009 *** ok: [192.168.0.201] ok: [192.168.0.202] ok: [192.168.0.203] ok: [192.168.0.204] ok: [192.168.0.205] TASK [prereq : Add br_netfilter to /etc/modules-load.d/] ****************************************************************************************************************** Wednesday 23 September 2020 16:16:28 +0200 (0:00:01.781) 0:00:14.790 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [prereq : Load br_netfilter] ***************************************************************************************************************************************** Wednesday 23 September 2020 16:16:29 +0200 (0:00:00.905) 0:00:15.696 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [prereq : Set bridge-nf-call-iptables (just to be sure)] ************************************************************************************************************* Wednesday 23 September 2020 16:16:30 +0200 (0:00:00.909) 0:00:16.605 *** skipping: [192.168.0.201] => (item=net.bridge.bridge-nf-call-iptables) skipping: [192.168.0.201] => (item=net.bridge.bridge-nf-call-ip6tables) skipping: [192.168.0.202] => (item=net.bridge.bridge-nf-call-iptables) skipping: [192.168.0.202] => (item=net.bridge.bridge-nf-call-ip6tables) skipping: [192.168.0.203] => (item=net.bridge.bridge-nf-call-iptables) skipping: [192.168.0.203] => (item=net.bridge.bridge-nf-call-ip6tables) skipping: [192.168.0.204] => (item=net.bridge.bridge-nf-call-iptables) skipping: [192.168.0.204] => (item=net.bridge.bridge-nf-call-ip6tables) skipping: [192.168.0.205] => (item=net.bridge.bridge-nf-call-iptables) skipping: [192.168.0.205] => (item=net.bridge.bridge-nf-call-ip6tables) TASK [prereq : Add /usr/local/bin to sudo secure_path] ******************************************************************************************************************** Wednesday 23 September 2020 16:16:31 +0200 (0:00:00.939) 0:00:17.544 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [download : Delete k3s if already present] *************************************************************************************************************************** Wednesday 23 September 2020 16:16:32 +0200 (0:00:01.169) 0:00:18.714 *** changed: [192.168.0.201] changed: [192.168.0.203] changed: [192.168.0.202] changed: [192.168.0.204] changed: [192.168.0.205] TASK [download : Download k3s binary x64] ********************************************************************************************************************************* Wednesday 23 September 2020 16:16:34 +0200 (0:00:01.707) 0:00:20.422 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [download : Download k3s binary arm64] ******************************************************************************************************************************* Wednesday 23 September 2020 16:16:34 +0200 (0:00:00.943) 0:00:21.365 *** changed: [192.168.0.202] changed: [192.168.0.204] changed: [192.168.0.201] changed: [192.168.0.205] changed: [192.168.0.203] TASK [download : Download k3s binary armhf] ******************************************************************************************************************************* Wednesday 23 September 2020 16:17:34 +0200 (0:00:59.059) 0:01:20.425 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [raspbian : Test for Raspbian] *************************************************************************************************************************************** Wednesday 23 September 2020 16:17:34 +0200 (0:00:00.940) 0:01:21.365 *** ok: [192.168.0.201] ok: [192.168.0.202] ok: [192.168.0.203] ok: [192.168.0.204] ok: [192.168.0.205] TASK [raspbian : Activating cgroup support] ******************************************************************************************************************************* Wednesday 23 September 2020 16:17:36 +0200 (0:00:01.165) 0:01:22.531 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [raspbian : Flush iptables before changing to iptables-legacy] ******************************************************************************************************* Wednesday 23 September 2020 16:17:36 +0200 (0:00:00.796) 0:01:23.327 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [raspbian : Changing to iptables-legacy] ***************************************************************************************************************************** Wednesday 23 September 2020 16:17:37 +0200 (0:00:00.795) 0:01:24.123 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [raspbian : Changing to ip6tables-legacy] **************************************************************************************************************************** Wednesday 23 September 2020 16:17:38 +0200 (0:00:00.798) 0:01:24.922 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [raspbian : Rebooting] *********************************************************************************************************************************************** Wednesday 23 September 2020 16:17:39 +0200 (0:00:00.940) 0:01:25.862 *** skipping: [192.168.0.201] skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] TASK [ubuntu : Enable cgroup via boot commandline if not already enabled for Ubuntu on ARM] ******************************************************************************* Wednesday 23 September 2020 16:17:40 +0200 (0:00:00.800) 0:01:26.663 *** ok: [192.168.0.203] ok: [192.168.0.202] ok: [192.168.0.201] ok: [192.168.0.204] ok: [192.168.0.205] PLAY [master] ************************************************************************************************************************************************************* TASK [Gathering Facts] **************************************************************************************************************************************************** Wednesday 23 September 2020 16:17:42 +0200 (0:00:01.887) 0:01:28.550 *** ok: [192.168.0.201] TASK [k3s/master : Copy K3s service file] ********************************************************************************************************************************* Wednesday 23 September 2020 16:17:49 +0200 (0:00:07.512) 0:01:36.063 *** ok: [192.168.0.201] TASK [k3s/master : Enable and check K3s service] ************************************************************************************************************************** Wednesday 23 September 2020 16:17:51 +0200 (0:00:02.229) 0:01:38.293 *** changed: [192.168.0.201] TASK [k3s/master : Wait for node-token] *********************************************************************************************************************************** Wednesday 23 September 2020 16:18:11 +0200 (0:00:19.552) 0:01:57.846 *** ok: [192.168.0.201] TASK [k3s/master : Register node-token file access mode] ****************************************************************************************************************** Wednesday 23 September 2020 16:18:13 +0200 (0:00:01.661) 0:01:59.508 *** ok: [192.168.0.201] TASK [k3s/master : Change file access node-token] ************************************************************************************************************************* Wednesday 23 September 2020 16:18:14 +0200 (0:00:01.250) 0:02:00.758 *** changed: [192.168.0.201] TASK [k3s/master : Read node-token from master] *************************************************************************************************************************** Wednesday 23 September 2020 16:18:15 +0200 (0:00:01.036) 0:02:01.794 *** ok: [192.168.0.201] TASK [k3s/master : Store Master node-token] ******************************************************************************************************************************* Wednesday 23 September 2020 16:18:16 +0200 (0:00:01.293) 0:02:03.087 *** ok: [192.168.0.201] TASK [k3s/master : Restore node-token file access] ************************************************************************************************************************ Wednesday 23 September 2020 16:18:17 +0200 (0:00:00.277) 0:02:03.365 *** changed: [192.168.0.201] TASK [k3s/master : Create directory .kube] ******************************************************************************************************************************** Wednesday 23 September 2020 16:18:18 +0200 (0:00:01.079) 0:02:04.445 *** ok: [192.168.0.201] TASK [k3s/master : Copy config file to user home directory] *************************************************************************************************************** Wednesday 23 September 2020 16:18:19 +0200 (0:00:01.083) 0:02:05.528 *** changed: [192.168.0.201] TASK [k3s/master : Replace https://localhost:6443 by https://master-ip:6443] ********************************************************************************************** Wednesday 23 September 2020 16:18:20 +0200 (0:00:01.586) 0:02:07.115 *** changed: [192.168.0.201] TASK [k3s/master : Create kubectl symlink] ******************************************************************************************************************************** Wednesday 23 September 2020 16:18:22 +0200 (0:00:02.239) 0:02:09.354 *** ok: [192.168.0.201] TASK [k3s/master : Create crictl symlink] ********************************************************************************************************************************* Wednesday 23 September 2020 16:18:23 +0200 (0:00:00.973) 0:02:10.328 *** ok: [192.168.0.201] PLAY [node] *************************************************************************************************************************************************************** TASK [Gathering Facts] **************************************************************************************************************************************************** Wednesday 23 September 2020 16:18:25 +0200 (0:00:01.351) 0:02:11.680 *** ok: [192.168.0.203] ok: [192.168.0.205] ok: [192.168.0.202] ok: [192.168.0.204] TASK [k3s/node : Copy K3s service file] *********************************************************************************************************************************** Wednesday 23 September 2020 16:18:35 +0200 (0:00:09.865) 0:02:21.546 *** ok: [192.168.0.202] ok: [192.168.0.203] ok: [192.168.0.204] ok: [192.168.0.205] TASK [k3s/node : Enable and check K3s service] **************************************************************************************************************************** Wednesday 23 September 2020 16:18:37 +0200 (0:00:02.715) 0:02:24.261 *** changed: [192.168.0.202] changed: [192.168.0.204] changed: [192.168.0.205] changed: [192.168.0.203] TASK [k3s/node : Create directory .kube] ********************************************************************************************************************************** Wednesday 23 September 2020 16:18:46 +0200 (0:00:08.523) 0:02:32.785 *** ok: [192.168.0.202] ok: [192.168.0.203] ok: [192.168.0.204] ok: [192.168.0.205] TASK [k3s/node : Create kubectl/crictl symlinks] ************************************************************************************************************************** Wednesday 23 September 2020 16:18:48 +0200 (0:00:01.660) 0:02:34.446 *** ok: [192.168.0.202] => (item=kubectl) ok: [192.168.0.203] => (item=kubectl) ok: [192.168.0.204] => (item=kubectl) ok: [192.168.0.202] => (item=crictl) ok: [192.168.0.205] => (item=kubectl) ok: [192.168.0.203] => (item=crictl) ok: [192.168.0.204] => (item=crictl) ok: [192.168.0.205] => (item=crictl) TASK [k3s/node : fetch the ~/.kube/config file] *************************************************************************************************************************** Wednesday 23 September 2020 16:18:50 +0200 (0:00:02.746) 0:02:37.192 *** skipping: [192.168.0.202] skipping: [192.168.0.203] skipping: [192.168.0.204] skipping: [192.168.0.205] PLAY RECAP **************************************************************************************************************************************************************** 192.168.0.201 : ok=21 changed=7 unreachable=0 failed=0 skipped=12 rescued=0 ignored=0 192.168.0.202 : ok=12 changed=3 unreachable=0 failed=0 skipped=13 rescued=0 ignored=0 192.168.0.203 : ok=12 changed=3 unreachable=0 failed=0 skipped=13 rescued=0 ignored=0 192.168.0.204 : ok=12 changed=3 unreachable=0 failed=0 skipped=13 rescued=0 ignored=0 192.168.0.205 : ok=12 changed=3 unreachable=0 failed=0 skipped=13 rescued=0 ignored=0 Wednesday 23 September 2020 16:18:51 +0200 (0:00:00.630) 0:02:37.823 *** =============================================================================== download : Download k3s binary arm64 ------------------------------------------------------------------------------------------------------------------------------ 59.06s k3s/master : Enable and check K3s service ------------------------------------------------------------------------------------------------------------------------- 19.55s Gathering Facts --------------------------------------------------------------------------------------------------------------------------------------------------- 10.33s Gathering Facts ---------------------------------------------------------------------------------------------------------------------------------------------------- 9.87s k3s/node : Enable and check K3s service ---------------------------------------------------------------------------------------------------------------------------- 8.52s Gathering Facts ---------------------------------------------------------------------------------------------------------------------------------------------------- 7.51s k3s/node : Create kubectl/crictl symlinks -------------------------------------------------------------------------------------------------------------------------- 2.75s k3s/node : Copy K3s service file ----------------------------------------------------------------------------------------------------------------------------------- 2.72s k3s/master : Replace https://localhost:6443 by https://master-ip:6443 ---------------------------------------------------------------------------------------------- 2.24s k3s/master : Copy K3s service file --------------------------------------------------------------------------------------------------------------------------------- 2.23s ubuntu : Enable cgroup via boot commandline if not already enabled for Ubuntu on ARM ------------------------------------------------------------------------------- 1.89s prereq : Enable IPv6 forwarding ------------------------------------------------------------------------------------------------------------------------------------ 1.78s download : Delete k3s if already present --------------------------------------------------------------------------------------------------------------------------- 1.71s k3s/master : Wait for node-token ----------------------------------------------------------------------------------------------------------------------------------- 1.66s k3s/node : Create directory .kube ---------------------------------------------------------------------------------------------------------------------------------- 1.66s prereq : Enable IPv4 forwarding ------------------------------------------------------------------------------------------------------------------------------------ 1.66s k3s/master : Copy config file to user home directory --------------------------------------------------------------------------------------------------------------- 1.59s k3s/master : Create crictl symlink --------------------------------------------------------------------------------------------------------------------------------- 1.35s k3s/master : Read node-token from master --------------------------------------------------------------------------------------------------------------------------- 1.29s k3s/master : Register node-token file access mode ------------------------------------------------------------------------------------------------------------------ 1.25s","title":"Run the ansible playbook site.yml"},{"location":"pi-stories3/#k3s_is_up_and_running","text":"Wow the installation went rather fast - an exciting moment - is k3s working fine? gdha@n5:~/projects/k3s-ansible$ k3s --version k3s version v1.19.2+k3s1 (d38505b1) gdha@n5:~/projects/k3s-ansible$ k3s kubectl cluster-info To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. The connection to the server localhost:8080 was refused - did you specify the right host or port? Ok - k3s needs the cluster configuration file via variable KUBECONFIG or the file ~/kube/config . We are on a worker node (n5) and choose as master node n1, therefore, the cluster configuration was created and resides on the master node. Or, we copy this to each node or decide only the work from the master node. It is a much better choice to copy the configuration to each node instead. To do so first copy the cluster configuration file from the master node (n1) to this node (n5): gdha@n5:~/projects/k3s-ansible$ scp n1:.kube/config ~/.kube/ config 100% 2793 2.0MB/s 00:00 Now see if we have more luck with a kubernetes command to get all the nodes in this cluster and dumping the cluster-info? gdha@n5:~/projects/k3s-ansible$ kubectl get nodes NAME STATUS ROLES AGE VERSION n2 Ready <none> 49d v1.19.2+k3s1 n5 Ready <none> 49d v1.19.2+k3s1 n4 Ready <none> 49d v1.19.2+k3s1 n3 Ready <none> 49d v1.19.2+k3s1 n1 Ready master 49d v1.19.2+k3s1 gdha@n5:~/projects/k3s-ansible$ k3s kubectl cluster-info Kubernetes master is running at https://192.168.0.201:6443 CoreDNS is running at https://192.168.0.201:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://192.168.0.201:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. Yes, that works fine, so lets copy the configuration file to the other nodes as well: gdha@n5:~/projects/k3s-ansible$ scp ~/.kube/config n2:~/.kube/ config 100% 2793 1.9MB/s 00:00 gdha@n5:~/projects/k3s-ansible$ scp ~/.kube/config n3:~/.kube/ config 100% 2793 1.7MB/s 00:00 gdha@n5:~/projects/k3s-ansible$ scp ~/.kube/config n4:~/.kube/ config And finally, to conclude this story, what pods are running within a basic k3s setup? gdha@n5:~/projects/k3s-ansible$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system helm-install-traefik-p7jkh 0/1 Completed 0 4d22h kube-system svclb-traefik-kxxn4 2/2 Running 2 4d22h kube-system svclb-traefik-r9q6r 2/2 Running 2 4d22h kube-system svclb-traefik-qlgn9 2/2 Running 2 4d22h kube-system traefik-5dd496474-4fwdm 1/1 Running 1 4d22h kube-system local-path-provisioner-7ff9579c6-l6t6s 1/1 Running 1 4d22h kube-system svclb-traefik-n6srr 2/2 Running 2 4d22h kube-system coredns-66c464876b-vqrd6 1/1 Running 1 4d22h kube-system metrics-server-7b4f8b595-bldsd 1/1 Running 1 4d22h kube-system svclb-traefik-74k9f 2/2 Running 2 4d22h","title":"k3s is up and running?"},{"location":"pi-stories3/#references","text":"[1] Ansible k3s-ansible playbook [2] Deploying a highly-available K3s with K3sup","title":"References"},{"location":"pi-stories3/#edit_history","text":"initial post on 28/Sep/2020","title":"Edit history"},{"location":"pi-stories4/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Install awesome kubectl aliases \u00b6 The article \" Awesome Kubernetes Command-Line Hacks \" points out some interesting items to take in account. In summary activate the autocompletion and generate the kubectl aliases to make your life better. Enable kubectl autocompletion \u00b6 Enable kubectl autocompletion is one of the first things you have to do to make your life a bit easier. There are two ways in which you can do this: Source the completion script in your ~/.bashrc file: echo 'source <(kubectl completion bash)' >>~/.bashrc Add the completion script to the /etc/bash_completion.d directory: kubectl completion bash >/etc/bash_completion.d/kubectl If you have an alias for kubectl, you can extend shell completion to work with that alias: echo 'alias k=kubectl' >>~/.bashrc echo 'complete -F __start_kubectl k' >>~/.bashrc Above information come from kubernetes kubectl page . If you are alone then go for the first option, if more users require these autocompletion then go for the latter option. Kubectl aliases \u00b6 Kubectl commands can be long and hard to type over and over again, therefore, why not apply the kubectl-aliases github project . References \u00b6 [1] Awesome Kubernetes Command-Line Hacks [2] kubectl-aliases GitHub Source Edit history \u00b6 initial post on 30/Sep/2020","title":"Raspberry Pi 4 Install awesome kubectl aliases"},{"location":"pi-stories4/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories4/#raspberry_pi_4_cluster_series_-_install_awesome_kubectl_aliases","text":"The article \" Awesome Kubernetes Command-Line Hacks \" points out some interesting items to take in account. In summary activate the autocompletion and generate the kubectl aliases to make your life better.","title":"Raspberry Pi 4 cluster Series - Install awesome kubectl aliases"},{"location":"pi-stories4/#enable_kubectl_autocompletion","text":"Enable kubectl autocompletion is one of the first things you have to do to make your life a bit easier. There are two ways in which you can do this: Source the completion script in your ~/.bashrc file: echo 'source <(kubectl completion bash)' >>~/.bashrc Add the completion script to the /etc/bash_completion.d directory: kubectl completion bash >/etc/bash_completion.d/kubectl If you have an alias for kubectl, you can extend shell completion to work with that alias: echo 'alias k=kubectl' >>~/.bashrc echo 'complete -F __start_kubectl k' >>~/.bashrc Above information come from kubernetes kubectl page . If you are alone then go for the first option, if more users require these autocompletion then go for the latter option.","title":"Enable kubectl autocompletion"},{"location":"pi-stories4/#kubectl_aliases","text":"Kubectl commands can be long and hard to type over and over again, therefore, why not apply the kubectl-aliases github project .","title":"Kubectl aliases"},{"location":"pi-stories4/#references","text":"[1] Awesome Kubernetes Command-Line Hacks [2] kubectl-aliases GitHub Source","title":"References"},{"location":"pi-stories4/#edit_history","text":"initial post on 30/Sep/2020","title":"Edit history"},{"location":"pi-stories5/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Installing cert-manager on the k3s cluster \u00b6 As certificates are crucial in a kuberbetes cluster one of the first pods that one should install is cert-manager . Installling cert-manager \u00b6 Installaion is extremelt easy with the following command: kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.0.4/cert-manager.yaml In the time of writing this article the current version was v1.0.4 - you can change that to the latest release available of course. Here follows an example of the instalaltion of cert-manager: $ kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.0.4/cert-manager.yaml customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created namespace/cert-manager created serviceaccount/cert-manager-cainjector created serviceaccount/cert-manager created serviceaccount/cert-manager-webhook created clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created clusterrole.rbac.authorization.k8s.io/cert-manager-view created clusterrole.rbac.authorization.k8s.io/cert-manager-edit created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created role.rbac.authorization.k8s.io/cert-manager:leaderelection created role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection created rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created service/cert-manager created service/cert-manager-webhook created deployment.apps/cert-manager-cainjector created deployment.apps/cert-manager created deployment.apps/cert-manager-webhook created mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created Just after previous command check if the cert-manager pods are created: $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system helm-install-traefik-p7jkh 0/1 Completed 0 49d kube-system metrics-server-7b4f8b595-bldsd 1/1 Running 3 49d kube-system local-path-provisioner-7ff9579c6-l6t6s 1/1 Running 3 49d kube-system svclb-traefik-r9q6r 2/2 Running 6 49d kube-system svclb-traefik-n6srr 2/2 Running 6 49d kube-system svclb-traefik-kxxn4 2/2 Running 6 49d kube-system coredns-66c464876b-vqrd6 1/1 Running 3 49d kube-system svclb-traefik-74k9f 2/2 Running 6 49d kube-system svclb-traefik-qlgn9 2/2 Running 6 49d kube-system traefik-5dd496474-4fwdm 1/1 Running 3 49d cert-manager cert-manager-86548b886-4xrbj 0/1 ContainerCreating 0 9s cert-manager cert-manager-cainjector-6d59c8d4f7-b2vdc 0/1 ContainerCreating 0 9s cert-manager cert-manager-webhook-578954cdd-lg5m4 0/1 ContainerCreating 0 9s After a minute or so check again with the wide option to see on which worker nodes the cert-managers pods are running: $ kubectl get pods -n cert-manager -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cert-manager-cainjector-6d59c8d4f7-b2vdc 1/1 Running 0 21m 10.42.1.17 n5 <none> <none> cert-manager-webhook-578954cdd-lg5m4 1/1 Running 0 21m 10.42.0.19 n1 <none> <none> cert-manager-86548b886-4xrbj 1/1 Running 0 21m 10.42.5.10 n4 <none> <none> References \u00b6 cert-manager documentation cert-manager sources","title":"Raspberry Pi 4 Installing cert-manager on our cluster"},{"location":"pi-stories5/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories5/#raspberry_pi_4_cluster_series_-_installing_cert-manager_on_the_k3s_cluster","text":"As certificates are crucial in a kuberbetes cluster one of the first pods that one should install is cert-manager .","title":"Raspberry Pi 4 cluster Series - Installing cert-manager on the k3s cluster"},{"location":"pi-stories5/#installling_cert-manager","text":"Installaion is extremelt easy with the following command: kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.0.4/cert-manager.yaml In the time of writing this article the current version was v1.0.4 - you can change that to the latest release available of course. Here follows an example of the instalaltion of cert-manager: $ kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.0.4/cert-manager.yaml customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created namespace/cert-manager created serviceaccount/cert-manager-cainjector created serviceaccount/cert-manager created serviceaccount/cert-manager-webhook created clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created clusterrole.rbac.authorization.k8s.io/cert-manager-view created clusterrole.rbac.authorization.k8s.io/cert-manager-edit created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created role.rbac.authorization.k8s.io/cert-manager:leaderelection created role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection created rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created service/cert-manager created service/cert-manager-webhook created deployment.apps/cert-manager-cainjector created deployment.apps/cert-manager created deployment.apps/cert-manager-webhook created mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created Just after previous command check if the cert-manager pods are created: $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system helm-install-traefik-p7jkh 0/1 Completed 0 49d kube-system metrics-server-7b4f8b595-bldsd 1/1 Running 3 49d kube-system local-path-provisioner-7ff9579c6-l6t6s 1/1 Running 3 49d kube-system svclb-traefik-r9q6r 2/2 Running 6 49d kube-system svclb-traefik-n6srr 2/2 Running 6 49d kube-system svclb-traefik-kxxn4 2/2 Running 6 49d kube-system coredns-66c464876b-vqrd6 1/1 Running 3 49d kube-system svclb-traefik-74k9f 2/2 Running 6 49d kube-system svclb-traefik-qlgn9 2/2 Running 6 49d kube-system traefik-5dd496474-4fwdm 1/1 Running 3 49d cert-manager cert-manager-86548b886-4xrbj 0/1 ContainerCreating 0 9s cert-manager cert-manager-cainjector-6d59c8d4f7-b2vdc 0/1 ContainerCreating 0 9s cert-manager cert-manager-webhook-578954cdd-lg5m4 0/1 ContainerCreating 0 9s After a minute or so check again with the wide option to see on which worker nodes the cert-managers pods are running: $ kubectl get pods -n cert-manager -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cert-manager-cainjector-6d59c8d4f7-b2vdc 1/1 Running 0 21m 10.42.1.17 n5 <none> <none> cert-manager-webhook-578954cdd-lg5m4 1/1 Running 0 21m 10.42.0.19 n1 <none> <none> cert-manager-86548b886-4xrbj 1/1 Running 0 21m 10.42.5.10 n4 <none> <none>","title":"Installling cert-manager"},{"location":"pi-stories5/#references","text":"cert-manager documentation cert-manager sources","title":"References"},{"location":"pi-stories6/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Upgrading k3s software on your cluster \u00b6 It is advisable to track the security vulnarabilities published by Rancher Labs around k3s. For example, in November 2020 a critical bug was detected in k3s (see [1]). Therefore, it is quite important to be able to update k3s without interupting the k3s cluster, hence this procedure from Rancher Labs. $ kubectl get nodes NAME STATUS ROLES AGE VERSION n3 Ready <none> 117d v1.19.2+k3s1 n2 Ready <none> 117d v1.19.2+k3s1 n4 Ready <none> 117d v1.19.2+k3s1 n1 Ready master 117d v1.19.2+k3s1 n5 Ready <none> 117d v1.19.2+k3s1 CRD installation \u00b6 We will follow the procedure described in [2] and are required first to install a kubernetes Custom Resource Definition (CRD) [3] followed by creating a Plan. To find the latest version of the system upgrade controller use the following command: curl -s \"https://api.github.com/repos/rancher/system-upgrade-controller/releases/latest\" | awk -F '\"' '/tag_name/{print $4}' v0.6.2 To download the CRD locally run the following command: $ wget https://raw.githubusercontent.com/rancher/system-upgrade-controller/v0.6.2/manifests/system-upgrade-controller.yaml Now get it applied by: $ kubectl apply -f ./system-upgrade-controller.yaml namespace/system-upgrade created serviceaccount/system-upgrade created clusterrolebinding.rbac.authorization.k8s.io/system-upgrade created configmap/default-controller-env created deployment.apps/system-upgrade-controller created $ kubectl get all -n system-upgrade NAME READY STATUS RESTARTS AGE pod/system-upgrade-controller-556df575dd-2qfrs 1/1 Running 0 17s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/system-upgrade-controller 1/1 1 1 17s NAME DESIRED CURRENT READY AGE replicaset.apps/system-upgrade-controller-556df575dd 1 1 1 18s How to Upgrade the CRD \u00b6 $ curl -s \"https://api.github.com/repos/rancher/system-upgrade-controller/releases/latest\" | awk -F '\"' '/tag_name/{print $4}' v0.9.1 $ wget https://raw.githubusercontent.com/rancher/system-upgrade-controller/v0.9.1/manifests/system-upgrade-controller.yaml $ kubectl replace -f ./system-upgrade-controller.yaml Making a k3s upgrade plan \u00b6 Before making a plan we need to decide to which k3s version we need to upgrade, therefore, check out the GitHub release page of k3s . The latest release of this writing was v1.19.4+k3s1 (30 November 2020). The upgrade plan will upgrade the k3s server node (called k3s-server in the plan) and the k3s worker nodes (called k3s-agent in the plan). For that reason we must first label our master node (in our case n1 ) if that was not yet done: $ kubectl get node --selector='node-role.kubernetes.io/master' NAME STATUS ROLES AGE VERSION n1 Ready master 117d v1.19.2+k3s1 Here we see that node n1 was already labelled 'master', however, if that was not yet the case we could realize this by: kubectl label node n1 node-role.kubernetes.io/master=true Apply the plan: $ kubectl apply -f ./k3s-upgrade-plan.yaml plan.upgrade.cattle.io/k3s-server created plan.upgrade.cattle.io/k3s-agent created Check out if the Plans were added correctly: $ kubectl describe plans.upgrade.cattle.io -n system-upgrade Name: k3s-server Namespace: system-upgrade Labels: k3s-upgrade=server Annotations: <none> API Version: upgrade.cattle.io/v1 Kind: Plan Metadata: Creation Timestamp: 2020-11-30T10:53:24Z Generation: 1 Managed Fields: API Version: upgrade.cattle.io/v1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:labels: .: f:k3s-upgrade: f:spec: .: f:concurrency: f:cordon: f:nodeSelector: .: f:matchExpressions: f:serviceAccountName: f:upgrade: .: f:image: f:version: Manager: kubectl-client-side-apply Operation: Update Time: 2020-11-30T10:53:24Z API Version: upgrade.cattle.io/v1 Fields Type: FieldsV1 fieldsV1: f:status: .: f:conditions: f:latestHash: f:latestVersion: Manager: system-upgrade-controller Operation: Update Time: 2020-11-30T10:53:24Z Resource Version: 190476 Self Link: /apis/upgrade.cattle.io/v1/namespaces/system-upgrade/plans/k3s-server UID: d8bb36e7-7602-4e8f-bda7-b75947752eb1 Spec: Concurrency: 1 Cordon: true Node Selector: Match Expressions: Key: k3s-upgrade Operator: Exists Key: k3s-upgrade Operator: NotIn Values: disabled false Key: k3s.io/hostname Operator: Exists Key: k3os.io/mode Operator: DoesNotExist Key: node-role.kubernetes.io/master Operator: In Values: true Service Account Name: system-upgrade Upgrade: Image: rancher/k3s-upgrade Version: v1.19.4+k3s1 Status: Conditions: Last Update Time: 2020-11-30T10:53:24Z Reason: Version Status: True Type: LatestResolved Latest Hash: e50d232791db24fa7ce5039d6f9cf61238b420aacf2ecd32db7cfce3 Latest Version: v1.19.4-k3s1 Events: <none> Name: k3s-agent Namespace: system-upgrade Labels: k3s-upgrade=agent Annotations: <none> API Version: upgrade.cattle.io/v1 Kind: Plan Metadata: Creation Timestamp: 2020-11-30T10:53:24Z Generation: 1 Managed Fields: API Version: upgrade.cattle.io/v1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:labels: .: f:k3s-upgrade: f:spec: .: f:concurrency: f:drain: .: f:force: f:nodeSelector: .: f:matchExpressions: f:prepare: .: f:args: f:image: f:serviceAccountName: f:upgrade: .: f:image: f:version: Manager: kubectl-client-side-apply Operation: Update Time: 2020-11-30T10:53:24Z API Version: upgrade.cattle.io/v1 Fields Type: FieldsV1 fieldsV1: f:status: .: f:conditions: f:latestHash: f:latestVersion: Manager: system-upgrade-controller Operation: Update Time: 2020-11-30T10:53:24Z Resource Version: 190477 Self Link: /apis/upgrade.cattle.io/v1/namespaces/system-upgrade/plans/k3s-agent UID: db4567a7-f80c-44eb-b5dd-0505159ace87 Spec: Concurrency: 2 Drain: Force: true Node Selector: Match Expressions: Key: k3s-upgrade Operator: Exists Key: k3s-upgrade Operator: NotIn Values: disabled false Key: k3s.io/hostname Operator: Exists Key: k3os.io/mode Operator: DoesNotExist Key: node-role.kubernetes.io/master Operator: NotIn Values: true Prepare: Args: prepare k3s-server Image: rancher/k3s-upgrade Service Account Name: system-upgrade Upgrade: Image: rancher/k3s-upgrade Version: v1.19.4+k3s1 Status: Conditions: Last Update Time: 2020-11-30T10:53:24Z Reason: Version Status: True Type: LatestResolved Latest Hash: e50d232791db24fa7ce5039d6f9cf61238b420aacf2ecd32db7cfce3 Latest Version: v1.19.4-k3s1 Events: <none> And, for the magic to happen we just have to enable to k3s upgrade with command: $ kubectl label node --all k3s-upgrade=enabled node/n2 labeled node/n4 labeled node/n3 labeled node/n1 labeled node/n5 labeled To check if the k3s upgrades are kicking in watch with: $ kubectl get pods -n system-upgrade -w NAME READY STATUS RESTARTS AGE system-upgrade-controller-556df575dd-2qfrs 1/1 Running 0 109m apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Init:0/2 0 73s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Init:0/2 0 73s apply-k3s-server-on-n1-with-e50d232791db24fa7ce5039d6f9cf-2j42m 1/1 Running 0 73s apply-k3s-server-on-n1-with-e50d232791db24fa7ce5039d6f9cf-2j42m 0/1 Completed 0 75s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Init:1/2 0 89s apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Init:1/2 0 92s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Init:1/2 0 104s apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Init:1/2 0 107s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 PodInitializing 0 108s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 1/1 Running 0 110s apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 PodInitializing 0 110s apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 1/1 Running 0 112s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Completed 0 2m19s apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Completed 0 2m21s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Pending 0 0s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Pending 0 0s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Pending 0 0s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Pending 0 0s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Init:0/2 0 0s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Init:0/2 0 0s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Init:0/2 0 18s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Init:0/2 0 30s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Init:1/2 0 33s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Init:1/2 0 42s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Init:1/2 0 47s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Init:1/2 0 61s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 PodInitializing 0 70s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 PodInitializing 0 77s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 1/1 Running 0 79s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 1/1 Running 0 79s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Completed 0 107s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Completed 0 109s apply-k3s-server-on-n1-with-e50d232791db24fa7ce5039d6f9cf-2j42m 0/1 Terminating 0 16m apply-k3s-server-on-n1-with-e50d232791db24fa7ce5039d6f9cf-2j42m 0/1 Terminating 0 16m apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Terminating 0 17m apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Terminating 0 17m apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Terminating 0 17m apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Terminating 0 17m apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Terminating 0 16m apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Terminating 0 16m apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Terminating 0 16m apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Terminating 0 16m As you can see from the sequence first the k3s-server gets updated and thereafter, 2 worker nodes at a time as we requested in the plan description. After a couple of minutes we see: $ kubectl get nodes NAME STATUS ROLES AGE VERSION n1 Ready master 117d v1.19.4+k3s1 n2 Ready <none> 117d v1.19.4+k3s1 n4 Ready <none> 117d v1.19.4+k3s1 n3 Ready <none> 117d v1.19.4+k3s1 n5 Ready <none> 117d v1.19.4+k3s1 We believe it is better to disable the k3s upgrades once it is done with the command: $ kubectl label node --all --overwrite k3s-upgrade=disabled node/n5 labeled node/n1 labeled node/n2 labeled node/n4 labeled node/n3 labeled If we want to upgrade again just edit the plan [4] again with the correct version of k3s and replace the plan with the command: $ kubectl replace -f ./k3s-upgrade-plan.yaml To start the k3s version upgrade overwrite the label k3s-upgrade again with keyword enabled . References \u00b6 [1] Rancher Operational Advisory: Attention All Rancher K3s Customers, DB bug requires upgrade [2] Automate K3s Upgrades with System Upgrade Controller [3] CRD system-upgrade-controller [4] k3s Upgrade Plan [5] GitHub Sources of our k3s upgrade controller","title":"Raspberry Pi 4 Upgrading k3s software on your cluster"},{"location":"pi-stories6/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories6/#raspberry_pi_4_cluster_series_-_upgrading_k3s_software_on_your_cluster","text":"It is advisable to track the security vulnarabilities published by Rancher Labs around k3s. For example, in November 2020 a critical bug was detected in k3s (see [1]). Therefore, it is quite important to be able to update k3s without interupting the k3s cluster, hence this procedure from Rancher Labs. $ kubectl get nodes NAME STATUS ROLES AGE VERSION n3 Ready <none> 117d v1.19.2+k3s1 n2 Ready <none> 117d v1.19.2+k3s1 n4 Ready <none> 117d v1.19.2+k3s1 n1 Ready master 117d v1.19.2+k3s1 n5 Ready <none> 117d v1.19.2+k3s1","title":"Raspberry Pi 4 cluster Series - Upgrading k3s software on your cluster"},{"location":"pi-stories6/#crd_installation","text":"We will follow the procedure described in [2] and are required first to install a kubernetes Custom Resource Definition (CRD) [3] followed by creating a Plan. To find the latest version of the system upgrade controller use the following command: curl -s \"https://api.github.com/repos/rancher/system-upgrade-controller/releases/latest\" | awk -F '\"' '/tag_name/{print $4}' v0.6.2 To download the CRD locally run the following command: $ wget https://raw.githubusercontent.com/rancher/system-upgrade-controller/v0.6.2/manifests/system-upgrade-controller.yaml Now get it applied by: $ kubectl apply -f ./system-upgrade-controller.yaml namespace/system-upgrade created serviceaccount/system-upgrade created clusterrolebinding.rbac.authorization.k8s.io/system-upgrade created configmap/default-controller-env created deployment.apps/system-upgrade-controller created $ kubectl get all -n system-upgrade NAME READY STATUS RESTARTS AGE pod/system-upgrade-controller-556df575dd-2qfrs 1/1 Running 0 17s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/system-upgrade-controller 1/1 1 1 17s NAME DESIRED CURRENT READY AGE replicaset.apps/system-upgrade-controller-556df575dd 1 1 1 18s","title":"CRD installation"},{"location":"pi-stories6/#how_to_upgrade_the_crd","text":"$ curl -s \"https://api.github.com/repos/rancher/system-upgrade-controller/releases/latest\" | awk -F '\"' '/tag_name/{print $4}' v0.9.1 $ wget https://raw.githubusercontent.com/rancher/system-upgrade-controller/v0.9.1/manifests/system-upgrade-controller.yaml $ kubectl replace -f ./system-upgrade-controller.yaml","title":"How to Upgrade the CRD"},{"location":"pi-stories6/#making_a_k3s_upgrade_plan","text":"Before making a plan we need to decide to which k3s version we need to upgrade, therefore, check out the GitHub release page of k3s . The latest release of this writing was v1.19.4+k3s1 (30 November 2020). The upgrade plan will upgrade the k3s server node (called k3s-server in the plan) and the k3s worker nodes (called k3s-agent in the plan). For that reason we must first label our master node (in our case n1 ) if that was not yet done: $ kubectl get node --selector='node-role.kubernetes.io/master' NAME STATUS ROLES AGE VERSION n1 Ready master 117d v1.19.2+k3s1 Here we see that node n1 was already labelled 'master', however, if that was not yet the case we could realize this by: kubectl label node n1 node-role.kubernetes.io/master=true Apply the plan: $ kubectl apply -f ./k3s-upgrade-plan.yaml plan.upgrade.cattle.io/k3s-server created plan.upgrade.cattle.io/k3s-agent created Check out if the Plans were added correctly: $ kubectl describe plans.upgrade.cattle.io -n system-upgrade Name: k3s-server Namespace: system-upgrade Labels: k3s-upgrade=server Annotations: <none> API Version: upgrade.cattle.io/v1 Kind: Plan Metadata: Creation Timestamp: 2020-11-30T10:53:24Z Generation: 1 Managed Fields: API Version: upgrade.cattle.io/v1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:labels: .: f:k3s-upgrade: f:spec: .: f:concurrency: f:cordon: f:nodeSelector: .: f:matchExpressions: f:serviceAccountName: f:upgrade: .: f:image: f:version: Manager: kubectl-client-side-apply Operation: Update Time: 2020-11-30T10:53:24Z API Version: upgrade.cattle.io/v1 Fields Type: FieldsV1 fieldsV1: f:status: .: f:conditions: f:latestHash: f:latestVersion: Manager: system-upgrade-controller Operation: Update Time: 2020-11-30T10:53:24Z Resource Version: 190476 Self Link: /apis/upgrade.cattle.io/v1/namespaces/system-upgrade/plans/k3s-server UID: d8bb36e7-7602-4e8f-bda7-b75947752eb1 Spec: Concurrency: 1 Cordon: true Node Selector: Match Expressions: Key: k3s-upgrade Operator: Exists Key: k3s-upgrade Operator: NotIn Values: disabled false Key: k3s.io/hostname Operator: Exists Key: k3os.io/mode Operator: DoesNotExist Key: node-role.kubernetes.io/master Operator: In Values: true Service Account Name: system-upgrade Upgrade: Image: rancher/k3s-upgrade Version: v1.19.4+k3s1 Status: Conditions: Last Update Time: 2020-11-30T10:53:24Z Reason: Version Status: True Type: LatestResolved Latest Hash: e50d232791db24fa7ce5039d6f9cf61238b420aacf2ecd32db7cfce3 Latest Version: v1.19.4-k3s1 Events: <none> Name: k3s-agent Namespace: system-upgrade Labels: k3s-upgrade=agent Annotations: <none> API Version: upgrade.cattle.io/v1 Kind: Plan Metadata: Creation Timestamp: 2020-11-30T10:53:24Z Generation: 1 Managed Fields: API Version: upgrade.cattle.io/v1 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:labels: .: f:k3s-upgrade: f:spec: .: f:concurrency: f:drain: .: f:force: f:nodeSelector: .: f:matchExpressions: f:prepare: .: f:args: f:image: f:serviceAccountName: f:upgrade: .: f:image: f:version: Manager: kubectl-client-side-apply Operation: Update Time: 2020-11-30T10:53:24Z API Version: upgrade.cattle.io/v1 Fields Type: FieldsV1 fieldsV1: f:status: .: f:conditions: f:latestHash: f:latestVersion: Manager: system-upgrade-controller Operation: Update Time: 2020-11-30T10:53:24Z Resource Version: 190477 Self Link: /apis/upgrade.cattle.io/v1/namespaces/system-upgrade/plans/k3s-agent UID: db4567a7-f80c-44eb-b5dd-0505159ace87 Spec: Concurrency: 2 Drain: Force: true Node Selector: Match Expressions: Key: k3s-upgrade Operator: Exists Key: k3s-upgrade Operator: NotIn Values: disabled false Key: k3s.io/hostname Operator: Exists Key: k3os.io/mode Operator: DoesNotExist Key: node-role.kubernetes.io/master Operator: NotIn Values: true Prepare: Args: prepare k3s-server Image: rancher/k3s-upgrade Service Account Name: system-upgrade Upgrade: Image: rancher/k3s-upgrade Version: v1.19.4+k3s1 Status: Conditions: Last Update Time: 2020-11-30T10:53:24Z Reason: Version Status: True Type: LatestResolved Latest Hash: e50d232791db24fa7ce5039d6f9cf61238b420aacf2ecd32db7cfce3 Latest Version: v1.19.4-k3s1 Events: <none> And, for the magic to happen we just have to enable to k3s upgrade with command: $ kubectl label node --all k3s-upgrade=enabled node/n2 labeled node/n4 labeled node/n3 labeled node/n1 labeled node/n5 labeled To check if the k3s upgrades are kicking in watch with: $ kubectl get pods -n system-upgrade -w NAME READY STATUS RESTARTS AGE system-upgrade-controller-556df575dd-2qfrs 1/1 Running 0 109m apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Init:0/2 0 73s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Init:0/2 0 73s apply-k3s-server-on-n1-with-e50d232791db24fa7ce5039d6f9cf-2j42m 1/1 Running 0 73s apply-k3s-server-on-n1-with-e50d232791db24fa7ce5039d6f9cf-2j42m 0/1 Completed 0 75s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Init:1/2 0 89s apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Init:1/2 0 92s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Init:1/2 0 104s apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Init:1/2 0 107s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 PodInitializing 0 108s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 1/1 Running 0 110s apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 PodInitializing 0 110s apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 1/1 Running 0 112s apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Completed 0 2m19s apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Completed 0 2m21s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Pending 0 0s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Pending 0 0s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Pending 0 0s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Pending 0 0s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Init:0/2 0 0s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Init:0/2 0 0s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Init:0/2 0 18s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Init:0/2 0 30s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Init:1/2 0 33s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Init:1/2 0 42s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Init:1/2 0 47s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Init:1/2 0 61s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 PodInitializing 0 70s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 PodInitializing 0 77s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 1/1 Running 0 79s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 1/1 Running 0 79s apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Completed 0 107s apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Completed 0 109s apply-k3s-server-on-n1-with-e50d232791db24fa7ce5039d6f9cf-2j42m 0/1 Terminating 0 16m apply-k3s-server-on-n1-with-e50d232791db24fa7ce5039d6f9cf-2j42m 0/1 Terminating 0 16m apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Terminating 0 17m apply-k3s-agent-on-n2-with-e50d232791db24fa7ce5039d6f9cf6-2zb2j 0/1 Terminating 0 17m apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Terminating 0 17m apply-k3s-agent-on-n4-with-e50d232791db24fa7ce5039d6f9cf6-sspgt 0/1 Terminating 0 17m apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Terminating 0 16m apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Terminating 0 16m apply-k3s-agent-on-n5-with-e50d232791db24fa7ce5039d6f9cf6-rtjr7 0/1 Terminating 0 16m apply-k3s-agent-on-n3-with-e50d232791db24fa7ce5039d6f9cf6-567zz 0/1 Terminating 0 16m As you can see from the sequence first the k3s-server gets updated and thereafter, 2 worker nodes at a time as we requested in the plan description. After a couple of minutes we see: $ kubectl get nodes NAME STATUS ROLES AGE VERSION n1 Ready master 117d v1.19.4+k3s1 n2 Ready <none> 117d v1.19.4+k3s1 n4 Ready <none> 117d v1.19.4+k3s1 n3 Ready <none> 117d v1.19.4+k3s1 n5 Ready <none> 117d v1.19.4+k3s1 We believe it is better to disable the k3s upgrades once it is done with the command: $ kubectl label node --all --overwrite k3s-upgrade=disabled node/n5 labeled node/n1 labeled node/n2 labeled node/n4 labeled node/n3 labeled If we want to upgrade again just edit the plan [4] again with the correct version of k3s and replace the plan with the command: $ kubectl replace -f ./k3s-upgrade-plan.yaml To start the k3s version upgrade overwrite the label k3s-upgrade again with keyword enabled .","title":"Making a k3s upgrade plan"},{"location":"pi-stories6/#references","text":"[1] Rancher Operational Advisory: Attention All Rancher K3s Customers, DB bug requires upgrade [2] Automate K3s Upgrades with System Upgrade Controller [3] CRD system-upgrade-controller [4] k3s Upgrade Plan [5] GitHub Sources of our k3s upgrade controller","title":"References"},{"location":"pi-stories7/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - YAML everywhere - what about correctness? \u00b6 When you dive deep into Kubernetes you will notice you cannot go around YAML [1] language. You hate it or love it, however, you better get used to it as it is part of the core of kubernetes. Writing YAML code from scratch is not a real pleasure, therefore, having a linter would be nice to avoid the low hanging fruit errors. We found a kubernetes linter tool called \" KubeLinter \" written in the Go language, however, there is no binary available for the Raspberry Pi4 architecture aarch64 on the release page of kube-linter . Therefore, we decided to build it ourselves from the sources. Installling the Go Language binaries \u00b6 On our node n1 we installed the Go Language with the commands: $ sudo apt install golang-go $ sudo apt install make Compiling the KubeLinter code \u00b6 Download the code from the Kube-Linter Github page and according the \" Building from source \" documentation it would be as easy as make build to generate the binary: $ git clone git@github.com:gdha/kube-linter.git $ cd kube-linter $ make build ... all modules verified + /home/gdha/projects/kube-linter/.gobin/packr go install github.com/gobuffalo/packr/packr packr Compiling Go source in ./cmd/kube-linter to bin/darwin/kube-linter # golang.stackrox.io/kube-linter/cmd/kube-linter /usr/lib/go-1.13/pkg/tool/linux_arm64/link: running gcc failed: exit status 1 /usr/bin/ld: unrecognized -a option `gezero_size' collect2: error: ld returned 1 exit status make: *** [Makefile:100: build] Error 2 As usual it didn't work as expected. Mind the bin/darwin/kube-linter compile line mentions darwin and that is not our architecture for RPI4. We figured out that by tweaking the Makefile [2] we could build the kube-linter to an executable. Installing kube-linter as /usr/local/bin/kube-linter \u00b6 We update the Makefile with an install rule for our binary so that we do not have to add the PATH to out .bashrc file. Just run make install to copy the compiled binary to /usr/local/bin/kube-linter . To verify it works try the command: $ kube-linter version 0.1.4-10-g5c30a676d3-dirty Testing it out on a real example \u00b6 In a previous post we discussed and explained the rolling upgrade of the k3s kubernetes software on our pods. [3] Therefore, what kind of information will the kube-linter produce on these YAML files? $ kube-linter lint ../k3s-upgrade-controller/ ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" does not have a read-only root file system (check: no-read-only-root-fs, remediation: Set readOnlyRootFilesystem to true in your container's securityContext.) ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" is not set to runAsNonRoot (check: run-as-non-root, remediation: Set runAsUser to a non-zero number, and runAsNonRoot to true, in your pod or container securityContext. See https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for more details.) ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" has cpu request 0 (check: unset-cpu-requirements, remediation: Set your container's CPU requests and limits depending on its requirements. See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for more details.) ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" has cpu limit 0 (check: unset-cpu-requirements, remediation: Set your container's CPU requests and limits depending on its requirements. See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for more details.) ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" has memory request 0 (check: unset-memory-requirements, remediation: Set your container's memory requests and limits depending on its requirements. See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for more details.) ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" has memory limit 0 (check: unset-memory-requirements, remediation: Set your container's memory requests and limits depending on its requirements. See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for more details.) Error: found 6 lint errors The recommendations are perhaps not really perfect for this example as we do need a writable file system to be able to perform an update and root permissions will be required as well. However, the test itself was successful as it produces a meaningfull output. For a more profound usage of kube-linter see the \" KubeLinter documentation \" [4]. References \u00b6 [1] YAML Ain't Markup Language [2] Kube-Linter GitHub fork [3] CRD system-upgrade-controller [4] KubeLinter Documenation","title":"Raspberry Pi 4 YAML everywhere -what about correctness?"},{"location":"pi-stories7/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories7/#raspberry_pi_4_cluster_series_-_yaml_everywhere_-_what_about_correctness","text":"When you dive deep into Kubernetes you will notice you cannot go around YAML [1] language. You hate it or love it, however, you better get used to it as it is part of the core of kubernetes. Writing YAML code from scratch is not a real pleasure, therefore, having a linter would be nice to avoid the low hanging fruit errors. We found a kubernetes linter tool called \" KubeLinter \" written in the Go language, however, there is no binary available for the Raspberry Pi4 architecture aarch64 on the release page of kube-linter . Therefore, we decided to build it ourselves from the sources.","title":"Raspberry Pi 4 cluster Series - YAML everywhere - what about correctness?"},{"location":"pi-stories7/#installling_the_go_language_binaries","text":"On our node n1 we installed the Go Language with the commands: $ sudo apt install golang-go $ sudo apt install make","title":"Installling the Go Language binaries"},{"location":"pi-stories7/#compiling_the_kubelinter_code","text":"Download the code from the Kube-Linter Github page and according the \" Building from source \" documentation it would be as easy as make build to generate the binary: $ git clone git@github.com:gdha/kube-linter.git $ cd kube-linter $ make build ... all modules verified + /home/gdha/projects/kube-linter/.gobin/packr go install github.com/gobuffalo/packr/packr packr Compiling Go source in ./cmd/kube-linter to bin/darwin/kube-linter # golang.stackrox.io/kube-linter/cmd/kube-linter /usr/lib/go-1.13/pkg/tool/linux_arm64/link: running gcc failed: exit status 1 /usr/bin/ld: unrecognized -a option `gezero_size' collect2: error: ld returned 1 exit status make: *** [Makefile:100: build] Error 2 As usual it didn't work as expected. Mind the bin/darwin/kube-linter compile line mentions darwin and that is not our architecture for RPI4. We figured out that by tweaking the Makefile [2] we could build the kube-linter to an executable.","title":"Compiling the KubeLinter code"},{"location":"pi-stories7/#installing_kube-linter_as_usrlocalbinkube-linter","text":"We update the Makefile with an install rule for our binary so that we do not have to add the PATH to out .bashrc file. Just run make install to copy the compiled binary to /usr/local/bin/kube-linter . To verify it works try the command: $ kube-linter version 0.1.4-10-g5c30a676d3-dirty","title":"Installing kube-linter as /usr/local/bin/kube-linter"},{"location":"pi-stories7/#testing_it_out_on_a_real_example","text":"In a previous post we discussed and explained the rolling upgrade of the k3s kubernetes software on our pods. [3] Therefore, what kind of information will the kube-linter produce on these YAML files? $ kube-linter lint ../k3s-upgrade-controller/ ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" does not have a read-only root file system (check: no-read-only-root-fs, remediation: Set readOnlyRootFilesystem to true in your container's securityContext.) ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" is not set to runAsNonRoot (check: run-as-non-root, remediation: Set runAsUser to a non-zero number, and runAsNonRoot to true, in your pod or container securityContext. See https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for more details.) ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" has cpu request 0 (check: unset-cpu-requirements, remediation: Set your container's CPU requests and limits depending on its requirements. See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for more details.) ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" has cpu limit 0 (check: unset-cpu-requirements, remediation: Set your container's CPU requests and limits depending on its requirements. See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for more details.) ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" has memory request 0 (check: unset-memory-requirements, remediation: Set your container's memory requests and limits depending on its requirements. See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for more details.) ../k3s-upgrade-controller/system-upgrade-controller.yaml: (object: system-upgrade/system-upgrade-controller apps/v1, Kind=Deployment) container \"system-upgrade-controller\" has memory limit 0 (check: unset-memory-requirements, remediation: Set your container's memory requests and limits depending on its requirements. See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for more details.) Error: found 6 lint errors The recommendations are perhaps not really perfect for this example as we do need a writable file system to be able to perform an update and root permissions will be required as well. However, the test itself was successful as it produces a meaningfull output. For a more profound usage of kube-linter see the \" KubeLinter documentation \" [4].","title":"Testing it out on a real example"},{"location":"pi-stories7/#references","text":"[1] YAML Ain't Markup Language [2] Kube-Linter GitHub fork [3] CRD system-upgrade-controller [4] KubeLinter Documenation","title":"References"},{"location":"pi-stories8/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Keep your Operating System updated \u00b6 When you login on one of your nodes you often see a message like the following: Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 5.4.0-1026-raspi aarch64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage System information as of Thu 28 Jan 2021 12:05:56 PM CET System load: 0.55 Usage of /: 18.3% of 28.40GB Memory usage: 14% Swap usage: 0% Temperature: 34.6 C Processes: 157 Users logged in: 1 IPv4 address for cni0: 10.42.1.1 IPv4 address for docker0: 172.17.0.1 IPv4 address for eth0: 192.168.0.205 * Introducing self-healing high availability clusters in MicroK8s. Simple, hardened, Kubernetes for production, from RaspberryPi to DC. https://microk8s.io/high-availability 62 updates can be installed immediately. 0 of these updates are security updates. To see these additional updates run: apt list --upgradable Last login: Thu Jan 28 12:01:54 2021 from 192.168.0.41 Pay attention to the amount of packages ready to update: See line \"62 updates can be installed immediately\" Doing that for a bunch of systems is tiring/boring so let get this done via ansible. First we need to download package information from all configured sources. $ ansible pi -m shell -b -a \"apt update\" n1 | SUCCESS | rc=0 >> Hit:1 http://ports.ubuntu.com/ubuntu-ports focal InRelease Get:2 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease [114 kB] Get:3 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease [101 kB] Get:4 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease [109 kB] Fetched 324 kB in 2s (187 kB/s) Reading package lists... Building dependency tree... Reading state information... 49 packages can be upgraded. Run 'apt list --upgradable' to see them. WARNING: apt does not have a stable CLI interface. Use with caution in scripts. ... n5 | SUCCESS | rc=0 >> Hit:1 http://ports.ubuntu.com/ubuntu-ports focal InRelease Get:2 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease [114 kB] Get:3 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease [101 kB] Get:4 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease [109 kB] Fetched 324 kB in 2s (196 kB/s) Reading package lists... Building dependency tree... Reading state information... 58 packages can be upgraded. Run 'apt list --upgradable' to see them. WARNING: apt does not have a stable CLI interface. Use with caution in scripts And, finally, to automate the installation of the packages: $ ansible pi -m shell -b -a \"apt --yes upgrade\"","title":"Raspberry Pi 4 Keep your Operating System updated"},{"location":"pi-stories8/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories8/#raspberry_pi_4_cluster_series_-_keep_your_operating_system_updated","text":"When you login on one of your nodes you often see a message like the following: Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 5.4.0-1026-raspi aarch64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage System information as of Thu 28 Jan 2021 12:05:56 PM CET System load: 0.55 Usage of /: 18.3% of 28.40GB Memory usage: 14% Swap usage: 0% Temperature: 34.6 C Processes: 157 Users logged in: 1 IPv4 address for cni0: 10.42.1.1 IPv4 address for docker0: 172.17.0.1 IPv4 address for eth0: 192.168.0.205 * Introducing self-healing high availability clusters in MicroK8s. Simple, hardened, Kubernetes for production, from RaspberryPi to DC. https://microk8s.io/high-availability 62 updates can be installed immediately. 0 of these updates are security updates. To see these additional updates run: apt list --upgradable Last login: Thu Jan 28 12:01:54 2021 from 192.168.0.41 Pay attention to the amount of packages ready to update: See line \"62 updates can be installed immediately\" Doing that for a bunch of systems is tiring/boring so let get this done via ansible. First we need to download package information from all configured sources. $ ansible pi -m shell -b -a \"apt update\" n1 | SUCCESS | rc=0 >> Hit:1 http://ports.ubuntu.com/ubuntu-ports focal InRelease Get:2 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease [114 kB] Get:3 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease [101 kB] Get:4 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease [109 kB] Fetched 324 kB in 2s (187 kB/s) Reading package lists... Building dependency tree... Reading state information... 49 packages can be upgraded. Run 'apt list --upgradable' to see them. WARNING: apt does not have a stable CLI interface. Use with caution in scripts. ... n5 | SUCCESS | rc=0 >> Hit:1 http://ports.ubuntu.com/ubuntu-ports focal InRelease Get:2 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease [114 kB] Get:3 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease [101 kB] Get:4 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease [109 kB] Fetched 324 kB in 2s (196 kB/s) Reading package lists... Building dependency tree... Reading state information... 58 packages can be upgraded. Run 'apt list --upgradable' to see them. WARNING: apt does not have a stable CLI interface. Use with caution in scripts And, finally, to automate the installation of the packages: $ ansible pi -m shell -b -a \"apt --yes upgrade\"","title":"Raspberry Pi 4 cluster Series - Keep your Operating System updated"},{"location":"pi-stories9/","text":"PI4 Stories \u00b6 Raspberry Pi 4 cluster Series - Installation of Longhorn \u00b6 Prepare our external USB block devices \u00b6 On all our pi systems we added an USB block device of the same size and we are sure that they all put in the same USB port so that we are sure the all have the same block device name, e.g. /dev/sda We create an ansible playbook to prepare the USB block devices and have it mounted on /app/longhorn on each node. E.g. $ df /app/longhorn/ Filesystem 1K-blocks Used Available Use% Mounted on /dev/sda1 117715864 61472 111631688 1% /app/longhorn Using helm to perform the installation \u00b6 If we want to use helm to performt the installation of longhorn we first need to install it as it isn't standard avaibale on these systems. Getting helm from URL https://helm.sh/docs/intro/install/ . $ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 $ chmod 700 get_helm.sh $ ./get_helm.sh Downloading https://get.helm.sh/helm-v3.5.3-linux-arm64.tar.gz Verifying checksum... Done. Preparing to install helm into /usr/local/bin helm installed into /usr/local/bin/helm Alright, now we have the helm executable available on our local system (e.g.node n1). We can now download the helm longhorn chart repository: $ helm repo add longhorn https://charts.longhorn.io \"longhorn\" has been added to your repositories $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"longhorn\" chart repository Update Complete. \u2388Happy Helming!\u2388 We need to be careful when we run the helm installer as we only want longhorn to use the USB block devices mounted at /app/longhorn and it should not be using the default location /var/lib/longhorn (as this might fill up the root partition). Information on how we can actually do this can be found at \"Adding Node Tags to New Nodes\" : $ helm install longhorn longhorn/longhorn --namespace longhorn-system \\ --set defaultSettings.defaultDataPath=\"/app/longhorn/\" NAME: longhorn LAST DEPLOYED: Wed Apr 14 09:23:52 2021 NAMESPACE: longhorn-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Longhorn is now installed on the cluster! Please wait a few minutes for other Longhorn components such as CSI deployments, Engine Images, and Instance Managers to be initialized. Visit our documentation at https://longhorn.io/docs/ Prepare the longhorn-ingress (required for UI) \u00b6 In-depth information about accessing the longhorn UI can be found at longhorn-ingress . In short this is the procedure we followed: $ USER=gdha; PASSWORD=*******; echo \"${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})\" >> auth $ cat auth gdha:$apr1$XXXXXXXXXXXXXXXXXXXXXXXX $ kubectl -n longhorn-system create secret generic basic-auth --from-file=auth secret/basic-auth created $ kubectl -n longhorn-system get secret basic-auth -o yaml apiVersion: v1 data: auth: Z2RoYTokYXByMSRLTU1hQWpiSiROVENtRWI2Qm05dDdvSmJXV1RlWVcuCg== kind: Secret metadata: creationTimestamp: \"2021-04-09T15:38:05Z\" managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:auth: {} f:type: {} manager: kubectl-create operation: Update time: \"2021-04-09T15:38:05Z\" name: basic-auth namespace: longhorn-system resourceVersion: \"763254\" uid: 0a88e170-071d-4813-934f-9dec352be01d type: Opaque Then we paste the following set of yaml command lines into kubectl to create the longhorn-ingress: $ echo \" apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: longhorn-ingress namespace: longhorn-system annotations: # type of authentication nginx.ingress.kubernetes.io/auth-type: basic # prevent the controller from redirecting (308) to HTTPS nginx.ingress.kubernetes.io/ssl-redirect: 'false' # name of the secret that contains the user/password definitions nginx.ingress.kubernetes.io/auth-secret: basic-auth # message to display with an appropriate context why the authentication is required nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required ' spec: rules: - http: paths: - path: / backend: serviceName: longhorn-frontend servicePort: 80 \" | kubectl -n longhorn-system create -f - ingress.networking.k8s.io/longhorn-ingress created $ kubectl -n longhorn-system get ingress NAME CLASS HOSTS ADDRESS PORTS AGE longhorn-ingress <none> * 192.168.0.201 80 11s Use the longhorn UI \u00b6 In previous command we saw that the IP address where the longhorn-ingress is running is in our case 192.168.0.201 and to test the connectivity we can use curl : $ curl -v http://192.168.0.201/ * Trying 192.168.0.201:80... * TCP_NODELAY set * Connected to 192.168.0.201 (192.168.0.201) port 80 (#0) > GET / HTTP/1.1 > Host: 192.168.0.201 > User-Agent: curl/7.68.0 > Accept: */* ... However, we a browser pointing to http://192.168.0.201/#/dashboard we get a better overview: Or, when selecting the node tab: And, the details of one node: References \u00b6 [1] Ansible playbook to prepare USB devices [2] Longhorn [3] Adding Node Tags to New Nodes [4] Accessing Loghorn through UI","title":"Raspberry Pi 4 Installation of Longhorn"},{"location":"pi-stories9/#pi4_stories","text":"","title":"PI4 Stories"},{"location":"pi-stories9/#raspberry_pi_4_cluster_series_-_installation_of_longhorn","text":"","title":"Raspberry Pi 4 cluster Series - Installation of Longhorn"},{"location":"pi-stories9/#prepare_our_external_usb_block_devices","text":"On all our pi systems we added an USB block device of the same size and we are sure that they all put in the same USB port so that we are sure the all have the same block device name, e.g. /dev/sda We create an ansible playbook to prepare the USB block devices and have it mounted on /app/longhorn on each node. E.g. $ df /app/longhorn/ Filesystem 1K-blocks Used Available Use% Mounted on /dev/sda1 117715864 61472 111631688 1% /app/longhorn","title":"Prepare our external USB block devices"},{"location":"pi-stories9/#using_helm_to_perform_the_installation","text":"If we want to use helm to performt the installation of longhorn we first need to install it as it isn't standard avaibale on these systems. Getting helm from URL https://helm.sh/docs/intro/install/ . $ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 $ chmod 700 get_helm.sh $ ./get_helm.sh Downloading https://get.helm.sh/helm-v3.5.3-linux-arm64.tar.gz Verifying checksum... Done. Preparing to install helm into /usr/local/bin helm installed into /usr/local/bin/helm Alright, now we have the helm executable available on our local system (e.g.node n1). We can now download the helm longhorn chart repository: $ helm repo add longhorn https://charts.longhorn.io \"longhorn\" has been added to your repositories $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"longhorn\" chart repository Update Complete. \u2388Happy Helming!\u2388 We need to be careful when we run the helm installer as we only want longhorn to use the USB block devices mounted at /app/longhorn and it should not be using the default location /var/lib/longhorn (as this might fill up the root partition). Information on how we can actually do this can be found at \"Adding Node Tags to New Nodes\" : $ helm install longhorn longhorn/longhorn --namespace longhorn-system \\ --set defaultSettings.defaultDataPath=\"/app/longhorn/\" NAME: longhorn LAST DEPLOYED: Wed Apr 14 09:23:52 2021 NAMESPACE: longhorn-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Longhorn is now installed on the cluster! Please wait a few minutes for other Longhorn components such as CSI deployments, Engine Images, and Instance Managers to be initialized. Visit our documentation at https://longhorn.io/docs/","title":"Using helm to perform the installation"},{"location":"pi-stories9/#prepare_the_longhorn-ingress_required_for_ui","text":"In-depth information about accessing the longhorn UI can be found at longhorn-ingress . In short this is the procedure we followed: $ USER=gdha; PASSWORD=*******; echo \"${USER}:$(openssl passwd -stdin -apr1 <<< ${PASSWORD})\" >> auth $ cat auth gdha:$apr1$XXXXXXXXXXXXXXXXXXXXXXXX $ kubectl -n longhorn-system create secret generic basic-auth --from-file=auth secret/basic-auth created $ kubectl -n longhorn-system get secret basic-auth -o yaml apiVersion: v1 data: auth: Z2RoYTokYXByMSRLTU1hQWpiSiROVENtRWI2Qm05dDdvSmJXV1RlWVcuCg== kind: Secret metadata: creationTimestamp: \"2021-04-09T15:38:05Z\" managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:auth: {} f:type: {} manager: kubectl-create operation: Update time: \"2021-04-09T15:38:05Z\" name: basic-auth namespace: longhorn-system resourceVersion: \"763254\" uid: 0a88e170-071d-4813-934f-9dec352be01d type: Opaque Then we paste the following set of yaml command lines into kubectl to create the longhorn-ingress: $ echo \" apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: longhorn-ingress namespace: longhorn-system annotations: # type of authentication nginx.ingress.kubernetes.io/auth-type: basic # prevent the controller from redirecting (308) to HTTPS nginx.ingress.kubernetes.io/ssl-redirect: 'false' # name of the secret that contains the user/password definitions nginx.ingress.kubernetes.io/auth-secret: basic-auth # message to display with an appropriate context why the authentication is required nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required ' spec: rules: - http: paths: - path: / backend: serviceName: longhorn-frontend servicePort: 80 \" | kubectl -n longhorn-system create -f - ingress.networking.k8s.io/longhorn-ingress created $ kubectl -n longhorn-system get ingress NAME CLASS HOSTS ADDRESS PORTS AGE longhorn-ingress <none> * 192.168.0.201 80 11s","title":"Prepare the longhorn-ingress (required for UI)"},{"location":"pi-stories9/#use_the_longhorn_ui","text":"In previous command we saw that the IP address where the longhorn-ingress is running is in our case 192.168.0.201 and to test the connectivity we can use curl : $ curl -v http://192.168.0.201/ * Trying 192.168.0.201:80... * TCP_NODELAY set * Connected to 192.168.0.201 (192.168.0.201) port 80 (#0) > GET / HTTP/1.1 > Host: 192.168.0.201 > User-Agent: curl/7.68.0 > Accept: */* ... However, we a browser pointing to http://192.168.0.201/#/dashboard we get a better overview: Or, when selecting the node tab: And, the details of one node:","title":"Use the longhorn UI"},{"location":"pi-stories9/#references","text":"[1] Ansible playbook to prepare USB devices [2] Longhorn [3] Adding Node Tags to New Nodes [4] Accessing Loghorn through UI","title":"References"}]}